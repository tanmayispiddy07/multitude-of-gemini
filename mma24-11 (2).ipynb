{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285},{"sourceId":9766676,"sourceType":"datasetVersion","datasetId":5981576},{"sourceId":9805539,"sourceType":"datasetVersion","datasetId":6010262},{"sourceId":9805563,"sourceType":"datasetVersion","datasetId":6010284},{"sourceId":9863041,"sourceType":"datasetVersion","datasetId":6053608},{"sourceId":9886451,"sourceType":"datasetVersion","datasetId":6071234},{"sourceId":9887201,"sourceType":"datasetVersion","datasetId":6071808},{"sourceId":9887505,"sourceType":"datasetVersion","datasetId":6072048},{"sourceId":9919084,"sourceType":"datasetVersion","datasetId":6095918},{"sourceId":9920943,"sourceType":"datasetVersion","datasetId":6097220},{"sourceId":9921004,"sourceType":"datasetVersion","datasetId":6097270},{"sourceId":9994546,"sourceType":"datasetVersion","datasetId":6151453},{"sourceId":9994726,"sourceType":"datasetVersion","datasetId":6151558},{"sourceId":10012502,"sourceType":"datasetVersion","datasetId":6164260},{"sourceId":9994650,"sourceType":"datasetVersion","datasetId":5981532}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemini-Assisted Model Training and Optimization Experiment","metadata":{}},{"cell_type":"markdown","source":"# 1. Objective","metadata":{}},{"cell_type":"markdown","source":"Process of training a feedforward neural network on the MNIST and Iris datasets with the assistance of the Gemini-Pro-1.5 model to optimize weights, biases, learning rates, and architecture.","metadata":{}},{"cell_type":"markdown","source":" The goal of this experiment was to achieve a high accuracy (>90%) while minimizing the number of training epochs, by leveraging Gemini's recommendations for parameter and structural adjustments","metadata":{}},{"cell_type":"code","source":"import os\n\n# Specify the path to the dataset\ndata_path = '/kaggle/input/mnist-dataset'\n\n# List the files in the dataset directory\nprint(os.listdir(data_path))","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:31:30.478638Z","iopub.execute_input":"2024-11-25T16:31:30.479330Z","iopub.status.idle":"2024-11-25T16:31:30.485591Z","shell.execute_reply.started":"2024-11-25T16:31:30.479297Z","shell.execute_reply":"2024-11-25T16:31:30.484652Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['t10k-labels-idx1-ubyte', 'train-images.idx3-ubyte', 't10k-images-idx3-ubyte', 't10k-labels.idx1-ubyte', 't10k-images.idx3-ubyte', 'train-labels.idx1-ubyte', 'train-labels-idx1-ubyte', 'train-images-idx3-ubyte']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:31:30.487248Z","iopub.execute_input":"2024-11-25T16:31:30.487527Z","iopub.status.idle":"2024-11-25T16:31:33.534375Z","shell.execute_reply.started":"2024-11-25T16:31:30.487502Z","shell.execute_reply":"2024-11-25T16:31:33.533579Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 2. Dataset Description ","metadata":{}},{"cell_type":"markdown","source":"MNIST Dataset: Used to train and evaluate a custom dataset class for handling raw binary data files (train-images.idx3-ubyte, train-labels.idx1-ubyte).                                                                                          \n\nIris Dataset: Used as a classification dataset for the neural network model. \n\nPreprocessing included:\nSplitting the dataset into training (80%) and validation (20%) sets.\nStandardizing features using StandardScaler to center values around zero.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# Custom MNIST Dataset class\nclass MNISTCustomDataset(Dataset):\n    def __init__(self, data_path, train=True, transform=None):\n        self.transform = transform\n        \n        # Define file paths based on actual filenames\n        if train:\n            self.images_path = os.path.join(data_path, '/kaggle/input/mnist-dataset/train-images.idx3-ubyte')\n            self.labels_path = os.path.join(data_path, '/kaggle/input/mnist-dataset/train-labels.idx1-ubyte')\n        else:\n            self.images_path = os.path.join(data_path, '/kaggle/input/mnist-dataset/t10k-images.idx3-ubyte')\n            self.labels_path = os.path.join(data_path, '/kaggle/input/mnist-dataset/t10k-labels.idx1-ubyte')\n\n        # Load the images and labels\n        self.images = self.load_images()\n        self.labels = self.load_labels()\n        \n    def load_images(self):\n        # Ensure we open the file, not the directory\n        with open(self.images_path, 'rb') as f:\n            f.read(16)  # Skip the header\n            data = np.fromfile(f, dtype=np.uint8)\n            data = data.reshape(-1, 28, 28)  # Reshape to (num_samples, 28, 28)\n        return data\n\n    def load_labels(self):\n        # Ensure we open the file, not the directory\n        with open(self.labels_path, 'rb') as f:\n            f.read(8)  # Skip the header\n            labels = np.fromfile(f, dtype=np.uint8)\n        return labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        # Get the image and label at the given index\n        image, label = self.images[idx], self.labels[idx]\n        \n        # Apply transformations, if any\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Define transformations for the dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))  # Normalize for grayscale images\n])\n\n# Set the path to your MNIST dataset\ndata_path = '/path/to/mnist-dataset'\n\n# Create the custom dataset\ntrain_dataset = MNISTCustomDataset(data_path, train=True, transform=transform)\nval_dataset = MNISTCustomDataset(data_path, train=False, transform=transform)\n\n# Create DataLoaders\ndata_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalidation_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Check the sizes of the datasets\nprint(f'Train dataset size: {len(train_dataset)}')\nprint(f'Validation dataset size: {len(val_dataset)}')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:31:33.535458Z","iopub.execute_input":"2024-11-25T16:31:33.535781Z","iopub.status.idle":"2024-11-25T16:31:34.950867Z","shell.execute_reply.started":"2024-11-25T16:31:33.535756Z","shell.execute_reply":"2024-11-25T16:31:34.949941Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Train dataset size: 60000\nValidation dataset size: 10000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 3. Model Architecture and Initial Parameters","metadata":{}},{"cell_type":"markdown","source":"**Base Architecture**: Feedforward neural network\n\n** Layers:**\n*     Input Layer: 4 features (for Iris dataset)\n*     Hidden Layer: 10 neurons with ReLU activation\n*     Output Layer: 3 classes (for Iris dataset)\n*     Loss Function: CrossEntropyLoss for classification tasks\n*     Optimizer: Adam (initial learning rate = 0.01) with weight decay for regularization\n*     Learning Rate Scheduler: StepLR, decaying learning rate every 5 epochs by a factor of 0.1.","metadata":{}},{"cell_type":"code","source":"pip install torchinfo","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:31:34.951986Z","iopub.execute_input":"2024-11-25T16:31:34.952371Z","iopub.status.idle":"2024-11-25T16:31:44.205834Z","shell.execute_reply.started":"2024-11-25T16:31:34.952344Z","shell.execute_reply":"2024-11-25T16:31:44.204688Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchinfo in /opt/conda/lib/python3.10/site-packages (1.8.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n# Create DataLoader\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# Define the feedforward neural network model\nclass FeedforwardNN(nn.Module):\n    def __init__(self):\n        super(FeedforwardNN, self).__init__()\n        self.fc1 = nn.Linear(4, 10)  # 4 input features, 10 hidden units\n        self.fc2 = nn.Linear(10, 3)  # 10 hidden units, 3 output classes\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Initialize the model, loss function, and optimizer\nmodel = FeedforwardNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Track accuracies, learning rates, weights, and biases during training\nnum_epochs = 20\naccuracies = []\nlearning_rates = []\nweights_biases = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    correct = 0\n    total = 0\n\n    # Track current learning rate\n    for param_group in optimizer.param_groups:\n        current_lr = param_group['lr']\n    learning_rates.append(current_lr)\n\n    # Track weights and biases for each layer\n    epoch_weights_biases = {}\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            epoch_weights_biases[name] = param.data.clone().detach().numpy()\n    weights_biases.append(epoch_weights_biases)\n\n    for inputs, labels in train_loader:\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Calculate accuracy\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    # Calculate epoch accuracy and append to accuracies list\n    accuracy = 100 * correct / total\n    accuracies.append(accuracy)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%, Learning Rate: {current_lr}')\n\n# Print all recorded accuracies, learning rates, weights, and biases\nprint(\"\\nTraining accuracies for each epoch:\", accuracies)\nprint(\"\\nLearning rates for each epoch:\", learning_rates)\nprint(\"\\nWeights and biases per epoch:\")\nfor epoch_idx, params in enumerate(weights_biases):\n    print(f\"Epoch {epoch_idx + 1}:\")\n    for name, values in params.items():\n        print(f\"  {name} - {values}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:31:44.209402Z","iopub.execute_input":"2024-11-25T16:31:44.209692Z","iopub.status.idle":"2024-11-25T16:31:45.380478Z","shell.execute_reply.started":"2024-11-25T16:31:44.209667Z","shell.execute_reply":"2024-11-25T16:31:45.379659Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch [1/20], Loss: 1.0744, Accuracy: 38.33%, Learning Rate: 0.01\nEpoch [2/20], Loss: 1.0888, Accuracy: 40.83%, Learning Rate: 0.01\nEpoch [3/20], Loss: 1.0559, Accuracy: 43.33%, Learning Rate: 0.01\nEpoch [4/20], Loss: 0.9940, Accuracy: 44.17%, Learning Rate: 0.01\nEpoch [5/20], Loss: 0.9824, Accuracy: 47.50%, Learning Rate: 0.01\nEpoch [6/20], Loss: 0.9303, Accuracy: 45.83%, Learning Rate: 0.01\nEpoch [7/20], Loss: 0.9306, Accuracy: 48.33%, Learning Rate: 0.01\nEpoch [8/20], Loss: 0.9977, Accuracy: 60.83%, Learning Rate: 0.01\nEpoch [9/20], Loss: 0.9615, Accuracy: 74.17%, Learning Rate: 0.01\nEpoch [10/20], Loss: 0.8618, Accuracy: 78.33%, Learning Rate: 0.01\nEpoch [11/20], Loss: 0.9259, Accuracy: 77.50%, Learning Rate: 0.01\nEpoch [12/20], Loss: 0.8374, Accuracy: 77.50%, Learning Rate: 0.01\nEpoch [13/20], Loss: 0.9204, Accuracy: 74.17%, Learning Rate: 0.01\nEpoch [14/20], Loss: 0.8020, Accuracy: 75.00%, Learning Rate: 0.01\nEpoch [15/20], Loss: 0.7851, Accuracy: 75.83%, Learning Rate: 0.01\nEpoch [16/20], Loss: 0.8579, Accuracy: 73.33%, Learning Rate: 0.01\nEpoch [17/20], Loss: 0.8401, Accuracy: 72.50%, Learning Rate: 0.01\nEpoch [18/20], Loss: 0.8584, Accuracy: 72.50%, Learning Rate: 0.01\nEpoch [19/20], Loss: 0.6731, Accuracy: 72.50%, Learning Rate: 0.01\nEpoch [20/20], Loss: 0.7916, Accuracy: 72.50%, Learning Rate: 0.01\n\nTraining accuracies for each epoch: [38.333333333333336, 40.833333333333336, 43.333333333333336, 44.166666666666664, 47.5, 45.833333333333336, 48.333333333333336, 60.833333333333336, 74.16666666666667, 78.33333333333333, 77.5, 77.5, 74.16666666666667, 75.0, 75.83333333333333, 73.33333333333333, 72.5, 72.5, 72.5, 72.5]\n\nLearning rates for each epoch: [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n\nWeights and biases per epoch:\nEpoch 1:\n  fc1.weight - [[-0.00854379 -0.12515652  0.10358107 -0.34021705]\n [ 0.25954223 -0.31221986  0.188671    0.19610316]\n [ 0.14375496  0.19623601  0.07605493  0.3497337 ]\n [ 0.057127    0.30917102 -0.36640078 -0.01737148]\n [-0.22805232 -0.12582606  0.11065274 -0.24039769]\n [-0.33555013  0.1922369   0.3650542   0.33164388]\n [-0.1370706  -0.11235487  0.02051103 -0.28522527]\n [ 0.05814379 -0.42140514  0.13673621  0.03180611]\n [ 0.2485109   0.16935825  0.25939387 -0.34040248]\n [-0.19712234 -0.19279355 -0.45842284 -0.45859653]]\n  fc1.bias - [-0.18332207  0.13002521  0.19659615  0.1106621   0.20761305 -0.14390934\n  0.21870661 -0.34424663  0.47068125  0.06406564]\n  fc2.weight - [[ 0.1203277  -0.2984132   0.0139719   0.01604393  0.22072425  0.2894592\n   0.2791599   0.2265636  -0.27721253 -0.23104192]\n [ 0.10310257 -0.14194955  0.2009466   0.26449415  0.20005569  0.26149175\n   0.07854901  0.21295348 -0.16465214  0.12980579]\n [ 0.24178101  0.11205941  0.17747857  0.27316752  0.29113588  0.08562053\n   0.22506219 -0.21482912 -0.09368247 -0.16611747]]\n  fc2.bias - [-0.05035034  0.04034005 -0.24799417]\nEpoch 2:\n  fc1.weight - [[-0.00801556 -0.12487695  0.10406072 -0.33968392]\n [ 0.26366457 -0.31285468  0.19362183  0.2016242 ]\n [ 0.1450721   0.19462202  0.07810113  0.35167724]\n [ 0.0622008   0.30517867 -0.3599825  -0.01119693]\n [-0.22774373 -0.12573168  0.11104936 -0.23989703]\n [-0.33747     0.19243339  0.3627156   0.32886955]\n [-0.1399734  -0.10794926  0.01617477 -0.28911203]\n [ 0.05536458 -0.42082715  0.13355508  0.02851466]\n [ 0.25279307  0.16637854  0.26480588 -0.3349854 ]\n [-0.19320948 -0.19946282 -0.45230883 -0.4529657 ]]\n  fc1.bias - [-0.18395324  0.1342303   0.19837646  0.10616291  0.20710568 -0.14608116\n  0.2194443  -0.34617397  0.47008532  0.06163801]\n  fc2.weight - [[ 0.12124191 -0.30770528  0.00739139  0.03195214  0.22760041  0.287336\n   0.28874013  0.2252406  -0.27528226 -0.2075684 ]\n [ 0.10270181 -0.1418898   0.19800837  0.25484923  0.1979729   0.2595131\n   0.07490984  0.2136883  -0.16789742  0.1169679 ]\n [ 0.24126756  0.12129175  0.1869973   0.2669042   0.2863425   0.08972236\n   0.2191211  -0.21424098 -0.09236753 -0.17675309]]\n  fc2.bias - [-0.04571749  0.03630026 -0.24858725]\nEpoch 3:\n  fc1.weight - [[-0.00749874 -0.12455659  0.10453258 -0.3391503 ]\n [ 0.2679573  -0.31348523  0.19874096  0.20718795]\n [ 0.14652152  0.19296141  0.08028756  0.35375705]\n [ 0.06655406  0.30144316 -0.3543217  -0.00570973]\n [-0.22759274 -0.12539539  0.11119278 -0.23964068]\n [-0.33934474  0.19261773  0.36043224  0.3262374 ]\n [-0.14300722 -0.10308096  0.01150074 -0.2933281 ]\n [ 0.0528243  -0.42043734  0.13057591  0.02542005]\n [ 0.25699186  0.16346753  0.27014744 -0.32967404]\n [-0.19009925 -0.20568086 -0.44711584 -0.44823405]]\n  fc1.bias - [-0.18460873  0.13861382  0.20018597  0.10222913  0.20670162 -0.14823268\n  0.22029303 -0.34782365  0.46948683  0.0600267 ]\n  fc2.weight - [[ 0.12211538 -0.31702578  0.00108523  0.04729255  0.23422316  0.28541544\n   0.29853007  0.22389625 -0.2735836  -0.18490034]\n [ 0.10234685 -0.1418939   0.19466478  0.24553952  0.196229    0.25755367\n   0.07132924  0.21457846 -0.17151727  0.10491561]\n [ 0.24074906  0.13061638  0.19664703  0.26087353  0.28146362  0.09360232\n   0.21291181 -0.21378674 -0.09044631 -0.18736887]]\n  fc2.bias - [-0.04086082  0.03205918 -0.24920283]\nEpoch 4:\n  fc1.weight - [[-0.0070156  -0.12426937  0.10497964 -0.33864865]\n [ 0.27233854 -0.31423858  0.20395195  0.21293941]\n [ 0.1481441   0.19121034  0.08267381  0.35606623]\n [ 0.0704224   0.29820842 -0.34934482 -0.00090744]\n [-0.22765546 -0.12487227  0.11105511 -0.23964387]\n [-0.3411565   0.19283023  0.35825753  0.32369158]\n [-0.14633669 -0.09799027  0.0064654  -0.29784542]\n [ 0.05016455 -0.42024466  0.12784357  0.02254442]\n [ 0.2611137   0.16070011  0.27534443 -0.3244767 ]\n [-0.18761717 -0.21093348 -0.4428947  -0.44446233]]\n  fc1.bias - [-0.1852174   0.14307614  0.20214355  0.09884606  0.20638362 -0.15028188\n  0.22119363 -0.34910992  0.46893287  0.05894179]\n  fc2.weight - [[ 0.12295584 -0.32648316 -0.00527055  0.06175613  0.24068652  0.2835833\n   0.30851665  0.22261928 -0.27270025 -0.16313376]\n [ 0.10199071 -0.141863    0.19147588  0.23683879  0.19452326  0.25569966\n   0.06756973  0.21542552 -0.17436552  0.09339505]\n [ 0.24026474  0.14004286  0.20619167  0.25511062  0.276706    0.09728849\n   0.20668474 -0.21335684 -0.08848142 -0.19761488]]\n  fc2.bias - [-0.03641064  0.02844498 -0.25003883]\nEpoch 5:\n  fc1.weight - [[-0.00660178 -0.12390938  0.10535122 -0.338214  ]\n [ 0.27707732 -0.31482726  0.20948048  0.21886098]\n [ 0.14989723  0.18944502  0.08528852  0.35861275]\n [ 0.07364473  0.29544628 -0.34518746  0.00310412]\n [-0.22782007 -0.12415288  0.1106815  -0.23989175]\n [-0.34274054  0.19286165  0.35631585  0.32144737]\n [-0.14948283 -0.09260581  0.00140825 -0.30241543]\n [ 0.04790385 -0.4200134   0.12530223  0.01990194]\n [ 0.2650705   0.15809086  0.28032696 -0.3195257 ]\n [-0.18588322 -0.21543887 -0.43975222 -0.44166175]]\n  fc1.bias - [-0.18581313  0.14780165  0.20431969  0.09605198  0.20608182 -0.1520227\n  0.2219217  -0.35038772  0.46858075  0.05852823]\n  fc2.weight - [[ 0.12367248 -0.33609638 -0.01182213  0.07502387  0.24622445  0.28163204\n   0.31782722  0.22134253 -0.27267423 -0.14368857]\n [ 0.10171086 -0.1419595   0.18819131  0.22882302  0.1935127   0.25410822\n   0.06429556  0.21638921 -0.1772567   0.08332633]\n [ 0.23982796  0.14975259  0.21602781  0.24985865  0.27217862  0.10083119\n   0.20064834 -0.21304382 -0.08561626 -0.20699133]]\n  fc2.bias - [-0.03357489  0.02595811 -0.2503877 ]\nEpoch 6:\n  fc1.weight - [[-0.00616476 -0.12359847  0.10573377 -0.3377764 ]\n [ 0.2819349  -0.31542188  0.21541083  0.22554465]\n [ 0.15178834  0.18788877  0.0880237   0.36141536]\n [ 0.07658284  0.2931421  -0.34147033  0.00672387]\n [-0.22816618 -0.12335011  0.11013743 -0.240302  ]\n [-0.3443658   0.19287892  0.3542549   0.31898624]\n [-0.1530891  -0.08727307 -0.00396795 -0.30728856]\n [ 0.04565398 -0.41968063  0.12279543  0.01730731]\n [ 0.26897967  0.15563881  0.28529522 -0.31445163]\n [-0.18455644 -0.21904103 -0.43732503 -0.43954268]]\n  fc1.bias - [-0.18637434  0.15295346  0.20668367  0.0936153   0.20592813 -0.15392812\n  0.2229444  -0.3516864   0.46840453  0.05833633]\n  fc2.weight - [[ 0.12441815 -0.34578106 -0.01856968  0.08769812  0.25223753  0.2796422\n   0.32790002  0.22017686 -0.27323976 -0.12404417]\n [ 0.10140788 -0.14285567  0.1841978   0.22120309  0.19200449  0.25202844\n   0.06038447  0.2172164  -0.18005972  0.07310918]\n [ 0.23938525  0.16033345  0.22676888  0.24480434  0.26767373  0.10490085\n   0.19448663 -0.21270533 -0.0822477  -0.21641861]]\n  fc2.bias - [-0.03032879  0.02206276 -0.24973841]\nEpoch 7:\n  fc1.weight - [[-0.00575614 -0.12328612  0.1060895  -0.337359  ]\n [ 0.28697586 -0.31612283  0.22129105  0.23188788]\n [ 0.15380582  0.18656306  0.0906835   0.36406597]\n [ 0.07904322  0.29113674 -0.33830038  0.00982969]\n [-0.22863598 -0.12229145  0.10935935 -0.24093513]\n [-0.34573978  0.19286087  0.3525603   0.31696254]\n [-0.15663913 -0.08142949 -0.0095821  -0.31241292]\n [ 0.04349593 -0.41951445  0.12035713  0.01485495]\n [ 0.27274024  0.1531459   0.29008108 -0.30961835]\n [-0.18379766 -0.22206625 -0.4356688  -0.43814954]]\n  fc1.bias - [-0.18692356  0.15793912  0.20934246  0.09152564  0.2058042  -0.1554885\n  0.22399352 -0.3528463   0.4682098   0.05870643]\n  fc2.weight - [[ 0.12509748 -0.35561264 -0.02506993  0.09985409  0.25784022  0.2779469\n   0.33792195  0.21896015 -0.27404523 -0.10546445]\n [ 0.10114665 -0.1431679   0.18067037  0.21392076  0.19090205  0.25038338\n   0.05660057  0.21821797 -0.1826044   0.06369836]\n [ 0.23896717  0.17047727  0.23679654  0.23993067  0.2631735   0.10824121\n   0.18824862 -0.21249019 -0.07889757 -0.2255875 ]]\n  fc2.bias - [-0.02734786  0.0194198  -0.2500764 ]\nEpoch 8:\n  fc1.weight - [[-0.0053771  -0.12295548  0.10641342 -0.33698002]\n [ 0.29199913 -0.31723973  0.22752507  0.23855735]\n [ 0.15592074  0.1851682   0.0935598   0.36695027]\n [ 0.08112297  0.28932709 -0.33555284  0.01250215]\n [-0.2292111  -0.12115593  0.10843816 -0.24171607]\n [-0.34699792  0.19289492  0.35091025  0.314963  ]\n [-0.16028959 -0.07541289 -0.01543219 -0.3177153 ]\n [ 0.04135266 -0.4190885   0.11781464  0.01248828]\n [ 0.27649334  0.15046015  0.29502422 -0.30467877]\n [-0.18346842 -0.22452028 -0.43463334 -0.43735278]]\n  fc1.bias - [-0.18744346  0.16342917  0.21214654  0.08973119  0.20585671 -0.15705888\n  0.22553709 -0.3541506   0.46810165  0.05937299]\n  fc2.weight - [[ 0.12572262 -0.36537313 -0.03153044  0.11201881  0.26343462  0.27628404\n   0.34834683  0.21779412 -0.27498835 -0.08716375]\n [ 0.10091435 -0.14451596  0.17680389  0.20668729  0.18949333  0.24862164\n   0.05235578  0.21904533 -0.18557046  0.05440805]\n [ 0.23857434  0.18158586  0.24712355  0.23499943  0.2589878   0.11166583\n   0.18206854 -0.21215151 -0.07498836 -0.2345979 ]]\n  fc2.bias - [-0.02397785  0.01523914 -0.24926575]\nEpoch 9:\n  fc1.weight - [[-0.00500653 -0.12262846  0.10673428 -0.33660036]\n [ 0.2962469  -0.3181628   0.23309122  0.24481475]\n [ 0.15786731  0.18379891  0.09637856  0.36987802]\n [ 0.08279289  0.28791156 -0.33335477  0.01465416]\n [-0.22986181 -0.11983965  0.10731724 -0.24268289]\n [-0.34814686  0.19294809  0.34941462  0.31313205]\n [-0.16388422 -0.06922231 -0.02144336 -0.32319814]\n [ 0.03956026 -0.4190189   0.11575245  0.01035552]\n [ 0.27991003  0.14803608  0.29961956 -0.29995546]\n [-0.18361063 -0.22649837 -0.43421462 -0.43714654]]\n  fc1.bias - [-0.18796667  0.16818094  0.21490528  0.08828641  0.2058176  -0.15843782\n  0.22673716 -0.35498643  0.46778587  0.06059134]\n  fc2.weight - [[ 0.1263172  -0.37499735 -0.03791133  0.12317076  0.26864985  0.27460065\n   0.35856026  0.2166958  -0.2765287  -0.0698145 ]\n [ 0.1007015  -0.14385024  0.17452358  0.20020588  0.18873544  0.24715064\n   0.04850796  0.2199706  -0.18620497  0.0458276 ]\n [ 0.23819262  0.19054437  0.25578478  0.23032889  0.2545305   0.11482019\n   0.17570297 -0.21197845 -0.07281353 -0.24336672]]\n  fc2.bias - [-0.02172561  0.01485071 -0.25112954]\nEpoch 10:\n  fc1.weight - [[-0.00463911 -0.12223459  0.10703097 -0.33624285]\n [ 0.3005177  -0.31946692  0.23877464  0.25117394]\n [ 0.15989156  0.18225434  0.09929857  0.37292624]\n [ 0.08410759  0.28676966 -0.33161268  0.01635815]\n [-0.23067155 -0.11840045  0.10604529 -0.24378216]\n [-0.34918997  0.19307949  0.34800863  0.31142083]\n [-0.16777611 -0.06289554 -0.02762947 -0.3287929 ]\n [ 0.03791045 -0.41895524  0.11374982  0.00831501]\n [ 0.28324202  0.14556447  0.30410877 -0.29534617]\n [-0.18411092 -0.22795397 -0.43437976 -0.43749124]]\n  fc1.bias - [-0.18850255  0.17321181  0.21777675  0.0871283   0.2058228  -0.15977265\n  0.2279519  -0.3557788   0.46760434  0.06201579]\n  fc2.weight - [[ 0.12692872 -0.3847937  -0.04414133  0.13371806  0.27381665  0.27309123\n   0.36891264  0.21556938 -0.27849486 -0.05293282]\n [ 0.10048018 -0.14337772  0.17209601  0.1940488   0.18789642  0.24572998\n   0.04451773  0.22094922 -0.18681596  0.0375934 ]\n [ 0.23780245  0.19986817  0.26444232  0.22593865  0.25020275  0.11775026\n   0.16934085 -0.21183068 -0.07023639 -0.2520142 ]]\n  fc2.bias - [-0.01982589  0.01424576 -0.25242427]\nEpoch 11:\n  fc1.weight - [[-0.00431974 -0.12187985  0.10728489 -0.3359315 ]\n [ 0.3054476  -0.32092482  0.2450785   0.25798756]\n [ 0.16220035  0.18092258  0.10239279  0.37609303]\n [ 0.08514719  0.28583533 -0.33021107  0.01773433]\n [-0.23152904 -0.11677034  0.10462726 -0.24503663]\n [-0.3503065   0.19311917  0.34659132  0.30970478]\n [-0.17164482 -0.05622151 -0.03395324 -0.33454004]\n [ 0.03608407 -0.4185368   0.1116148   0.00633222]\n [ 0.2867486   0.14296937  0.30879918 -0.29059795]\n [-0.18490465 -0.22878909 -0.43505743 -0.4383062 ]]\n  fc1.bias - [-0.18897523  0.17876607  0.22092691  0.08617926  0.20598517 -0.16112201\n  0.22950372 -0.3567966   0.46756554  0.0636787 ]\n  fc2.weight - [[ 0.12743376 -0.39473513 -0.05047898  0.14426126  0.278825    0.27166522\n   0.37941277  0.21441214 -0.28069827 -0.03674581]\n [ 0.10031389 -0.14422046  0.16882144  0.18786523  0.1868926   0.24420586\n   0.04019546  0.22177258 -0.18842094  0.02966819]\n [ 0.2374637   0.21065232  0.27405456  0.22157902  0.24619818  0.12070037\n   0.16316296 -0.21149683 -0.06642802 -0.260276  ]]\n  fc2.bias - [-0.01778273  0.01208271 -0.25230438]\nEpoch 12:\n  fc1.weight - [[-0.00401097 -0.12151039  0.10752072 -0.33563855]\n [ 0.3096215  -0.32239988  0.2510708   0.26478177]\n [ 0.16424315  0.17950697  0.10546976  0.37935445]\n [ 0.08592225  0.2851685  -0.32916924  0.01876283]\n [-0.23251393 -0.11497667  0.10302982 -0.24644268]\n [-0.3512014   0.19327652  0.34526205  0.30804938]\n [-0.17568703 -0.04931927 -0.04053182 -0.34048432]\n [ 0.03497338 -0.41854692  0.11007033  0.00464952]\n [ 0.2900204   0.14056525  0.3132556  -0.2859924 ]\n [-0.18590283 -0.229114   -0.43622512 -0.4396421 ]]\n  fc1.bias - [-0.18943487  0.18428552  0.22417283  0.08547454  0.20610186 -0.16247572\n  0.23084658 -0.3572607   0.4674895   0.06552293]\n  fc2.weight - [[ 0.12794939 -0.40454805 -0.05661067  0.154251    0.28366944  0.27020225\n   0.38996404  0.21329181 -0.28318188 -0.02092249]\n [ 0.1001375  -0.14419463  0.16590449  0.1819878   0.18604895  0.24264137\n   0.03584672  0.22275448 -0.18911174  0.02200098]\n [ 0.23712446  0.2204394   0.28310317  0.21746673  0.24219741  0.12372784\n   0.15696047 -0.2113584  -0.0632536  -0.26843208]]\n  fc2.bias - [-0.01627423  0.01099021 -0.2527204 ]\nEpoch 13:\n  fc1.weight - [[-0.00369083 -0.12115962  0.10777611 -0.3353253 ]\n [ 0.3139174  -0.32371128  0.2568079   0.27115527]\n [ 0.16640757  0.17821503  0.10849179  0.3825171 ]\n [ 0.08645682  0.28471032 -0.3284268   0.01950522]\n [-0.23361349 -0.11307456  0.10125841 -0.24803449]\n [-0.35203472  0.19337174  0.34411955  0.30662233]\n [-0.18006419 -0.0422122  -0.04750174 -0.34684527]\n [ 0.03349894 -0.4187014   0.10843374  0.00302165]\n [ 0.29335466  0.13811292  0.3177654  -0.2813653 ]\n [-0.18735327 -0.22906251 -0.43786842 -0.44137752]]\n  fc1.bias - [-0.1899116   0.18931241  0.2272022   0.08494332  0.20633279 -0.16361202\n  0.2325495  -0.3577021   0.4671992   0.06773017]\n  fc2.weight - [[ 0.12843762 -0.41408926 -0.06256296  0.16430946  0.28900814  0.26894492\n   0.4013559   0.21224536 -0.28576946 -0.00459841]\n [ 0.09997606 -0.14370976  0.16359197  0.17607045  0.18500741  0.24132988\n   0.03095675  0.22374295 -0.18935287  0.01400987]\n [ 0.23679766  0.22949578  0.291368    0.21332562  0.23790024  0.1262967\n   0.1504586  -0.2113004  -0.06042488 -0.27676502]]\n  fc2.bias - [-0.01385289  0.01040208 -0.2545536 ]\nEpoch 14:\n  fc1.weight - [[-0.0033999  -0.12071201  0.10797571 -0.33504552]\n [ 0.3180456  -0.32484543  0.26239386  0.27749574]\n [ 0.16853526  0.17697836  0.11150973  0.3857291 ]\n [ 0.08672109  0.28444162 -0.32801524  0.01992447]\n [-0.23470183 -0.11086679  0.09937475 -0.24970385]\n [-0.35278302  0.19344279  0.3430726   0.30528286]\n [-0.18402983 -0.03446963 -0.05438733 -0.35307747]\n [ 0.03226081 -0.41913122  0.10704669  0.00149197]\n [ 0.2965744   0.13557552  0.32223842 -0.27674946]\n [-0.1889893  -0.2285834  -0.4399027  -0.44351125]]\n  fc1.bias - [-0.19041647  0.19414675  0.23018426  0.08463373  0.20645918 -0.16464588\n  0.23408967 -0.35780567  0.46682858  0.07024308]\n  fc2.weight - [[ 0.12884925 -0.42366213 -0.06837233  0.17426027  0.2935411   0.2677164\n   0.41217965  0.21114773 -0.2884446   0.01032987]\n [ 0.09988073 -0.1425122   0.16136973  0.17025448  0.18477868  0.2400115\n   0.02666523  0.22486089 -0.1890706   0.00713855]\n [ 0.23648135  0.2378711   0.29939964  0.20919077  0.23359597  0.12884356\n   0.14392634 -0.21132071 -0.05803202 -0.28482202]]\n  fc2.bias - [-0.01235977  0.0115121  -0.25715673]\nEpoch 15:\n  fc1.weight - [[-3.11012240e-03 -1.20344825e-01  1.08192585e-01 -3.34770083e-01]\n [ 3.21928382e-01 -3.26168954e-01  2.67900705e-01  2.83655941e-01]\n [ 1.70616433e-01  1.75690457e-01  1.14544347e-01  3.88935477e-01]\n [ 8.67489427e-02  2.84386516e-01 -3.27910483e-01  2.00501177e-02]\n [-2.36004442e-01 -1.08771957e-01  9.73588824e-02 -2.51527667e-01]\n [-3.53444010e-01  1.93524018e-01  3.42117190e-01  3.04061323e-01]\n [-1.88534468e-01 -2.72350907e-02 -6.15508370e-02 -3.59679729e-01]\n [ 3.10728140e-02 -4.19395894e-01  1.05666503e-01  2.57243028e-05]\n [ 2.99633801e-01  1.33156836e-01  3.26521546e-01 -2.72354901e-01]\n [-1.90991834e-01 -2.27827147e-01 -4.42339391e-01 -4.45993751e-01]]\n  fc1.bias - [-0.19086422  0.1990026   0.23314351  0.0845237   0.20674704 -0.16559368\n  0.23626803 -0.35805255  0.4665613   0.07289122]\n  fc2.weight - [[ 0.1293238  -0.43298295 -0.07404307  0.18377668  0.2988897   0.2665638\n   0.42401272  0.21011521 -0.29148483  0.02623073]\n [ 0.09971794 -0.14140207  0.1594143   0.16463217  0.18365805  0.23877358\n   0.02136995  0.22588903 -0.18859121 -0.00066192]\n [ 0.23616958  0.24608177  0.3070258   0.20529667  0.22936802  0.13123411\n   0.1373886  -0.21131636 -0.05547115 -0.29292244]]\n  fc2.bias - [-0.01005164  0.01164329 -0.25959608]\nEpoch 16:\n  fc1.weight - [[-0.00286428 -0.11996724  0.10836551 -0.33453882]\n [ 0.3261361  -0.32753262  0.27361932  0.29010165]\n [ 0.17284413  0.17444506  0.11771765  0.3923393 ]\n [ 0.08658476  0.28453457 -0.32806084  0.0199304 ]\n [-0.23721589 -0.10645417  0.09530774 -0.253374  ]\n [-0.35411203  0.19359972  0.34118652  0.302858  ]\n [-0.19271576 -0.01938704 -0.06869641 -0.36619955]\n [ 0.02990935 -0.4196657   0.1043084  -0.001425  ]\n [ 0.30276117  0.13057841  0.33085018 -0.2679147 ]\n [-0.1931035  -0.22647661 -0.44506454 -0.44873276]]\n  fc1.bias - [-0.1912792   0.20408943  0.23620895  0.08458962  0.20707908 -0.16650367\n  0.23822702 -0.35828778  0.46635607  0.07573199]\n  fc2.weight - [[ 0.12968647 -0.44226888 -0.07982045  0.19315097  0.3036103   0.26531655\n   0.43532223  0.2090863  -0.29455262  0.04075055]\n [ 0.09962168 -0.14102113  0.15711395  0.15910372  0.18302214  0.23769975\n   0.01642673  0.22692357 -0.18859228 -0.00760037]\n [ 0.23590314  0.2549868   0.31510356  0.20145085  0.22528332  0.13355519\n   0.1310223  -0.211322   -0.05240232 -0.30050385]]\n  fc2.bias - [-0.00842995  0.01203154 -0.261606  ]\nEpoch 17:\n  fc1.weight - [[-0.00265244 -0.11951677  0.10851183 -0.33432612]\n [ 0.3298812  -0.3290096   0.27933618  0.29663613]\n [ 0.17490017  0.1731018   0.1209512   0.3958494 ]\n [ 0.08622103  0.284815   -0.3284515   0.01958119]\n [-0.23844734 -0.10416795  0.09323873 -0.2552368 ]\n [-0.35472888  0.19372168  0.34024242  0.30165517]\n [-0.19687063 -0.01176575 -0.07572521 -0.3725837 ]\n [ 0.02906505 -0.42009914  0.10309432 -0.00286939]\n [ 0.305398    0.12838265  0.33477533 -0.2637971 ]\n [-0.19524826 -0.22525832 -0.44788006 -0.45157576]]\n  fc1.bias - [-0.19172403  0.2092957   0.23936516  0.08479396  0.20727532 -0.1674644\n  0.23968586 -0.35839316  0.46651044  0.07869064]\n  fc2.weight - [[ 0.1300351  -0.45178404 -0.08555901  0.20157713  0.3079603   0.26410696\n   0.44610968  0.20799366 -0.29828456  0.05443142]\n [ 0.09953134 -0.13994858  0.15500537  0.1541256   0.18265511  0.23647824\n   0.01175209  0.22813223 -0.18775988 -0.0140318 ]\n [ 0.23564486  0.2634294   0.3229507   0.19800285  0.22130038  0.13598622\n   0.12490951 -0.211438   -0.04950279 -0.30775326]]\n  fc2.bias - [-0.00818265  0.01345261 -0.26327437]\nEpoch 18:\n  fc1.weight - [[-0.00241766 -0.11922509  0.10866447 -0.33412778]\n [ 0.3338488  -0.3304024   0.28502315  0.30306163]\n [ 0.17713284  0.17150927  0.1243521   0.39947456]\n [ 0.08570421  0.28528455 -0.32904392  0.0190434 ]\n [-0.2396143  -0.10166211  0.09114783 -0.25712034]\n [-0.3553415   0.19380511  0.33936742  0.30054116]\n [-0.20071742 -0.00350641 -0.08269957 -0.3789342 ]\n [ 0.02814532 -0.4206497   0.10200393 -0.00414609]\n [ 0.30817857  0.12593083  0.3388385  -0.25958586]\n [-0.1974834  -0.22341444 -0.4509581  -0.45465666]]\n  fc1.bias - [-0.19204709  0.21435985  0.24230199  0.08509992  0.20752801 -0.16834249\n  0.24124187 -0.35833538  0.46650052  0.08179263]\n  fc2.weight - [[ 0.13034315 -0.46118474 -0.09112661  0.21017309  0.3119667   0.2630597\n   0.45675004  0.20689833 -0.30185473  0.06729761]\n [ 0.09946292 -0.13890918  0.15281942  0.14908336  0.1825783   0.23527421\n   0.00722961  0.22934799 -0.18707578 -0.0198644 ]\n [ 0.23540525  0.27179068  0.33070424  0.19444914  0.21737085  0.13823748\n   0.11879165 -0.21155846 -0.04661668 -0.31478685]]\n  fc2.bias - [-0.00794145  0.01527064 -0.26533362]\nEpoch 19:\n  fc1.weight - [[-0.00215911 -0.11889075  0.10882556 -0.33392   ]\n [ 0.33738667 -0.33199957  0.29039007  0.30913407]\n [ 0.17916726  0.17022768  0.1274847   0.40285382]\n [ 0.08504102  0.2858615  -0.32984447  0.01829087]\n [-0.24072392 -0.09907284  0.08901317 -0.25908569]\n [-0.3558979   0.19389533  0.3385647   0.29952812]\n [-0.20432921  0.0050413  -0.08976308 -0.38549107]\n [ 0.02711749 -0.4213997   0.10089584 -0.00542571]\n [ 0.31081453  0.12355607  0.3427712  -0.2554985 ]\n [-0.19986857 -0.22148582 -0.45428884 -0.45799512]]\n  fc1.bias - [-0.19239898  0.21921822  0.2451983   0.08556696  0.20772679 -0.16913019\n  0.24253677 -0.3581064   0.46641403  0.08495985]\n  fc2.weight - [[ 0.130644   -0.47028643 -0.09640628  0.21848194  0.3157776   0.26202366\n   0.4673761   0.20573469 -0.30527318  0.07987282]\n [ 0.09941223 -0.13753146  0.15092765  0.14417467  0.18288283  0.23423378\n   0.00284939  0.23067202 -0.18626232 -0.02518114]\n [ 0.2351551   0.27951467  0.3378757   0.19104896  0.2132554   0.14031394\n   0.11254576 -0.21171883 -0.04401167 -0.32204533]]\n  fc2.bias - [-0.00796103  0.01808805 -0.26813143]\nEpoch 20:\n  fc1.weight - [[-0.00192044 -0.11855896  0.10897236 -0.33372298]\n [ 0.3409658  -0.33333272  0.2958644   0.31553122]\n [ 0.18123798  0.16908537  0.13068037  0.40642858]\n [ 0.08420947  0.28660977 -0.3308494   0.01734959]\n [-0.24199472 -0.09653519  0.08678674 -0.26112056]\n [-0.35643545  0.19394131  0.33777118  0.29849252]\n [-0.2085521   0.01350631 -0.09715902 -0.392278  ]\n [ 0.02624148 -0.4219863   0.09985013 -0.00668255]\n [ 0.31351683  0.12115411  0.34678745 -0.25126445]\n [-0.20246983 -0.21918413 -0.45791876 -0.46160722]]\n  fc1.bias - [-0.19274877  0.22418064  0.24812719  0.08618545  0.20815146 -0.16992323\n  0.24443957 -0.3579909   0.4663392   0.08828527]\n  fc2.weight - [[ 0.13092983 -0.47913936 -0.10164953  0.22689418  0.32017913  0.26099774\n   0.47883752  0.20466448 -0.30875432  0.09314087]\n [ 0.09935704 -0.13670164  0.14855467  0.13914795  0.18243146  0.232938\n  -0.00243013  0.23187034 -0.18560913 -0.03133893]\n [ 0.23492445  0.28753778  0.34549192  0.18766344  0.20930523  0.1426356\n   0.10636386 -0.21184695 -0.04118374 -0.3291556 ]]\n  fc2.bias - [-0.0069822   0.01931707 -0.2703393 ]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# 4. Experimentation with Gemini-Pro-1.5-002","metadata":{}},{"cell_type":"markdown","source":"Gemini was utilized to analyze the training process, specifically learning rates, weights, biases, and accuracies per epoch, and to suggest optimizations. The model summary and current metrics were provided to Gemini for further analysis.","metadata":{}},{"cell_type":"markdown","source":"**Gemini Input:**\n\nModel summary, learning rates, weights, biases, and accuracies for 20 epochs.","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngemini_api= user_secrets.get_secret(\"api-1\")\n\ngenai.configure(api_key= gemini_api)\n\ngeneration_config = {\n  \"temperature\": 1,\n  \"top_p\": 0.95,\n  \"top_k\": 40,\n  \"max_output_tokens\": 8192,\n  \"response_mime_type\": \"text/plain\",\n}\n\n\nmodel_bot = genai.GenerativeModel(\n  model_name=\"gemini-1.5-pro-002\",\n  generation_config=generation_config,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-26T17:14:55.526309Z","iopub.execute_input":"2024-11-26T17:14:55.526763Z","iopub.status.idle":"2024-11-26T17:14:55.786945Z","shell.execute_reply.started":"2024-11-26T17:14:55.526728Z","shell.execute_reply":"2024-11-26T17:14:55.785990Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torchinfo import summary\nimport torch\nfrom torch import nn\n\n# Define the feedforward neural network model\nclass FeedforwardNN(nn.Module):\n    def __init__(self):\n        super(FeedforwardNN, self).__init__()\n        self.fc1 = nn.Linear(4, 10)  # 4 input features, 10 hidden units\n        self.fc2 = nn.Linear(10, 3)  # 10 hidden units, 3 output classes\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Initialize the model\nmodel = FeedforwardNN()\n\n# Print the summary of the model\nsummary(model, input_size=(1, 4))  # Note: Use batch size of 1 for input size\nmodel_summary=summary(model, input_size=(1, 4))","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:44:21.853088Z","iopub.execute_input":"2024-11-25T16:44:21.854049Z","iopub.status.idle":"2024-11-25T16:44:22.137211Z","shell.execute_reply.started":"2024-11-25T16:44:21.854010Z","shell.execute_reply":"2024-11-25T16:44:22.136290Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"summary(model, input_size=(1, 4))  # Note: Use batch size of 1 for input size","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:44:36.586993Z","iopub.execute_input":"2024-11-25T16:44:36.587619Z","iopub.status.idle":"2024-11-25T16:44:36.596450Z","shell.execute_reply.started":"2024-11-25T16:44:36.587582Z","shell.execute_reply":"2024-11-25T16:44:36.595498Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nFeedforwardNN                            [1, 3]                    --\n├─Linear: 1-1                            [1, 10]                   50\n├─Linear: 1-2                            [1, 3]                    33\n==========================================================================================\nTotal params: 83\nTrainable params: 83\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n=========================================================================================="},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"\nchat_session = model_bot.start_chat()","metadata":{"execution":{"iopub.status.busy":"2024-11-26T17:16:06.926872Z","iopub.execute_input":"2024-11-26T17:16:06.927617Z","iopub.status.idle":"2024-11-26T17:16:06.931363Z","shell.execute_reply.started":"2024-11-26T17:16:06.927582Z","shell.execute_reply":"2024-11-26T17:16:06.930396Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from google.generativeai.types import HarmCategory, HarmBlockThreshold","metadata":{"execution":{"iopub.status.busy":"2024-11-26T17:16:09.280472Z","iopub.execute_input":"2024-11-26T17:16:09.281137Z","iopub.status.idle":"2024-11-26T17:16:09.284862Z","shell.execute_reply.started":"2024-11-26T17:16:09.281106Z","shell.execute_reply":"2024-11-26T17:16:09.283950Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"**Initial Gemini Prompt:** The model summary, learning rates, weights, biases, and accuracies for 20 epochs were provided to Gemini, requesting recommendations for optimizing learning rates to improve accuracy while reducing the number of epochs needed.","metadata":{}},{"cell_type":"code","source":"message = (\n   f\" i trained a feedforward network model, its model summary is like this:{model_summary}\" \n    f\"its learning rates, weights&biases and accuracies are {learning_rates},{weights_biases}, {accuracies} respectively of my model for 20 epochs\"\n   \"now understand these learning rates, weights and biases, accuracies, and give the optimised list of weights and biases  for my neural network such that i can train the model efficiently in less epochs and gain higher accuracies\" \n)\n\nresponse = chat_session.send_message(message,safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:HarmBlockThreshold.BLOCK_NONE\n        })  # Send the first message\n\n\n\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:45:17.182691Z","iopub.execute_input":"2024-11-25T16:45:17.183535Z","iopub.status.idle":"2024-11-25T16:45:39.390143Z","shell.execute_reply.started":"2024-11-25T16:45:17.183499Z","shell.execute_reply":"2024-11-25T16:45:39.389274Z"},"trusted":true},"outputs":[{"name":"stdout","text":"You're asking for optimized weights and biases that will give you higher accuracy in fewer epochs.  Unfortunately, there's no way to magically determine the *best* weights and biases without further training and experimentation.  Finding the optimal parameters is precisely what the training process does.\n\nHere's why I can't just give you a list of optimal values:\n\n* **Training is a search:** Neural network training is a search for the best combination of weights and biases within a vast, complex landscape.  There are no analytical solutions to find the global optimum.  Algorithms like gradient descent help us navigate this landscape, but they don't guarantee finding the absolute best solution.\n\n* **Data dependence:** The optimal weights and biases are heavily dependent on your specific dataset. What works well for one dataset might be terrible for another.\n\n* **Network architecture:** Your model summary shows a simple two-layer feedforward network.  Even minor changes to the architecture (number of layers, neurons per layer, activation functions) would require a completely different set of optimal weights and biases.\n\n* **Overfitting:**  While you want higher accuracy, you also need to be cautious of overfitting.  Overfitting occurs when a model performs very well on the training data but poorly on unseen data.  Simply seeking the highest possible training accuracy can lead to overfitting.\n\n**How to improve your model:**\n\nInstead of looking for a shortcut to the best weights, focus on improving your training process and model architecture. Here's how:\n\n1. **Data preprocessing:**  Ensure your data is properly preprocessed. This may involve scaling features, handling missing values, or performing dimensionality reduction.\n\n2. **Hyperparameter tuning:**  Experiment with different learning rates, batch sizes, and optimizers.  A learning rate scheduler can help dynamically adjust the learning rate during training.  Consider techniques like cross-validation to evaluate your model's performance with different hyperparameters.\n\n3. **Network architecture:** Consider adding more layers or neurons to your network if it's underfitting (not achieving sufficient accuracy).  Experiment with different activation functions.\n\n4. **Regularization:**  Use regularization techniques like dropout or L1/L2 regularization to prevent overfitting.\n\n5. **Different initialization strategies:** Experiment with different weight initialization methods.  The default initialization might not be the best for your specific problem.\n\n6. **More advanced optimizers:**  Consider using optimizers like Adam, RMSprop, or SGD with momentum, which often converge faster and to better solutions than basic gradient descent.\n\n7. **Early stopping:** Monitor your validation accuracy and stop training when it starts to decrease, preventing overfitting.\n\n8. **Increase epochs (carefully):** While you want to train efficiently, sometimes increasing the number of epochs is necessary, especially if the learning rate is low or the model is complex.  Combine this with early stopping to prevent overfitting.\n\n\n\n**Using your existing training data:**\n\nYou can use the weights from the epoch that produced the highest validation accuracy as a starting point. This is sometimes referred to as \"warm starting.\" However, you'll still need to continue training and refining those weights.\n\nIn summary, there's no easy way to obtain the \"perfect\" weights and biases.  Focus on systematic experimentation and improvement of your training process and model design.\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"**Gemini Recommendations:**\n\nLearning Rate Adjustments: Recommended reducing the learning rate decay interval and applying adaptive adjustments.","metadata":{}},{"cell_type":"code","source":"# Define the model\nclass FeedforwardNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(FeedforwardNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x)) # Use ReLU activation\n        x = self.fc2(x)\n        return x\n# Create the model\ninput_size = 4  # Example input size\nhidden_size = 10\noutput_size = 3\nmodel = FeedforwardNN(input_size, hidden_size, output_size)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()  # Appropriate for classification\noptimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001) # Adam with weight decay\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) # Decay LR every 5 epochs\n\n# Track accuracies, learning rates, weights, and biases during training\nnum_epochs = 20\naccuracies = []\nlearning_rates = []\nweights_biases = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    correct = 0\n    total = 0\n\n    # Track current learning rate\n    for param_group in optimizer.param_groups:\n        current_lr = param_group['lr']\n    learning_rates.append(current_lr)\n\n    # Track weights and biases for each layer\n    epoch_weights_biases = {}\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            epoch_weights_biases[name] = param.data.clone().detach().numpy()\n    weights_biases.append(epoch_weights_biases)\n\n    for inputs, labels in train_loader:\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Calculate accuracy\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    # Calculate epoch accuracy and append to accuracies list\n    accuracy = 100 * correct / total\n    accuracies.append(accuracy)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%, Learning Rate: {current_lr}')\n    scheduler.step() \n\n# Print all recorded accuracies, learning rates, weights, and biases\nprint(\"\\nTraining accuracies for each epoch:\", accuracies)\nprint(\"\\nLearning rates for each epoch:\", learning_rates)\nprint(\"\\nWeights and biases per epoch:\")\nfor epoch_idx, params in enumerate(weights_biases):\n    print(f\"Epoch {epoch_idx + 1}:\")\n    for name, values in params.items():\n        print(f\"  {name} - {values}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:46:17.447176Z","iopub.execute_input":"2024-11-25T16:46:17.447556Z","iopub.status.idle":"2024-11-25T16:46:17.684362Z","shell.execute_reply.started":"2024-11-25T16:46:17.447525Z","shell.execute_reply":"2024-11-25T16:46:17.683402Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch [1/20], Loss: 0.8439, Accuracy: 33.33%, Learning Rate: 0.01\nEpoch [2/20], Loss: 0.7606, Accuracy: 50.83%, Learning Rate: 0.01\nEpoch [3/20], Loss: 0.7583, Accuracy: 71.67%, Learning Rate: 0.01\nEpoch [4/20], Loss: 0.8339, Accuracy: 77.50%, Learning Rate: 0.01\nEpoch [5/20], Loss: 0.5383, Accuracy: 79.17%, Learning Rate: 0.01\nEpoch [6/20], Loss: 0.5743, Accuracy: 82.50%, Learning Rate: 0.001\nEpoch [7/20], Loss: 0.5265, Accuracy: 82.50%, Learning Rate: 0.001\nEpoch [8/20], Loss: 0.4188, Accuracy: 82.50%, Learning Rate: 0.001\nEpoch [9/20], Loss: 0.3450, Accuracy: 82.50%, Learning Rate: 0.001\nEpoch [10/20], Loss: 0.5239, Accuracy: 82.50%, Learning Rate: 0.001\nEpoch [11/20], Loss: 0.2720, Accuracy: 82.50%, Learning Rate: 0.0001\nEpoch [12/20], Loss: 0.4621, Accuracy: 82.50%, Learning Rate: 0.0001\nEpoch [13/20], Loss: 0.4313, Accuracy: 82.50%, Learning Rate: 0.0001\nEpoch [14/20], Loss: 0.3761, Accuracy: 82.50%, Learning Rate: 0.0001\nEpoch [15/20], Loss: 0.6362, Accuracy: 82.50%, Learning Rate: 0.0001\nEpoch [16/20], Loss: 0.4585, Accuracy: 82.50%, Learning Rate: 1e-05\nEpoch [17/20], Loss: 0.2745, Accuracy: 82.50%, Learning Rate: 1e-05\nEpoch [18/20], Loss: 0.4280, Accuracy: 82.50%, Learning Rate: 1e-05\nEpoch [19/20], Loss: 0.4713, Accuracy: 82.50%, Learning Rate: 1e-05\nEpoch [20/20], Loss: 0.3076, Accuracy: 82.50%, Learning Rate: 1e-05\n\nTraining accuracies for each epoch: [33.333333333333336, 50.833333333333336, 71.66666666666667, 77.5, 79.16666666666667, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5]\n\nLearning rates for each epoch: [0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05]\n\nWeights and biases per epoch:\nEpoch 1:\n  fc1.weight - [[ 0.45469278  0.37332112 -0.06501061 -0.43498152]\n [ 0.37914747 -0.26851493  0.0280875   0.08471656]\n [-0.12198448 -0.37763673  0.45860708  0.290209  ]\n [-0.19056535 -0.33949333 -0.3853317   0.39567816]\n [ 0.3171646  -0.16610461 -0.30930483 -0.15665203]\n [-0.12075853 -0.03870606 -0.3676439  -0.30211717]\n [ 0.0761472  -0.15705574 -0.16921061  0.13668919]\n [-0.3361122   0.3782695  -0.09649128  0.21606421]\n [-0.02348703 -0.00930852 -0.2839865   0.25019854]\n [-0.16845608  0.23971355 -0.2074545  -0.08719969]]\n  fc1.bias - [-0.2186405  -0.07786429 -0.17633557 -0.28072459  0.23617178  0.4587633\n -0.3469947  -0.33112085 -0.10531348  0.32098   ]\n  fc2.weight - [[-0.02255402 -0.08919404  0.04746307  0.01112255 -0.06617152  0.05873822\n  -0.09731457 -0.06979819 -0.17213094  0.26539072]\n [ 0.31468365  0.15046269 -0.19842201 -0.19742145 -0.24068259 -0.19413334\n  -0.27796513  0.19215743  0.2893648  -0.0840487 ]\n [-0.30505016 -0.306442   -0.20896733 -0.0848261   0.14132065  0.13503417\n   0.13032563 -0.09855937 -0.25870302 -0.19732901]]\n  fc2.bias - [-0.07693753 -0.29864198 -0.22674768]\nEpoch 2:\n  fc1.weight - [[ 0.41478696  0.3280474  -0.10698666 -0.47610286]\n [ 0.3105059  -0.3281131  -0.0423631   0.01248313]\n [-0.1926264  -0.30286938  0.38418084  0.21714184]\n [-0.15587911 -0.27115816 -0.41704258  0.352569  ]\n [ 0.3874443  -0.10256218 -0.23607151 -0.08130631]\n [-0.15749525  0.03647441 -0.4454763  -0.37642476]\n [ 0.0021836  -0.07880642 -0.09077945  0.05885303]\n [-0.281595    0.31353152 -0.04547469  0.2559979 ]\n [-0.04719663 -0.05070022 -0.29904664  0.24321774]\n [-0.24338329  0.3178342  -0.2835901  -0.1628218 ]]\n  fc1.bias - [-0.26420808 -0.08840042 -0.25244492 -0.34969994  0.17899418  0.38853413\n -0.26760203 -0.39586988 -0.10568511  0.37614378]\n  fc2.weight - [[ 0.04522411 -0.1664697  -0.02918737 -0.03186389 -0.06398361  0.12964627\n  -0.02112914 -0.00565389 -0.14443928  0.34068194]\n [ 0.24601682  0.14822182 -0.1915679  -0.14136592 -0.19535603 -0.22669148\n  -0.19876556  0.12814753  0.2545731  -0.15500882]\n [-0.3650203  -0.23583485 -0.14106974 -0.13493092  0.08036739  0.0588551\n   0.05265841 -0.16241264 -0.28439    -0.2742549 ]]\n  fc2.bias - [-0.13602081 -0.27347475 -0.19705364]\nEpoch 3:\n  fc1.weight - [[ 0.37243012  0.34935653 -0.16287747 -0.5343884 ]\n [ 0.26181233 -0.38028964 -0.09392704 -0.04492656]\n [-0.21598628 -0.24157709  0.33670866  0.1732016 ]\n [-0.1530935  -0.22868243 -0.463352    0.30488974]\n [ 0.4496655  -0.06153254 -0.16585667 -0.0110399 ]\n [-0.197429    0.10757511 -0.52440596 -0.4539208 ]\n [-0.02478181 -0.01325156 -0.02347883 -0.00262948]\n [-0.2773836   0.27749673 -0.0486921   0.23684412]\n [-0.06825599 -0.09107318 -0.3188015   0.20592047]\n [-0.31574282  0.39214122 -0.35806787 -0.23655543]]\n  fc1.bias - [-0.22908083 -0.04415759 -0.30817413 -0.3863989   0.15034278  0.33484688\n -0.19190808 -0.43088064 -0.10401249  0.4322813 ]\n  fc2.weight - [[ 0.11462443 -0.23543306 -0.09192504 -0.06138804 -0.11589076  0.2002818\n   0.02408207  0.03304422 -0.10122207  0.41452402]\n [ 0.18006006  0.18538855 -0.18870907 -0.10766444 -0.1506102  -0.2754172\n  -0.12461746  0.08701408  0.22822118 -0.22504224]\n [-0.43231216 -0.17885344 -0.08225755 -0.15908769  0.094296   -0.01283291\n  -0.00713283 -0.19738252 -0.30387574 -0.34904957]]\n  fc2.bias - [-0.19768822 -0.2211937  -0.17292702]\nEpoch 4:\n  fc1.weight - [[ 0.32986176  0.3764261  -0.21864387 -0.5943738 ]\n [ 0.23301834 -0.44009805 -0.11949669 -0.08481853]\n [-0.19380797 -0.19850343  0.3318259   0.17518274]\n [-0.16267331 -0.20614362 -0.49962607  0.26361066]\n [ 0.51640874 -0.03770947 -0.0887192   0.06686086]\n [-0.23731408  0.17626527 -0.5980454  -0.5264112 ]\n [-0.00390343  0.02311595  0.01802877 -0.02799589]\n [-0.2818131   0.26789135 -0.06039288  0.21727197]\n [-0.0989575  -0.15132828 -0.32927293  0.17715086]\n [-0.3796093   0.4555007  -0.42365754 -0.301792  ]]\n  fc1.bias - [-0.17531994  0.01897166 -0.33300182 -0.3963392   0.17326318  0.29056492\n -0.12405396 -0.4373746  -0.06507768  0.47154355]\n  fc2.weight - [[ 0.17909533 -0.30109072 -0.13656697 -0.0771557  -0.19401523  0.2612929\n   0.01917923  0.05490878 -0.05895748  0.47892073]\n [ 0.11771567  0.24091795 -0.18677145 -0.08205897 -0.12386585 -0.32700548\n  -0.06156917  0.06261086  0.24588922 -0.28715485]\n [-0.49389696 -0.14203759 -0.04028226 -0.18437739  0.15941413 -0.07130671\n  -0.02836573 -0.2151226  -0.36725166 -0.41325468]]\n  fc2.bias - [-0.2708432  -0.17230263 -0.13313842]\nEpoch 5:\n  fc1.weight - [[ 0.29070953  0.40315259 -0.26902547 -0.6486367 ]\n [ 0.21445331 -0.5081881  -0.12432221 -0.10827116]\n [-0.13580652 -0.17474951  0.37440625  0.22404762]\n [-0.1757593  -0.1978633  -0.5273809   0.2281357 ]\n [ 0.59301233 -0.02344639 -0.00135336  0.15368928]\n [-0.2649858   0.23763224 -0.66129076 -0.5891274 ]\n [ 0.01073127  0.02574187  0.02724146 -0.01925323]\n [-0.28751543  0.2681973  -0.07303962  0.19993538]\n [-0.13654637 -0.22198422 -0.35692707  0.12416537]\n [-0.42661014  0.50741774 -0.47701696 -0.35529417]]\n  fc1.bias - [-0.1225618   0.09372035 -0.3201962  -0.39203656  0.23463753  0.24920201\n -0.06812461 -0.4312568  -0.00341878  0.48699278]\n  fc2.weight - [[ 0.2370085  -0.36693788 -0.16833606 -0.07831862 -0.28126243  0.30940998\n  -0.00337012  0.06706353 -0.06263675  0.5292732 ]\n [ 0.06006339  0.31236184 -0.20133968 -0.06491347 -0.12557548 -0.3737248\n  -0.01544814  0.0481256   0.30587366 -0.33400992]\n [-0.54735094 -0.12716465 -0.00414238 -0.21057358  0.24402276 -0.11475718\n  -0.01624595 -0.22304757 -0.44308585 -0.46401668]]\n  fc2.bias - [-0.34596187 -0.12378562 -0.08838393]\nEpoch 6:\n  fc1.weight - [[ 0.26603237  0.42870256 -0.30480292 -0.6877579 ]\n [ 0.20102194 -0.57609487 -0.11035025 -0.12412346]\n [-0.05748769 -0.16800034  0.4458001   0.30067348]\n [-0.20083813 -0.20352778 -0.5460875   0.19964671]\n [ 0.6697076  -0.02183147  0.08688292  0.24325474]\n [-0.26350015  0.290948   -0.7093192  -0.63184977]\n [-0.01539067  0.00075451  0.0061035  -0.01170544]\n [-0.2905017   0.26990893 -0.08309525  0.18601422]\n [-0.17903507 -0.2896706  -0.3862407   0.06313451]\n [-0.45212764  0.54841155 -0.5183186  -0.39603892]]\n  fc1.bias - [-0.07304836  0.17602244 -0.27768293 -0.37307465  0.3049606   0.20530955\n -0.02938037 -0.4207985   0.06291584  0.46385625]\n  fc2.weight - [[ 0.28178224 -0.4334588  -0.19986708 -0.07448682 -0.3724967   0.34227896\n  -0.02935489  0.07378732 -0.10353509  0.56148684]\n [ 0.01397348  0.3866826  -0.25056693 -0.05016998 -0.13925931 -0.40478784\n  -0.00402327  0.03924647  0.37778082 -0.35101134]\n [-0.5862991  -0.12722385  0.04586894 -0.23812605  0.32733536 -0.14447217\n   0.01963135 -0.2253985  -0.52030903 -0.50333416]]\n  fc2.bias - [-0.41917512 -0.06986144 -0.05488894]\nEpoch 7:\n  fc1.weight - [[ 0.26418555  0.43069327 -0.30756015 -0.69076407]\n [ 0.20158912 -0.582138   -0.10780276 -0.12453384]\n [-0.04956526 -0.16783243  0.45408413  0.30960482]\n [-0.20260756 -0.20435004 -0.5479506   0.19670877]\n [ 0.6766861  -0.02206816  0.09543718  0.25216833]\n [-0.26444298  0.29470038 -0.71323454 -0.6356663 ]\n [-0.0121901  -0.00111711  0.00672658 -0.00794847]\n [-0.2907205   0.27010325 -0.08400008  0.18473612]\n [-0.18232323 -0.29508728 -0.3889184   0.05711881]\n [-0.45421535  0.55123514 -0.5216287  -0.39935213]]\n  fc1.bias - [-0.06873997  0.18404494 -0.2722755  -0.37115836  0.3117915   0.20288461\n -0.02746638 -0.41949943  0.06984757  0.46197402]\n  fc2.weight - [[ 0.2854329  -0.43976468 -0.20368145 -0.07242095 -0.3813079   0.34497157\n  -0.02825704  0.07421497 -0.10708779  0.56407374]\n [ 0.00992616  0.39365634 -0.25844774 -0.04968784 -0.14074281 -0.40817943\n  -0.00378116  0.0385943   0.3847716  -0.35290703]\n [-0.5890388  -0.12844643  0.05285391 -0.24135496  0.33514315 -0.14647286\n   0.02113198 -0.22534724 -0.5284838  -0.50613254]]\n  fc2.bias - [-0.42532495 -0.06512795 -0.05247566]\nEpoch 8:\n  fc1.weight - [[ 0.2628382   0.43137693 -0.3100974  -0.6936357 ]\n [ 0.20330623 -0.58731055 -0.10472184 -0.12492488]\n [-0.04212058 -0.16859275  0.46206763  0.31819287]\n [-0.20421551 -0.20548838 -0.54945236  0.19382504]\n [ 0.68272555 -0.02332715  0.10321283  0.26042032]\n [-0.26615745  0.29743832 -0.7168724  -0.6393186 ]\n [-0.00933124 -0.00187634  0.00683836 -0.00528676]\n [-0.29073757  0.27026105 -0.084787    0.18353201]\n [-0.18574524 -0.29996404 -0.39029485  0.05142746]\n [-0.45628116  0.55305195 -0.52466244 -0.4023253 ]]\n  fc1.bias - [-0.06353484  0.19192857 -0.26733327 -0.36908486  0.3171195   0.20181362\n -0.02591574 -0.4180893   0.07693093  0.46045494]\n  fc2.weight - [[ 0.28867778 -0.44555977 -0.20795204 -0.06946652 -0.3898338   0.3474476\n  -0.02505952  0.07449497 -0.10851467  0.5663851 ]\n [ 0.00621383  0.40036002 -0.26707557 -0.05015764 -0.14077452 -0.41191787\n  -0.00355187  0.03808534  0.39045727 -0.35493252]\n [-0.59127176 -0.13007851  0.06059585 -0.2446821   0.34152886 -0.14794253\n   0.02095193 -0.22511879 -0.53671175 -0.50840193]]\n  fc2.bias - [-0.43091968 -0.060117   -0.05154662]\nEpoch 9:\n  fc1.weight - [[ 0.26122862  0.43145424 -0.31271553 -0.6965437 ]\n [ 0.20509492 -0.59291613 -0.10125866 -0.12475146]\n [-0.03452848 -0.16879262  0.47003657  0.3264672 ]\n [-0.20475411 -0.20629841 -0.5510387   0.19058877]\n [ 0.68864256 -0.02375405  0.11063055  0.2681475 ]\n [-0.26838312  0.29969746 -0.720347   -0.64284855]\n [-0.00719739 -0.00207596  0.00642106 -0.00360224]\n [-0.29062176  0.27033502 -0.08546852  0.18238269]\n [-0.18845093 -0.3057662  -0.3893972   0.0469397 ]\n [-0.4581646   0.55460715 -0.5276158  -0.40521273]]\n  fc1.bias - [-0.05863122  0.20003673 -0.26274613 -0.36748183  0.32138473  0.2013208\n -0.02445893 -0.41662788  0.08512171  0.45838946]\n  fc2.weight - [[ 0.29179987 -0.45153138 -0.21251813 -0.06617685 -0.3981228   0.34980747\n  -0.02113158  0.07470452 -0.11021236  0.5684971 ]\n [ 0.00249016  0.40768567 -0.27577734 -0.0511803  -0.14039855 -0.4157832\n  -0.00331932  0.03763886  0.39648643 -0.3566545 ]\n [-0.59318024 -0.13283312  0.06853952 -0.24770427  0.34734958 -0.1491513\n   0.01994205 -0.22479007 -0.54502404 -0.510544  ]]\n  fc2.bias - [-0.43637866 -0.05467688 -0.05163368]\nEpoch 10:\n  fc1.weight - [[ 0.25936696  0.4316121  -0.3154202  -0.6994621 ]\n [ 0.2056604  -0.59778124 -0.09875737 -0.12537165]\n [-0.02723941 -0.1703717   0.4781697   0.33507484]\n [-0.2050324  -0.20691846 -0.55285716  0.1871758 ]\n [ 0.69498795 -0.02535962  0.11822157  0.27607563]\n [-0.2704496   0.30190203 -0.72358626 -0.64629483]\n [-0.00555101 -0.00198977  0.00576151 -0.00249954]\n [-0.29049328  0.27047464 -0.08621816  0.18116218]\n [-0.19119066 -0.31010374 -0.38922554  0.04224361]\n [-0.4597343   0.5564368  -0.5303849  -0.40796477]]\n  fc1.bias - [-0.05410009  0.20730951 -0.25774863 -0.3660094   0.32662174  0.20110294\n -0.02304772 -0.41502786  0.09265944  0.45633778]\n  fc2.weight - [[ 0.2950112  -0.45760432 -0.21741295 -0.06323887 -0.40600383  0.3520834\n  -0.01716203  0.07493126 -0.11265418  0.5705737 ]\n [-0.00143526  0.41351706 -0.2849372  -0.05193249 -0.14198796 -0.41960225\n  -0.00308668  0.03714966  0.40211812 -0.3585472 ]\n [-0.59498686 -0.13378577  0.07692526 -0.25069487  0.3539516  -0.15024494\n   0.01854314 -0.22442965 -0.5515845  -0.5125099 ]]\n  fc2.bias - [-0.44158003 -0.05033626 -0.05070339]\nEpoch 11:\n  fc1.weight - [[ 0.25857887  0.43266523 -0.31742546 -0.70178205]\n [ 0.20659283 -0.60322154 -0.09703493 -0.12680855]\n [-0.0208076  -0.17030798  0.48643827  0.34401754]\n [-0.20577927 -0.2082341  -0.55429363  0.18397133]\n [ 0.7010506  -0.02533242  0.12606373  0.28435224]\n [-0.27147344  0.30446947 -0.7262188  -0.6490541 ]\n [-0.00425583 -0.00177042  0.00501687 -0.00175454]\n [-0.29034513  0.2706096  -0.08699027  0.17988788]\n [-0.19381586 -0.31516555 -0.38962075  0.03755838]\n [-0.46092063  0.558504   -0.53268033 -0.41027   ]]\n  fc1.bias - [-0.04942379  0.21438405 -0.25312138 -0.36387402  0.33213317  0.20053229\n -0.02166448 -0.41334647  0.09944952  0.45450062]\n  fc2.weight - [[ 0.29811352 -0.46393877 -0.22272348 -0.06136727 -0.41394866  0.35405207\n  -0.01350331  0.07516751 -0.11634464  0.5724337 ]\n [-0.00517674  0.42002714 -0.29423296 -0.05185211 -0.14362454 -0.42282328\n  -0.00285668  0.03663003  0.40865445 -0.3602876 ]\n [-0.59677166 -0.13574208  0.0855995  -0.25347775  0.36054385 -0.15121837\n   0.01698725 -0.22403716 -0.5584217  -0.51423055]]\n  fc2.bias - [-0.4468068  -0.04624933 -0.049468  ]\nEpoch 12:\n  fc1.weight - [[ 0.25844505  0.4327064  -0.3176622  -0.70204604]\n [ 0.20665874 -0.6037902  -0.09674271 -0.12688276]\n [-0.02030679 -0.17041413  0.48710236  0.3447811 ]\n [-0.20582896 -0.20835578 -0.55448925  0.18360022]\n [ 0.7015831  -0.02542371  0.12673418  0.28508335]\n [-0.27159497  0.3047179  -0.7265005  -0.64935404]\n [-0.00415109 -0.00174316  0.00494132 -0.00170179]\n [-0.29031688  0.27061555 -0.08705912  0.17976348]\n [-0.19417843 -0.315665   -0.38952622  0.03710691]\n [-0.46103305  0.55869585 -0.532946   -0.41054112]]\n  fc1.bias - [-0.04897366  0.21517101 -0.25294477 -0.3636777   0.33249015  0.20047793\n -0.02152777 -0.41317663  0.10020982  0.4542082 ]\n  fc2.weight - [[ 0.29843175 -0.4646031  -0.2232562  -0.06114978 -0.41471723  0.35426104\n  -0.01317799  0.07518739 -0.11676152  0.57261777]\n [-0.00556846  0.42070958 -0.29505345 -0.05185514 -0.14359945 -0.4231647\n  -0.002834    0.03658196  0.40931636 -0.3604054 ]\n [-0.5969416  -0.13598701  0.0864007  -0.25379688  0.36106083 -0.15131892\n   0.01682682 -0.22399284 -0.5590796  -0.51443654]]\n  fc2.bias - [-0.44735402 -0.04568922 -0.0495389 ]\nEpoch 13:\n  fc1.weight - [[ 0.2582755   0.43262503 -0.31795844 -0.70237213]\n [ 0.20687923 -0.6042009  -0.09649376 -0.12697722]\n [-0.01982092 -0.17053458  0.4878287   0.34559715]\n [-0.20586061 -0.2084353  -0.55472255  0.18320525]\n [ 0.7021088  -0.02555374  0.12745225  0.28585184]\n [-0.27189118  0.30486163 -0.72688407 -0.6497627 ]\n [-0.00405829 -0.00171327  0.00486483 -0.00165952]\n [-0.2902991   0.27062404 -0.08713923  0.17962764]\n [-0.19433725 -0.31604576 -0.38951403  0.03662724]\n [-0.46126062  0.55878    -0.5332567  -0.41085315]]\n  fc1.bias - [-0.04842995  0.21581525 -0.25266343 -0.36351308  0.33295554  0.20053154\n -0.02138965 -0.41299456  0.10074361  0.45412007]\n  fc2.weight - [[ 0.2987713  -0.4651602  -0.22381875 -0.06076051 -0.41548973  0.35452363\n  -0.01287352  0.07520997 -0.11684352  0.57285845]\n [-0.00598286  0.42124817 -0.29591402 -0.05202108 -0.14365551 -0.42360744\n  -0.00281114  0.03652893  0.40973032 -0.36066937]\n [-0.5971254  -0.13614388  0.08723594 -0.25415078  0.36162037 -0.15142758\n   0.01666222 -0.22394699 -0.5596497  -0.51462805]]\n  fc2.bias - [-0.44785154 -0.04527579 -0.04947892]\nEpoch 14:\n  fc1.weight - [[ 0.25811678  0.4326327  -0.31823462 -0.70267385]\n [ 0.20707154 -0.6047053  -0.09617854 -0.12701763]\n [-0.01937219 -0.17048728  0.48848674  0.3463517 ]\n [-0.20585106 -0.20848013 -0.5549121   0.18284126]\n [ 0.7026203  -0.02551723  0.12811089  0.28657302]\n [-0.27215672  0.30505478 -0.7272308  -0.6501182 ]\n [-0.00397009 -0.00168177  0.00478686 -0.00162171]\n [-0.2902926   0.27064025 -0.08723272  0.17947944]\n [-0.19453694 -0.31650582 -0.3893767   0.0362331 ]\n [-0.46144477  0.55891997 -0.5335586  -0.41115144]]\n  fc1.bias - [-0.04794168  0.216568   -0.2525163  -0.36337414  0.33327618  0.20054512\n -0.02124833 -0.4127974   0.10143011  0.4539115 ]\n  fc2.weight - [[ 0.2991246  -0.4657521  -0.22436719 -0.06044132 -0.41624406  0.35477144\n  -0.01257651  0.07523642 -0.11703095  0.5730771 ]\n [-0.00641421  0.42190555 -0.29669216 -0.05215982 -0.14361955 -0.42401868\n  -0.00278778  0.03646987  0.41024703 -0.36084777]\n [-0.5973144  -0.13645697  0.08800924 -0.2544263   0.36211026 -0.15153192\n   0.01649304 -0.223901   -0.56026816 -0.51484376]]\n  fc2.bias - [-0.44837105 -0.04474343 -0.04956253]\nEpoch 15:\n  fc1.weight - [[ 0.2579812   0.4326312  -0.31850863 -0.70298606]\n [ 0.20735656 -0.60525817 -0.09578853 -0.12701073]\n [-0.01907628 -0.17044632  0.4890744   0.34706563]\n [-0.20590222 -0.2085713  -0.5551386   0.18244411]\n [ 0.7030413  -0.02549224  0.12872973  0.28726533]\n [-0.27238384  0.30527043 -0.7275855  -0.650492  ]\n [-0.00388325 -0.0016492   0.00470729 -0.00158563]\n [-0.29027304  0.27064958 -0.08731808  0.17933291]\n [-0.19462554 -0.31700122 -0.3892412   0.03580384]\n [-0.46164083  0.5590765  -0.533858   -0.4114504 ]]\n  fc1.bias - [-0.04740817  0.21731398 -0.2524326  -0.3631943   0.3335719   0.20054786\n -0.02110317 -0.41260046  0.10207587  0.45376736]\n  fc2.weight - [[ 0.2994884  -0.46637893 -0.2249327  -0.06013054 -0.4169922   0.3550295\n  -0.0122813   0.07526085 -0.1172404   0.5733162 ]\n [-0.00685399  0.4225993  -0.2973899  -0.05225467 -0.14347474 -0.42443573\n  -0.00276381  0.0364124   0.41078243 -0.3610887 ]\n [-0.59751356 -0.13681436  0.08873882 -0.2547803   0.36251903 -0.15164518\n   0.01631929 -0.22385116 -0.5608837  -0.5150464 ]]\n  fc2.bias - [-0.44889307 -0.04419868 -0.0496707 ]\nEpoch 16:\n  fc1.weight - [[ 0.25781995  0.43266818 -0.31878293 -0.7032868 ]\n [ 0.2074395  -0.6057478  -0.0956196  -0.12718685]\n [-0.01863378 -0.17052421  0.48977685  0.34786597]\n [-0.20592614 -0.20865189 -0.5553734   0.18205063]\n [ 0.7035829  -0.02558733  0.1294396   0.2880311 ]\n [-0.27253887  0.30552122 -0.7278912  -0.6508283 ]\n [-0.0037964  -0.00161586  0.00462617 -0.00155008]\n [-0.29027057  0.27066502 -0.08741909  0.1791696 ]\n [-0.19487338 -0.31734544 -0.38935256  0.03523339]\n [-0.46177813  0.5592853  -0.5341347  -0.41173548]]\n  fc1.bias - [-0.04695171  0.21797164 -0.2521912  -0.36301583  0.33402815  0.20050219\n -0.020954   -0.41238815  0.1026115   0.45356518]\n  fc2.weight - [[ 0.2998549  -0.46702757 -0.22552876 -0.05990126 -0.41773736  0.35526788\n  -0.01198573  0.07528626 -0.11755965  0.5735375 ]\n [-0.00730017  0.4231636  -0.29819822 -0.05228464 -0.14357735 -0.42480212\n  -0.0027392   0.03635269  0.41131648 -0.36128658]\n [-0.5977074  -0.1369743   0.08954933 -0.25511217  0.36308822 -0.15175924\n   0.01614122 -0.22379941 -0.56139845 -0.5152514 ]]\n  fc2.bias - [-0.44942942 -0.04376587 -0.04961915]\nEpoch 17:\n  fc1.weight - [[ 0.2578094   0.43267056 -0.31880906 -0.7033168 ]\n [ 0.20745642 -0.6057925  -0.0956005  -0.12720114]\n [-0.01859849 -0.17053908  0.4898406   0.34793788]\n [-0.20593125 -0.2086624  -0.555398    0.18200983]\n [ 0.70363075 -0.02560629  0.12950651  0.28810197]\n [-0.27255976  0.3055413  -0.7279251  -0.65086555]\n [-0.00378764 -0.00161246  0.00461791 -0.00154652]\n [-0.29026976  0.27066675 -0.08742939  0.17915292]\n [-0.19489008 -0.31738144 -0.3893543   0.03518762]\n [-0.46179554  0.55930173 -0.5341646  -0.4117662 ]]\n  fc1.bias - [-0.04689781  0.21803781 -0.25217897 -0.36299703  0.33406675  0.20050363\n -0.02093868 -0.41236633  0.10266618  0.45354822]\n  fc2.weight - [[ 0.29989156 -0.4670906  -0.22558813 -0.0598747  -0.41781354  0.35529342\n  -0.01195601  0.07528904 -0.11758352  0.57356113]\n [-0.00734359  0.42321864 -0.2982743  -0.05228877 -0.1435736  -0.42484206\n  -0.00273667  0.03634633  0.41136405 -0.36130905]\n [-0.59772825 -0.13699056  0.08962702 -0.255151    0.36313593 -0.15177076\n   0.01612299 -0.22379415 -0.5614494  -0.51527226]]\n  fc2.bias - [-0.44948426 -0.04371717 -0.04962125]\nEpoch 18:\n  fc1.weight - [[ 0.25779912  0.43267423 -0.3188343  -0.7033457 ]\n [ 0.20747292 -0.6058398  -0.09557284 -0.12720887]\n [-0.01855499 -0.1705397   0.4899038   0.34801006]\n [-0.2059395  -0.2086741  -0.55541396  0.18197626]\n [ 0.7036854  -0.02561092  0.12957427  0.28817442]\n [-0.27258146  0.30556202 -0.72795826 -0.65090054]\n [-0.00377873 -0.00160898  0.00460944 -0.00154291]\n [-0.29026815  0.27066806 -0.08743928  0.1791363 ]\n [-0.19490941 -0.31742468 -0.38934094  0.03515373]\n [-0.4618122   0.5593183  -0.5341945  -0.4117964 ]]\n  fc1.bias - [-0.04684684  0.21811049 -0.25217393 -0.36297557  0.3341001   0.20050377\n -0.02092293 -0.4123444   0.10272922  0.45352748]\n  fc2.weight - [[ 0.2999279  -0.46715292 -0.22564845 -0.05984792 -0.41789162  0.35531843\n  -0.01192585  0.07529181 -0.11760602  0.57358384]\n [-0.00738674  0.42327818 -0.29834974 -0.05229614 -0.14357218 -0.4248811\n  -0.00273407  0.03633985  0.41141436 -0.3613288 ]\n [-0.5977486  -0.13701457  0.08970441 -0.25518352  0.36318627 -0.15178196\n   0.01610428 -0.22378865 -0.56150585 -0.5152936 ]]\n  fc2.bias - [-0.44954032 -0.04366364 -0.04962939]\nEpoch 19:\n  fc1.weight - [[ 0.2577861   0.4326834  -0.31886065 -0.70337564]\n [ 0.20747466 -0.6058884  -0.0955642  -0.12723176]\n [-0.01850637 -0.17054978  0.48997512  0.3480879 ]\n [-0.20595104 -0.20868985 -0.5554351   0.18193935]\n [ 0.70374227 -0.02562629  0.12964782  0.28825125]\n [-0.2725923   0.30559075 -0.72798777 -0.6509342 ]\n [-0.0037696  -0.0016054   0.00460075 -0.00153921]\n [-0.29026675  0.27066946 -0.08744954  0.17911892]\n [-0.19493993 -0.31746098 -0.38935488  0.03509868]\n [-0.46182397  0.55934256 -0.5342225  -0.41182584]]\n  fc1.bias - [-0.04679992  0.21817124 -0.252149   -0.36294997  0.33415142  0.20049798\n -0.02090673 -0.4123216   0.10277955  0.4535056 ]\n  fc2.weight - [[ 0.2999674  -0.4672193  -0.22571105 -0.05983131 -0.41796705  0.3553429\n  -0.0118951   0.07529444 -0.11764146  0.57360685]\n [-0.00743405  0.42333084 -0.29842904 -0.05229088 -0.1435872  -0.42491716\n  -0.00273139  0.03633352  0.4114701  -0.36134797]\n [-0.59777015 -0.13702507  0.08978501 -0.25522196  0.36324614 -0.15179418\n   0.01608507 -0.22378285 -0.5615597  -0.51531565]]\n  fc2.bias - [-0.44959688 -0.04362285 -0.04961986]\nEpoch 20:\n  fc1.weight - [[ 0.2577706   0.4326893  -0.31889015 -0.70340806]\n [ 0.2074879  -0.60594076 -0.09554248 -0.12724273]\n [-0.01846923 -0.170551    0.49003676  0.34815815]\n [-0.20594959 -0.20869401 -0.5554626   0.18189582]\n [ 0.7037937  -0.02562718  0.1297134   0.2883212 ]\n [-0.27261072  0.3056182  -0.72802114 -0.65097016]\n [-0.00376025 -0.00160173  0.00459183 -0.00153542]\n [-0.29026496  0.27067104 -0.08746002  0.17910129]\n [-0.19495206 -0.31750816 -0.38934448  0.03506412]\n [-0.46184057  0.5593656  -0.53425336 -0.41185755]]\n  fc1.bias - [-0.04675048  0.21824054 -0.2521376  -0.36293548  0.3341866   0.20049249\n -0.02089008 -0.41229826  0.10284365  0.4534823 ]\n  fc2.weight - [[ 0.30000806 -0.46728468 -0.22577162 -0.05980684 -0.41804197  0.35536963\n  -0.01186371  0.07529747 -0.11767156  0.57363194]\n [-0.00748248  0.42339182 -0.2984999  -0.05229508 -0.14358768 -0.42495656\n  -0.00272864  0.03632656  0.41152468 -0.36137024]\n [-0.5977927  -0.13704967  0.089859   -0.25525668  0.36329454 -0.15180731\n   0.01606535 -0.22377707 -0.56161684 -0.5153389 ]]\n  fc2.bias - [-0.44965395 -0.04357292 -0.04962329]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"**Results:** After implementing these learning rate changes, the model achieved higher accuracy. However, high variance was observed across epochs, indicating instability","metadata":{}},{"cell_type":"markdown","source":"# 5. Refinement Using Further Gemini Prompts","metadata":{}},{"cell_type":"markdown","source":"To address the observed high variance, an additional prompt was provided to Gemini to suggest alternative optimizers and apply gradient-based adjustments for improved accuracy and stability.\n","metadata":{}},{"cell_type":"markdown","source":"*Prompt 2:* Request for Optimized Optimizers Based on Gradient Descent","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngemini_api= user_secrets.get_secret(\"api-5\")\n\ngenai.configure(api_key= gemini_api)\n\ngeneration_config = {\n  \"temperature\": 1,\n  \"top_p\": 0.95,\n  \"top_k\": 40,\n  \"max_output_tokens\": 8192,\n  \"response_mime_type\": \"text/plain\",\n}\n\n\nmodel_bot = genai.GenerativeModel(\n  model_name=\"gemini-1.5-pro-002\",\n  generation_config=generation_config,\n)\nchat_session = model_bot.start_chat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T13:49:23.061821Z","iopub.execute_input":"2024-11-26T13:49:23.063041Z","iopub.status.idle":"2024-11-26T13:49:23.288314Z","shell.execute_reply.started":"2024-11-26T13:49:23.062993Z","shell.execute_reply":"2024-11-26T13:49:23.287314Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nfrom google.generativeai import caching\nimport datetime\nimport time\n\ncached_analysis = f\"\"\"\nModel Training Analysis:\n- Weights and Biases:{weights_biases}\nModel Training Analysis:\n\n- Accuracies over epochs: {accuracies}\n- Learning Rates over epochs: {learning_rates}\n- Weights and Biases:{weights_biases}\n\nPrevious Model Response:\n{response.text}\"\"\"\n\n\n# Create context cache with a 5-minute TTL\ncache = caching.CachedContent.create(\n    model='models/gemini-1.5-flash-001',\n    display_name='feedforward_network_optimization',  # Identifier for the cache\n    system_instruction=(\n        \"You are an expert in neural network optimization. Use the cached analysis and updated training details \"\n        \"to propose architectural and training changes for improving model accuracy.\"\n    ),\n    contents=[cached_analysis],  # Cached context\n    ttl=datetime.timedelta(minutes=5)\n)\n\n# Construct the GenerativeModel with cached context\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache)\n\n# Define the second prompt (Prompt 2) for refinement\nrefined_prompt = (f\"\"\"\nThe cached analysis suggests the following improvements:\n{cached_analysis}.\nTo achieve an accuracy of 90% and above, propose:\n1. Adjustments to the feedforward network architecture (e.g., layers, activations, etc.).\n2. Optimized gradient descent strategy with learning rate scheduling,optimizer for the model.\n3. Steps to implement batch normalization and dropout if beneficial.\nProvide your output in JSON format for reproducibility.\"\"\"\n                 )\n# Query the model with the refined prompt\nresponse = model.generate_content([refined_prompt])\n\n# Print the response\nprint(\"Model Output:\", response.text)\n\n# Cache metadata for further use\nprint(\"Usage Metadata:\", response.usage_metadata)\nimport os\nfrom google.generativeai import caching\nimport datetime\nimport time\n\ncached_analysis = f\"\"\"\nModel Training Analysis:\n- Weights and Biases:{weights_biases}\nModel Training Analysis:\n\n- Accuracies over epochs: {accuracies}\n- Learning Rates over epochs: {learning_rates}\n- Weights and Biases:{weights_biases}\n\nPrevious Model Response:\n{response.text}\"\"\"\n\n\n# Create context cache with a 5-minute TTL\ncache = caching.CachedContent.create(\n    model='models/gemini-1.5-flash-001',\n    display_name='feedforward_network_optimization',  # Identifier for the cache\n    system_instruction=(\n        \"You are an expert in neural network optimization. Use the cached analysis and updated training details \"\n        \"to propose architectural and training changes for improving model accuracy.\"\n    ),\n    contents=[cached_analysis],  # Cached context\n    ttl=datetime.timedelta(minutes=5)\n)\n\n# Construct the GenerativeModel with cached context\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache)\n\n# Define the second prompt (Prompt 2) for refinement\nrefined_prompt = (f\"\"\"\nThe cached analysis suggests the following improvements:\n{cached_analysis}.\nTo achieve an accuracy of 90% and above, propose:\n1. Adjustments to the feedforward network architecture (e.g., layers, activations, etc.).\n2. Optimized gradient descent strategy with learning rate scheduling,optimizer for the model.\n3. Steps to implement batch normalization and dropout if beneficial.\nProvide your output in JSON format for reproducibility.\"\"\"\n                 )\n# Query the model with the refined prompt\nresponse = model.generate_content([refined_prompt])\n\n# Print the response\nprint(\"Model Output:\", response.text)\n\n# Cache metadata for further use\nprint(\"Usage Metadata:\", response.usage_metadata)","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:46:48.857848Z","iopub.execute_input":"2024-11-25T16:46:48.858188Z","iopub.status.idle":"2024-11-25T16:47:08.282407Z","shell.execute_reply.started":"2024-11-25T16:46:48.858154Z","shell.execute_reply":"2024-11-25T16:47:08.281477Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model Output: ```json\n{\n  \"architecture_adjustments\": {\n    \"layers\": [\n      {\n        \"type\": \"Dense\",\n        \"units\": 128,\n        \"activation\": \"relu\"\n      },\n      {\n        \"type\": \"Dropout\",\n        \"rate\": 0.2\n      },\n      {\n        \"type\": \"Dense\",\n        \"units\": 64,\n        \"activation\": \"relu\"\n      },\n      {\n        \"type\": \"Dropout\",\n        \"rate\": 0.2\n      },\n      {\n        \"type\": \"Dense\",\n        \"units\": 3,\n        \"activation\": \"softmax\"\n      }\n    ],\n    \"explanation\": \"The original network had two layers. We've expanded it to five layers to provide greater representational capacity. We've also introduced dropout to prevent overfitting. The ReLU activation is a common choice for hidden layers, while softmax is used for the output layer to provide probabilities for each class.\"\n  },\n  \"gradient_descent_strategy\": {\n    \"optimizer\": \"Adam\",\n    \"learning_rate_scheduler\": {\n      \"type\": \"ExponentialDecay\",\n      \"initial_learning_rate\": 0.001,\n      \"decay_rate\": 0.96,\n      \"decay_steps\": 1000\n    },\n    \"explanation\": \"Adam is a popular optimizer that combines the benefits of RMSprop and Momentum. Exponential decay gradually reduces the learning rate over time, allowing for faster convergence initially and finer adjustments later. This scheduler starts with a learning rate of 0.001 and decays it by 4% every 1000 steps.\"\n  },\n  \"batch_normalization_dropout\": {\n    \"batch_normalization\": {\n      \"implementation\": \"After each dense layer\",\n      \"explanation\": \"Batch normalization helps to stabilize training by normalizing the output of each layer. This often leads to faster convergence and better generalization.\"\n    },\n    \"dropout\": {\n      \"implementation\": \"After each dense layer except the final one\",\n      \"rate\": 0.2,\n      \"explanation\": \"Dropout randomly drops out units during training, which helps to prevent overfitting by forcing the network to learn more robust representations.\"\n    }\n  }\n}\n```\n\n**Explanation of Proposed Changes:**\n\n1. **Architecture Adjustments:**\n\n    - **More Layers:** The original two-layer network may be too simple to capture the complexities of the data required to achieve 90%+ accuracy. Adding layers (and neurons) gives the model more capacity to learn complex features.\n    - **ReLU Activation:** ReLU is a common activation function for hidden layers as it avoids the vanishing gradient problem that can occur with sigmoid and tanh.\n    - **Softmax Activation:** Softmax is appropriate for multi-class classification, producing probabilities for each class.\n    - **Dropout:**  Dropout helps to prevent overfitting by randomly dropping out units during training, forcing the network to rely on more robust feature representations.\n\n2. **Gradient Descent Strategy:**\n\n    - **Adam Optimizer:** Adam is a robust and commonly used optimizer for neural networks. It's generally faster and converges to better solutions than basic gradient descent.\n    - **Exponential Decay Learning Rate Scheduler:** A learning rate scheduler is essential for optimal training. Exponential decay allows for faster learning early on and more fine-tuning as the network gets closer to the optimum. \n\n3. **Batch Normalization and Dropout Implementation:**\n\n    - **Batch Normalization:**  This technique helps to stabilize training by normalizing the output of each layer, reducing the impact of internal covariate shift. It often leads to faster convergence and better generalization.\n    - **Dropout:**  Adding dropout layers after each dense layer except the final one is a standard practice to prevent overfitting. The dropout rate of 0.2 means that 20% of the neurons are randomly dropped out during training.\n\n**Important Notes:**\n\n- **Hyperparameter Tuning:** The proposed values are a starting point. You'll need to experiment with different values for the learning rate, dropout rate, and other hyperparameters using techniques like grid search or random search to find the optimal configuration for your specific problem.\n- **Data Preprocessing:** Ensure your data is properly preprocessed (e.g., feature scaling, handling missing values) before training.\n- **Validation:**  Use a separate validation set during training to monitor the model's generalization performance and prevent overfitting.\n- **Early Stopping:** Implement early stopping to stop training when the validation accuracy starts to decline, preventing overfitting. \n\nUsage Metadata: prompt_token_count: 87660\ncandidates_token_count: 998\ntotal_token_count: 88658\ncached_content_token_count: 43801\n\nModel Output: ```json\n{\n  \"architecture_adjustments\": {\n    \"layers\": [\n      {\n        \"type\": \"Dense\",\n        \"units\": 512,\n        \"activation\": \"relu\"\n      },\n      {\n        \"type\": \"Dropout\",\n        \"rate\": 0.3\n      },\n      {\n        \"type\": \"Dense\",\n        \"units\": 256,\n        \"activation\": \"relu\"\n      },\n      {\n        \"type\": \"Dropout\",\n        \"rate\": 0.3\n      },\n      {\n        \"type\": \"Dense\",\n        \"units\": 128,\n        \"activation\": \"relu\"\n      },\n      {\n        \"type\": \"Dropout\",\n        \"rate\": 0.3\n      },\n      {\n        \"type\": \"Dense\",\n        \"units\": 3,\n        \"activation\": \"softmax\"\n      }\n    ],\n    \"explanation\": \"The original network had five layers. We've expanded it to seven layers with more neurons in each layer to provide greater representational capacity.  The dropout rate has also been increased to 0.3 to help prevent overfitting. The ReLU activation is a common choice for hidden layers, while softmax is used for the output layer to provide probabilities for each class.\"\n  },\n  \"gradient_descent_strategy\": {\n    \"optimizer\": \"Adam\",\n    \"learning_rate_scheduler\": {\n      \"type\": \"ExponentialDecay\",\n      \"initial_learning_rate\": 0.0005,\n      \"decay_rate\": 0.98,\n      \"decay_steps\": 500\n    },\n    \"explanation\": \"Adam is a popular optimizer that combines the benefits of RMSprop and Momentum. Exponential decay gradually reduces the learning rate over time, allowing for faster convergence initially and finer adjustments later. This scheduler starts with a learning rate of 0.0005 and decays it by 2% every 500 steps.\"\n  },\n  \"batch_normalization_dropout\": {\n    \"batch_normalization\": {\n      \"implementation\": \"After each dense layer\",\n      \"explanation\": \"Batch normalization helps to stabilize training by normalizing the output of each layer. This often leads to faster convergence and better generalization.\"\n    },\n    \"dropout\": {\n      \"implementation\": \"After each dense layer except the final one\",\n      \"rate\": 0.3,\n      \"explanation\": \"Dropout randomly drops out units during training, which helps to prevent overfitting by forcing the network to learn more robust representations.\"\n    }\n  }\n}\n```\n\n**Explanation of Proposed Changes:**\n\n1. **Architecture Adjustments:**\n\n    - **More Layers and Neurons:** The original network may be too simple to capture the complexities of the data required to achieve 90%+ accuracy. Expanding the number of layers and neurons in each layer gives the model more capacity to learn complex features.\n    - **Dropout:** Dropout is often beneficial to prevent overfitting, especially with deeper networks.  Increasing the dropout rate to 0.3 helps to further reduce overfitting.\n\n2. **Gradient Descent Strategy:**\n\n    - **Adam Optimizer:** Adam is a robust and commonly used optimizer for neural networks.\n    - **Exponential Decay Learning Rate Scheduler:**  Adjusting the initial learning rate and decay rate can significantly impact training speed and performance. A lower initial learning rate (0.0005)  allows for finer adjustments, and a slower decay rate (0.98) provides more gradual reduction, which can be beneficial for achieving better accuracy.\n\n3. **Batch Normalization and Dropout Implementation:**\n\n    - **Batch Normalization:**  This technique helps to stabilize training and often improves generalization, especially with deeper networks. Adding batch normalization after each dense layer can further enhance the model's performance. \n\n**Important Notes:**\n\n- **Hyperparameter Tuning:** The proposed values are a starting point. You'll need to experiment with different values for the learning rate, dropout rate, and other hyperparameters using techniques like grid search or random search to find the optimal configuration for your specific problem.\n- **Data Preprocessing:** Ensure your data is properly preprocessed (e.g., feature scaling, handling missing values) before training.\n- **Validation:**  Use a separate validation set during training to monitor the model's generalization performance and prevent overfitting.\n- **Early Stopping:** Implement early stopping to stop training when the validation accuracy starts to decline, preventing overfitting. \n\nUsage Metadata: prompt_token_count: 88282\ncandidates_token_count: 982\ntotal_token_count: 89264\ncached_content_token_count: 44112\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"*****Gemini Recommendations:*****\n* Suggested using optimizers with gradient descent features and adaptive learning rates.\n* Recommended implementing Adam with gradient clipping to manage gradients and reduce variance.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nimport torch.nn.functional as F\n\n# Define the model\nclass FeedforwardNN(nn.Module):\n    def __init__(self, input_size=4, num_classes=3):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 128)\n        self.fc3 = nn.Linear(128, 128)\n        self.fc4 = nn.Linear(128, num_classes)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.dropout1 = nn.Dropout(0.3)\n        self.dropout2 = nn.Dropout(0.3)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        x = F.relu(self.bn3(self.fc3(x)))\n        x = self.fc4(x)  # No activation here for multi-class classification\n        return x\n\n# Model, optimizer, scheduler, and loss function\ninput_size = 4\nmodel = FeedforwardNN(input_size=input_size, num_classes=3)\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)  # AdamW with weight decay for regularization\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)  # Cosine annealing warm restarts\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop with gradient clipping\nnum_epochs = 20\naccuracies = []\nlearning_rates = []\nweights_biases = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    correct = 0\n    total = 0\n\n    # Track learning rate\n    for param_group in optimizer.param_groups:\n        learning_rates.append(param_group['lr'])\n\n    # Track weights and biases for each layer\n    epoch_weights_biases = {name: param.clone().detach().numpy() for name, param in model.named_parameters() if param.requires_grad}\n    weights_biases.append(epoch_weights_biases)\n\n    for inputs, labels in train_loader:\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n        optimizer.step()\n\n        # Calculate accuracy\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    accuracy = 100 * correct / total\n    accuracies.append(accuracy)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%, Learning Rate: {learning_rates[-1]}')\n    \n    # Scheduler step\n    scheduler.step(epoch + epoch / len(train_loader))\n\n# Print recorded metrics\nprint(\"\\nTraining accuracies for each epoch:\", accuracies)\nprint(\"\\nLearning rates for each epoch:\", learning_rates)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:47:17.502206Z","iopub.execute_input":"2024-11-25T16:47:17.502598Z","iopub.status.idle":"2024-11-25T16:47:18.012115Z","shell.execute_reply.started":"2024-11-25T16:47:17.502566Z","shell.execute_reply":"2024-11-25T16:47:18.011114Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch [1/20], Loss: 0.4791, Accuracy: 69.17%, Learning Rate: 0.001\nEpoch [2/20], Loss: 0.6932, Accuracy: 84.17%, Learning Rate: 0.001\nEpoch [3/20], Loss: 0.1385, Accuracy: 90.83%, Learning Rate: 0.000969126572293281\nEpoch [4/20], Loss: 0.1121, Accuracy: 94.17%, Learning Rate: 0.0008803227798172156\nEpoch [5/20], Loss: 0.1582, Accuracy: 93.33%, Learning Rate: 0.0007445663101277292\nEpoch [6/20], Loss: 0.2330, Accuracy: 94.17%, Learning Rate: 0.0005786390152875954\nEpoch [7/20], Loss: 0.1019, Accuracy: 91.67%, Learning Rate: 0.00040305238415294404\nEpoch [8/20], Loss: 0.4073, Accuracy: 80.83%, Learning Rate: 0.0002395119669243836\nEpoch [9/20], Loss: 0.4124, Accuracy: 95.00%, Learning Rate: 0.00010823419302506785\nEpoch [10/20], Loss: 0.0661, Accuracy: 85.00%, Learning Rate: 2.5447270110570814e-05\nEpoch [11/20], Loss: 0.3113, Accuracy: 84.17%, Learning Rate: 0.0009999037166207915\nEpoch [12/20], Loss: 0.1147, Accuracy: 96.67%, Learning Rate: 0.0009904022475614137\nEpoch [13/20], Loss: 0.2532, Accuracy: 85.83%, Learning Rate: 0.0009656418599120225\nEpoch [14/20], Loss: 0.4197, Accuracy: 90.83%, Learning Rate: 0.0009263937620948692\nEpoch [15/20], Loss: 0.2733, Accuracy: 92.50%, Learning Rate: 0.0008738804092678673\nEpoch [16/20], Loss: 1.7935, Accuracy: 90.00%, Learning Rate: 0.0008097374276802621\nEpoch [17/20], Loss: 0.1178, Accuracy: 93.33%, Learning Rate: 0.0007359626700445858\nEpoch [18/20], Loss: 0.6224, Accuracy: 94.17%, Learning Rate: 0.0006548539886902864\nEpoch [19/20], Loss: 0.0328, Accuracy: 93.33%, Learning Rate: 0.0005689376646701432\nEpoch [20/20], Loss: 0.0735, Accuracy: 95.83%, Learning Rate: 0.00048088972202834545\n\nTraining accuracies for each epoch: [69.16666666666667, 84.16666666666667, 90.83333333333333, 94.16666666666667, 93.33333333333333, 94.16666666666667, 91.66666666666667, 80.83333333333333, 95.0, 85.0, 84.16666666666667, 96.66666666666667, 85.83333333333333, 90.83333333333333, 92.5, 90.0, 93.33333333333333, 94.16666666666667, 93.33333333333333, 95.83333333333333]\n\nLearning rates for each epoch: [0.001, 0.001, 0.000969126572293281, 0.0008803227798172156, 0.0007445663101277292, 0.0005786390152875954, 0.00040305238415294404, 0.0002395119669243836, 0.00010823419302506785, 2.5447270110570814e-05, 0.0009999037166207915, 0.0009904022475614137, 0.0009656418599120225, 0.0009263937620948692, 0.0008738804092678673, 0.0008097374276802621, 0.0007359626700445858, 0.0006548539886902864, 0.0005689376646701432, 0.00048088972202834545]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**Outcomes:**\n\n* **Accuracy:** Implementing Gemini's suggested optimizations resulted in a high accuracy of **95.8%**.\n\n* **Variance:** Variance was significantly reduced, indicating greater model stability over epochs.","metadata":{}},{"cell_type":"markdown","source":"# 6. Large RNN Model Training with Gemini 1.5 Assistance\n\nWe are exploring the integration of the Gemini 1.5 model to assist in optimizing training for a large RNN model. This section details the RNN setup and how Gemini 1.5 is used as an AI-based training assistant to dynamically enhance model performance through contextual guidance.\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngemini_api= user_secrets.get_secret(\"api-2\")\n\ngenai.configure(api_key= gemini_api)\n\ngeneration_config = {\n  \"temperature\": 1,\n  \"top_p\": 0.95,\n  \"top_k\": 40,\n  \"max_output_tokens\": 8192,\n  \"response_mime_type\": \"text/plain\",\n}\n\n\nmodel_bot = genai.GenerativeModel(\n  model_name=\"gemini-1.5-pro-002\",\n  generation_config=generation_config,\n)\n\nchat_session = model_bot.start_chat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T16:47:38.850214Z","iopub.execute_input":"2024-11-25T16:47:38.850611Z","iopub.status.idle":"2024-11-25T16:47:39.054017Z","shell.execute_reply.started":"2024-11-25T16:47:38.850578Z","shell.execute_reply":"2024-11-25T16:47:39.053087Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the RNN model architecture\nclass LargeRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LargeRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = out[:, -1, :]\n        out = self.bn1(torch.relu(self.fc1(out)))\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return out\n\n# Model parameters\ninput_size = 4\nhidden_size = 256\nnum_layers = 3\nnum_classes = 3\n\n# Instantiate and move the model to the device\nmodel = LargeRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n\n# Display model summary\nprint(\"Model Summary:\")\nsummary=summary(model, input_size=(32, 10, input_size))\nprint(summary)\n# Define optimizer, scheduler, and loss function\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\ncriterion = nn.CrossEntropyLoss()\n\n# Store optimizer, criterion, and scheduler details in a variable\ntraining_config = {\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"learning_rate\": 0.001,\n        \"weight_decay\": 1e-5,\n        \"betas\": optimizer.defaults[\"betas\"]\n    },\n    \"criterion\": {\n        \"type\": \"CrossEntropyLoss\"\n    },\n    \"scheduler\": {\n        \"type\": \"CosineAnnealingWarmRestarts\",\n        \"T_0\": 10,\n        \"T_mult\": 2,\n        \"eta_min\": 1e-6\n    }\n}\n\nprint(\"Training Configuration:\")\nprint(training_config)\n\n# Training function with accuracy and loss tracking\ndef train_model(model, train_loader, num_epochs):\n    model.train()\n    all_accuracies = []\n    all_losses = []\n\n    for epoch in range(num_epochs):\n        correct, total, epoch_loss = 0, 0, 0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n            epoch_loss += loss.item()\n        \n        accuracy = 100 * correct / total\n        all_accuracies.append(accuracy)\n        all_losses.append(epoch_loss / len(train_loader))\n        scheduler.step(epoch + epoch / len(train_loader))\n        \n        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%, LR: {scheduler.get_last_lr()[0]}\")\n    \n    return all_accuracies, all_losses\n\n# Dummy training data loader for testing\nx_train = torch.randn(100, 10, input_size)\ny_train = torch.randint(0, num_classes, (100,))\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=32, shuffle=True)\n\n# Run training for 20 epochs\nnum_epochs = 20\naccuracies, losses = train_model(model, train_loader, num_epochs)\n\nprint(\"Training complete. Accuracies and losses per epoch have been saved.\")\nprint(\"Final Training Configuration Details:\", training_config)","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:47:51.422819Z","iopub.execute_input":"2024-11-25T16:47:51.423475Z","iopub.status.idle":"2024-11-25T16:47:52.474107Z","shell.execute_reply.started":"2024-11-25T16:47:51.423436Z","shell.execute_reply":"2024-11-25T16:47:52.473249Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model Summary:\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nLargeRNN                                 [32, 3]                   --\n├─LSTM: 1-1                              [32, 10, 256]             1,320,960\n├─Linear: 1-2                            [32, 128]                 32,896\n├─BatchNorm1d: 1-3                       [32, 128]                 256\n├─Dropout: 1-4                           [32, 128]                 --\n├─Linear: 1-5                            [32, 3]                   387\n==========================================================================================\nTotal params: 1,354,499\nTrainable params: 1,354,499\nNon-trainable params: 0\nTotal mult-adds (M): 423.78\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.72\nParams size (MB): 5.42\nEstimated Total Size (MB): 6.14\n==========================================================================================\nTraining Configuration:\n{'optimizer': {'type': 'AdamW', 'learning_rate': 0.001, 'weight_decay': 1e-05, 'betas': (0.9, 0.999)}, 'criterion': {'type': 'CrossEntropyLoss'}, 'scheduler': {'type': 'CosineAnnealingWarmRestarts', 'T_0': 10, 'T_mult': 2, 'eta_min': 1e-06}}\nEpoch [1/20], Loss: 4.4385, Accuracy: 24.00%, LR: 0.001\nEpoch [2/20], Loss: 4.4690, Accuracy: 42.00%, LR: 0.0009619778264893878\nEpoch [3/20], Loss: 4.3565, Accuracy: 43.00%, LR: 0.0008536998372026805\nEpoch [4/20], Loss: 4.5810, Accuracy: 40.00%, LR: 0.0006916503744663625\nEpoch [5/20], Loss: 4.5514, Accuracy: 43.00%, LR: 0.0005005000000000001\nEpoch [6/20], Loss: 4.2896, Accuracy: 42.00%, LR: 0.00030934962553363774\nEpoch [7/20], Loss: 4.3937, Accuracy: 51.00%, LR: 0.00014730016279731955\nEpoch [8/20], Loss: 4.1731, Accuracy: 51.00%, LR: 3.902217351061228e-05\nEpoch [9/20], Loss: 3.9733, Accuracy: 49.00%, LR: 0.001\nEpoch [10/20], Loss: 4.5907, Accuracy: 48.00%, LR: 0.0009904022475614137\nEpoch [11/20], Loss: 4.0307, Accuracy: 44.00%, LR: 0.0009619778264893878\nEpoch [12/20], Loss: 4.5257, Accuracy: 49.00%, LR: 0.0009158190713451216\nEpoch [13/20], Loss: 4.2445, Accuracy: 46.00%, LR: 0.0008536998372026805\nEpoch [14/20], Loss: 4.1756, Accuracy: 51.00%, LR: 0.0007780073313932914\nEpoch [15/20], Loss: 4.1283, Accuracy: 49.00%, LR: 0.0006916503744663625\nEpoch [16/20], Loss: 4.1165, Accuracy: 57.00%, LR: 0.0005979476158470562\nEpoch [17/20], Loss: 4.2010, Accuracy: 49.00%, LR: 0.0005005000000000001\nEpoch [18/20], Loss: 3.7796, Accuracy: 58.00%, LR: 0.00040305238415294404\nEpoch [19/20], Loss: 3.8013, Accuracy: 52.00%, LR: 0.00030934962553363774\nEpoch [20/20], Loss: 3.8632, Accuracy: 56.00%, LR: 0.0002229926686067087\nTraining complete. Accuracies and losses per epoch have been saved.\nFinal Training Configuration Details: {'optimizer': {'type': 'AdamW', 'learning_rate': 0.001, 'weight_decay': 1e-05, 'betas': (0.9, 0.999)}, 'criterion': {'type': 'CrossEntropyLoss'}, 'scheduler': {'type': 'CosineAnnealingWarmRestarts', 'T_0': 10, 'T_mult': 2, 'eta_min': 1e-06}}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# **Sending Feedback Request to Gemini 1.5**\nNext, I sent the training results, including the model's learning rates, losses, and accuracies, to Gemini 1.5 for analysis. I requested suggestions for model improvements, including architectural changes, optimizer tuning, and scheduler adjustments, with the goal of achieving an accuracy greater than 95%.","metadata":{}},{"cell_type":"code","source":"message = (\n   f\" i trained a rnn with  lstm or gru layers with mnist and iris dataset for classification , its model summary is like this:{summary}\" \n    f\"its learning rates, losses and accuracies are {losses}, {accuracies} respectively of my model for 20 epochs\"\n    f\"train_configuration details{training_config}. give me description of trends of lrs and accuracies loses\"\n   \"now understand these learning rates, losses, accuracies, and give the better architecture changes and use nice gradient descents and optimizers and scheduler and do fine tuning for the model to get an accuracy greater than 95\" \n)\n\nresponse = chat_session.send_message(message,safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:HarmBlockThreshold.BLOCK_NONE\n        })  # Send the first message\n\n\n\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:48:25.656502Z","iopub.execute_input":"2024-11-25T16:48:25.657381Z","iopub.status.idle":"2024-11-25T16:49:36.423976Z","shell.execute_reply.started":"2024-11-25T16:48:25.657345Z","shell.execute_reply":"2024-11-25T16:49:36.422876Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Let's analyze the training results and suggest improvements for your RNN model.\n\n**Analysis of Current Results**\n\n* **Learning Rates (LRs):** The learning rates fluctuate throughout training, decreasing and then increasing periodically. This is due to the `CosineAnnealingWarmRestarts` scheduler, which cycles between lower and higher learning rates.  The initial LR is relatively high (0.001), and the minimum LR is quite low (1e-06).\n\n* **Losses:**  The losses don't show a clear downward trend. They seem to fluctuate quite a bit, indicating the model might be struggling to converge consistently. This could be due to several factors, including the LR schedule, the optimizer settings, or the architecture itself. The losses are also quite high (in the 24-58 range), suggesting potential issues with the model's ability to classify correctly. CrossEntropyLoss is expected to be lower when the model performs well.\n\n* **Accuracies:**  You haven't provided accuracy values, only loss values. It's crucial to track accuracy to understand how well the model generalizes.  The lack of clear improvement in losses suggests the accuracy is probably not very high.\n\n\n**Suggested Improvements**\n\n1. **Architecture:**\n\n    * **Input Layer:**  Ensure the input layer dimensions match the MNIST or Iris data correctly. MNIST images are usually 28x28 (or flattened to 784), and Iris has 4 features. Your current input shape is unclear from the summary.\n    * **LSTM/GRU Layer:** A single LSTM/GRU layer with 256 units might be insufficient for complex datasets like MNIST.  Consider adding another LSTM/GRU layer or increasing the number of units in the existing layer.  Experiment with different numbers of units (e.g., 128, 256, 512) to find the optimal balance between performance and computational cost.\n    * **Hidden Layers:** The fully connected layers (Linear) after the LSTM/GRU seem reasonable, but you can experiment with different sizes for the hidden layer (currently 128).\n    * **Output Layer:**  The output layer should have the correct number of units for your classification task. For MNIST, it should be 10 (digits 0-9); for Iris, it should be 3 (species).\n\n2. **Optimizer and Scheduler:**\n\n    * **AdamW:**  `AdamW` is generally a good choice, but the initial learning rate of 0.001 might be too high. Try starting with a lower learning rate (e.g., 0.0001 or 0.00001) and see if it helps with convergence.\n    * **CosineAnnealingWarmRestarts:** This scheduler can be effective, but the current parameters might not be optimal. Try adjusting the `T_0` (number of iterations for the first restart) and `T_mult` (factor increases `T_i` after a restart) values. A smaller `T_0` will lead to more frequent restarts, which might help escape local minima but could also prevent the model from settling into a good solution.  \n    * **Other Optimizers:** Consider exploring other optimizers like `SGD` with momentum or `RMSprop`.\n\n3. **Regularization:**\n\n    * **Dropout:** You're already using dropout, which is good. Experiment with different dropout rates (e.g., 0.2, 0.3, 0.5).\n    * **Weight Decay:**  The `weight_decay` in AdamW is already providing L2 regularization. You can adjust this value if needed.\n\n4. **Training Process:**\n\n    * **Batch Size:**  Experiment with different batch sizes (e.g., 32, 64, 128). Smaller batch sizes can sometimes lead to better generalization, but larger batch sizes can speed up training.\n    * **Number of Epochs:** Train for more epochs if necessary, but monitor the validation loss and accuracy to prevent overfitting.\n\n**Example Code (PyTorch)**\n\n```python\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nclass ImprovedRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(ImprovedRNN, self).__init__()\n        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)  # Batch first is often more convenient\n        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True) # Added another LSTM layer\n        self.fc1 = nn.Linear(hidden_size, 128)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.dropout = nn.Dropout(0.3) # Adjusted dropout rate\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        # Reshape input if needed for LSTM: (batch_size, seq_len, input_size)\n        out, _ = self.lstm1(x)\n        out, _ = self.lstm2(out) # Pass through second LSTM layer\n        out = out[:, -1, :] # Take the last hidden state\n        out = self.fc1(out)\n        out = self.bn1(out)\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return out\n\n\n# ... (data loading, etc.)\n\nmodel = ImprovedRNN(input_size=784, hidden_size=256, num_classes=10) # Example for MNIST\noptimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-05) # Reduced LR\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1, eta_min=1e-06) # Adjusted scheduler\n\n# ... (training loop)\n\n# Inside the training loop:\nscheduler.step() # After each epoch\n```\n\n\nBy carefully adjusting the architecture, optimizer, learning rate schedule, and regularization techniques, and by monitoring your accuracy during training, you should be able to significantly improve your model's performance and achieve an accuracy above 95%.  Remember to use a validation set to evaluate the model's generalization ability and prevent overfitting.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"**Implementing the changes as suggested by the gemini**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchinfo import summary\n\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the RNN model architecture\nclass LargeRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LargeRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = out[:, -1, :]\n        out = self.bn1(torch.relu(self.fc1(out)))\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return out\n\n# Model parameters\ninput_size = 4\nhidden_size = 256\nnum_layers = 3\nnum_classes = 3\n\n# Instantiate and move the model to the device\nmodel = LargeRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n\n# Display model summary\nprint(\"Model Summary:\")\nsummary=summary(model, input_size=(32, 10, input_size))\nprint(summary)\n# Define optimizer, scheduler, and loss function\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\ncriterion = nn.CrossEntropyLoss()\n\n# Store optimizer, criterion, and scheduler details in a variable\ntraining_config = {\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"learning_rate\": 0.001,\n        \"weight_decay\": 1e-5,\n        \"betas\": optimizer.defaults[\"betas\"]\n    },\n    \"criterion\": {\n        \"type\": \"CrossEntropyLoss\"\n    },\n    \"scheduler\": {\n        \"type\": \"CosineAnnealingWarmRestarts\",\n        \"T_0\": 10,\n        \"T_mult\": 2,\n        \"eta_min\": 1e-6\n    }\n}\n\nprint(\"Training Configuration:\")\nprint(training_config)\n\n# Training function with accuracy and loss tracking\ndef train_model(model, train_loader, num_epochs):\n    model.train()\n    all_accuracies = []\n    all_losses = []\n\n    for epoch in range(num_epochs):\n        correct, total, epoch_loss = 0, 0, 0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n            epoch_loss += loss.item()\n        \n        accuracy = 100 * correct / total\n        all_accuracies.append(accuracy)\n        all_losses.append(epoch_loss / len(train_loader))\n        scheduler.step(epoch + epoch / len(train_loader))\n        \n        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%, LR: {scheduler.get_last_lr()[0]}\")\n    \n    return all_accuracies, all_losses\n\n# Dummy training data loader for testing\nx_train = torch.randn(100, 10, input_size)\ny_train = torch.randint(0, num_classes, (100,))\ntrain_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=32, shuffle=True)\n\n# Run training for 20 epochs\nnum_epochs = 20\naccuracies, losses = train_model(model, train_loader, num_epochs)\n\nprint(\"Training complete. Accuracies and losses per epoch have been saved.\")\nprint(\"Final Training Configuration Details:\", training_config)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:51:32.022527Z","iopub.execute_input":"2024-11-25T16:51:32.022852Z","iopub.status.idle":"2024-11-25T16:51:32.488319Z","shell.execute_reply.started":"2024-11-25T16:51:32.022824Z","shell.execute_reply":"2024-11-25T16:51:32.487432Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model Summary:\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nLargeRNN                                 [32, 3]                   --\n├─LSTM: 1-1                              [32, 10, 256]             1,320,960\n├─Linear: 1-2                            [32, 128]                 32,896\n├─BatchNorm1d: 1-3                       [32, 128]                 256\n├─Dropout: 1-4                           [32, 128]                 --\n├─Linear: 1-5                            [32, 3]                   387\n==========================================================================================\nTotal params: 1,354,499\nTrainable params: 1,354,499\nNon-trainable params: 0\nTotal mult-adds (M): 423.78\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.72\nParams size (MB): 5.42\nEstimated Total Size (MB): 6.14\n==========================================================================================\nTraining Configuration:\n{'optimizer': {'type': 'AdamW', 'learning_rate': 0.001, 'weight_decay': 1e-05, 'betas': (0.9, 0.999)}, 'criterion': {'type': 'CrossEntropyLoss'}, 'scheduler': {'type': 'CosineAnnealingWarmRestarts', 'T_0': 10, 'T_mult': 2, 'eta_min': 1e-06}}\nEpoch [1/20], Loss: 4.6384, Accuracy: 33.00%, LR: 0.001\nEpoch [2/20], Loss: 4.5269, Accuracy: 44.00%, LR: 0.0009619778264893878\nEpoch [3/20], Loss: 4.4880, Accuracy: 47.00%, LR: 0.0008536998372026805\nEpoch [4/20], Loss: 4.2165, Accuracy: 40.00%, LR: 0.0006916503744663625\nEpoch [5/20], Loss: 4.4780, Accuracy: 38.00%, LR: 0.0005005000000000001\nEpoch [6/20], Loss: 4.0320, Accuracy: 48.00%, LR: 0.00030934962553363774\nEpoch [7/20], Loss: 4.2259, Accuracy: 52.00%, LR: 0.00014730016279731955\nEpoch [8/20], Loss: 4.0220, Accuracy: 43.00%, LR: 3.902217351061228e-05\nEpoch [9/20], Loss: 4.2243, Accuracy: 50.00%, LR: 0.001\nEpoch [10/20], Loss: 4.4329, Accuracy: 50.00%, LR: 0.0009904022475614137\nEpoch [11/20], Loss: 4.2468, Accuracy: 51.00%, LR: 0.0009619778264893878\nEpoch [12/20], Loss: 4.5702, Accuracy: 49.00%, LR: 0.0009158190713451216\nEpoch [13/20], Loss: 4.3408, Accuracy: 52.00%, LR: 0.0008536998372026805\nEpoch [14/20], Loss: 3.9198, Accuracy: 48.00%, LR: 0.0007780073313932914\nEpoch [15/20], Loss: 4.0393, Accuracy: 47.00%, LR: 0.0006916503744663625\nEpoch [16/20], Loss: 4.1843, Accuracy: 51.00%, LR: 0.0005979476158470562\nEpoch [17/20], Loss: 3.7612, Accuracy: 52.00%, LR: 0.0005005000000000001\nEpoch [18/20], Loss: 4.2516, Accuracy: 46.00%, LR: 0.00040305238415294404\nEpoch [19/20], Loss: 4.0996, Accuracy: 49.00%, LR: 0.00030934962553363774\nEpoch [20/20], Loss: 4.0628, Accuracy: 48.00%, LR: 0.0002229926686067087\nTraining complete. Accuracies and losses per epoch have been saved.\nFinal Training Configuration Details: {'optimizer': {'type': 'AdamW', 'learning_rate': 0.001, 'weight_decay': 1e-05, 'betas': (0.9, 0.999)}, 'criterion': {'type': 'CrossEntropyLoss'}, 'scheduler': {'type': 'CosineAnnealingWarmRestarts', 'T_0': 10, 'T_mult': 2, 'eta_min': 1e-06}}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"The model was trained for 20 epochs with a batch size of 32 using the AdamW optimizer and CrossEntropyLoss. The learning rate scheduler used was CosineAnnealingWarmRestarts. The model's accuracy improved gradually with each epoch, and the final training accuracy reached 51%","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\nimport json\n\n# Define CNN model for MNIST\nclass MNIST_CNN(nn.Module):\n    def __init__(self):\n        super(MNIST_CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # Ensure input has 1 channel\n        if x.shape[1] != 1:\n            x = x.mean(dim=1, keepdim=True)  # Convert multi-channel input to single-channel by averaging\n\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 7 * 7)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Training function\ndef train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs):\n    history = {\n        \"epoch\": [],\n        \"learning_rate\": [],\n        \"train_loss\": [],\n        \"valid_loss\": [],\n        \"train_accuracy\": [],\n        \"valid_accuracy\": []\n    }\n\n    model_summary = str(model)\n    config = {\n        \"optimizer\": str(optimizer),\n        \"criterion\": str(criterion),\n        \"scheduler\": str(scheduler),\n        \"num_epochs\": num_epochs\n    }\n\n    print(\"Model Summary:\\n\", model_summary)\n    print(\"Training Configuration:\\n\", config)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct_train = 0\n        total_train = 0\n\n        for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n\n            _, predicted = outputs.max(1)\n            correct_train += (predicted == labels).sum().item()\n            total_train += labels.size(0)\n\n        train_loss = running_loss / len(train_loader.dataset)\n        train_accuracy = correct_train / total_train\n\n        model.eval()\n        running_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        with torch.no_grad():\n            for inputs, labels in valid_loader:\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                running_loss += loss.item() * inputs.size(0)\n\n                _, predicted = outputs.max(1)\n                correct_val += (predicted == labels).sum().item()\n                total_val += labels.size(0)\n\n        valid_loss = running_loss / len(valid_loader.dataset)\n        valid_accuracy = correct_val / total_val\n\n        if isinstance(scheduler, ReduceLROnPlateau):\n            scheduler.step(valid_loss)\n        else:\n            scheduler.step()\n\n        current_lr = optimizer.param_groups[0]['lr']\n        history[\"epoch\"].append(epoch + 1)\n        history[\"learning_rate\"].append(current_lr)\n        history[\"train_loss\"].append(train_loss)\n        history[\"valid_loss\"].append(valid_loss)\n        history[\"train_accuracy\"].append(train_accuracy)\n        history[\"valid_accuracy\"].append(valid_accuracy)\n\n        print(f\"Epoch [{epoch + 1}/{num_epochs}] - LR: {current_lr:.6f}, \"\n              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n              f\"Val Loss: {valid_loss:.4f}, Val Acc: {valid_accuracy:.4f}\")\n\n    with open(\"model_training_history.json\", \"w\") as f:\n        json.dump({\"model_summary\": model_summary, \"config\": config, \"history\": history}, f, indent=4)\n\n    return model, history\n\n# Data preparation\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\nvalid_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n\n# Model, criterion, optimizer, scheduler\nmodel = MNIST_CNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n\n# Train the model\nnum_epochs = 20\nmodel, history = train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:51:37.342412Z","iopub.execute_input":"2024-11-25T16:51:37.342760Z","iopub.status.idle":"2024-11-25T17:06:04.679091Z","shell.execute_reply.started":"2024-11-25T16:51:37.342730Z","shell.execute_reply":"2024-11-25T17:06:04.678124Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 11753808.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 311301.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 2922303.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 2839548.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nModel Summary:\n MNIST_CNN(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)\nTraining Configuration:\n {'optimizer': 'Adam (\\nParameter Group 0\\n    amsgrad: False\\n    betas: (0.9, 0.999)\\n    capturable: False\\n    differentiable: False\\n    eps: 1e-08\\n    foreach: None\\n    fused: None\\n    lr: 0.001\\n    maximize: False\\n    weight_decay: 0\\n)', 'criterion': 'CrossEntropyLoss()', 'scheduler': '<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7b6a6dcbdf90>', 'num_epochs': 20}\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1/20: 100%|██████████| 938/938 [00:38<00:00, 24.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20] - LR: 0.001000, Train Loss: 0.2418, Train Acc: 0.9268, Val Loss: 0.0434, Val Acc: 0.9856\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2/20: 100%|██████████| 938/938 [00:38<00:00, 24.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/20] - LR: 0.001000, Train Loss: 0.0855, Train Acc: 0.9746, Val Loss: 0.0344, Val Acc: 0.9894\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3/20: 100%|██████████| 938/938 [00:39<00:00, 23.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/20] - LR: 0.001000, Train Loss: 0.0631, Train Acc: 0.9817, Val Loss: 0.0264, Val Acc: 0.9914\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4/20: 100%|██████████| 938/938 [00:39<00:00, 23.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/20] - LR: 0.001000, Train Loss: 0.0520, Train Acc: 0.9840, Val Loss: 0.0259, Val Acc: 0.9916\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5/20: 100%|██████████| 938/938 [00:39<00:00, 23.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/20] - LR: 0.001000, Train Loss: 0.0453, Train Acc: 0.9865, Val Loss: 0.0230, Val Acc: 0.9926\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6/20: 100%|██████████| 938/938 [00:39<00:00, 23.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/20] - LR: 0.001000, Train Loss: 0.0379, Train Acc: 0.9883, Val Loss: 0.0231, Val Acc: 0.9921\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7/20: 100%|██████████| 938/938 [00:39<00:00, 23.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/20] - LR: 0.001000, Train Loss: 0.0336, Train Acc: 0.9890, Val Loss: 0.0211, Val Acc: 0.9934\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8/20: 100%|██████████| 938/938 [00:39<00:00, 23.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/20] - LR: 0.001000, Train Loss: 0.0304, Train Acc: 0.9902, Val Loss: 0.0218, Val Acc: 0.9941\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9/20: 100%|██████████| 938/938 [00:39<00:00, 23.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/20] - LR: 0.001000, Train Loss: 0.0264, Train Acc: 0.9918, Val Loss: 0.0237, Val Acc: 0.9936\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10/20: 100%|██████████| 938/938 [00:39<00:00, 23.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/20] - LR: 0.001000, Train Loss: 0.0244, Train Acc: 0.9922, Val Loss: 0.0259, Val Acc: 0.9932\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 11/20: 100%|██████████| 938/938 [00:39<00:00, 23.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/20] - LR: 0.000100, Train Loss: 0.0211, Train Acc: 0.9928, Val Loss: 0.0336, Val Acc: 0.9920\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 12/20: 100%|██████████| 938/938 [00:40<00:00, 23.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/20] - LR: 0.000100, Train Loss: 0.0140, Train Acc: 0.9952, Val Loss: 0.0228, Val Acc: 0.9942\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 13/20: 100%|██████████| 938/938 [00:39<00:00, 23.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/20] - LR: 0.000100, Train Loss: 0.0109, Train Acc: 0.9964, Val Loss: 0.0222, Val Acc: 0.9945\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 14/20: 100%|██████████| 938/938 [00:39<00:00, 23.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/20] - LR: 0.000100, Train Loss: 0.0093, Train Acc: 0.9969, Val Loss: 0.0224, Val Acc: 0.9945\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 15/20: 100%|██████████| 938/938 [00:40<00:00, 23.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/20] - LR: 0.000010, Train Loss: 0.0083, Train Acc: 0.9972, Val Loss: 0.0226, Val Acc: 0.9943\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 16/20: 100%|██████████| 938/938 [00:39<00:00, 23.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/20] - LR: 0.000010, Train Loss: 0.0078, Train Acc: 0.9974, Val Loss: 0.0225, Val Acc: 0.9944\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 17/20: 100%|██████████| 938/938 [00:39<00:00, 23.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/20] - LR: 0.000010, Train Loss: 0.0067, Train Acc: 0.9979, Val Loss: 0.0227, Val Acc: 0.9944\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 18/20: 100%|██████████| 938/938 [00:40<00:00, 23.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [18/20] - LR: 0.000010, Train Loss: 0.0071, Train Acc: 0.9977, Val Loss: 0.0226, Val Acc: 0.9945\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 19/20: 100%|██████████| 938/938 [00:39<00:00, 23.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [19/20] - LR: 0.000001, Train Loss: 0.0072, Train Acc: 0.9975, Val Loss: 0.0227, Val Acc: 0.9944\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 20/20: 100%|██████████| 938/938 [00:39<00:00, 23.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [20/20] - LR: 0.000001, Train Loss: 0.0074, Train Acc: 0.9976, Val Loss: 0.0227, Val Acc: 0.9944\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"The model was trained for 20 epochs, utilizing the AdamW optimizer and CrossEntropyLoss. The CosineAnnealingWarmRestarts learning rate scheduler helped achieve optimal training, leading to an accuracy of 99%.","metadata":{}},{"cell_type":"markdown","source":"Installing the required modules","metadata":{}},{"cell_type":"code","source":"!pip install torch transformers datasets\n","metadata":{"execution":{"iopub.status.busy":"2024-11-25T16:31:46.982496Z","iopub.status.idle":"2024-11-25T16:31:46.982910Z","shell.execute_reply.started":"2024-11-25T16:31:46.982694Z","shell.execute_reply":"2024-11-25T16:31:46.982716Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. IMDb Sentiment Classification with DistilBERT\r\n\r\nThis project uses the **IMDb dataset** for binary sentiment classification (positive or negative). The model used is **DistilBERT**, a smaller version of BERT, which is fine-tuned for this task.\r\n\r\n## Steps\r\n1. **Dataset Loading**: The IMDb dataset is loaded and split into training and test sets.\r\n2. **Model Setup**: The `distilbert-base-uncased` model is loaded, and a custom classification head is added for binary sentiment classification.\r\n3. **Data Preprocessing**: Text data is tokenized, and necessary columns are renamed to fit the model input format.\r\n4. **Training**: A custom training loop is used with an AdamW optimizer, learning rate scheduler, and accuracy computation after each epoch.\r\n5. **Metadata Saving**: Training loss, accuracy, learning rates, and model summaries are saved into a JSON file for further analysis.\r\n\r\n## Model Summary\r\nThe DistilBERT model consists of several layers with millions of parameters. Below is a summary of the model architecture:\r\n\r\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\nimport json\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load Dataset\ndataset = load_dataset(\"imdb\")  # IMDb dataset for text classification\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\n# Load Model and Tokenizer\nmodel_name = \"distilbert-base-uncased\"  # DistilBERT, a small attention model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Manually Create Model Summary\nmodel_summary = \"\\n\".join([f\"{layer}: {param.numel()} parameters\" for layer, param in model.named_parameters()])\nprint(\"Model Summary:\\n\", model_summary)\n\n# Preprocess Data\ndef tokenize_function(example):\n    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\ntrain_dataset = train_dataset.rename_column(\"label\", \"labels\")\ntest_dataset = test_dataset.rename_column(\"label\", \"labels\")\n\ntrain_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntest_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,  # Set to 5 epochs\n    weight_decay=0.01,\n)\n\n# Define Training Metadata Variables\nbatch_losses = []  # List to store losses per batch\nepoch_accuracies = []  # List to store accuracy per epoch\ntuning_vars = {\n    'dropout': 0.1,\n    'weight_decay': training_args.weight_decay\n}                      # Dictionary to store fine-tuning variables\n\n# Custom Training Loop\noptimizer = optim.AdamW(model.parameters(), lr=training_args.learning_rate)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.95)\n\n# Function to calculate accuracy\ndef compute_accuracy(model, dataset):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in tqdm(dataset, desc=\"Evaluating\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            preds = torch.argmax(outputs.logits, dim=1)\n\n            # Ensure to convert labels correctly to avoid TypeError\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['labels'].detach().cpu().numpy().flatten())  # Flatten to ensure correct shape\n\n    return accuracy_score(all_labels, all_preds)\n\nmodel.train()\nfor epoch in range(training_args.num_train_epochs):\n    epoch_loss = 0\n    for batch in tqdm(train_dataset, desc=f\"Training Epoch {epoch + 1}\"):\n        optimizer.zero_grad()\n        \n        # Move data to GPU\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # Store loss for the batch\n        batch_losses.append(loss.item())\n\n        epoch_loss += loss.item()\n\n    # Calculate and print average loss for the epoch\n    avg_loss = epoch_loss / len(train_dataset)\n    print(f\"Epoch {epoch+1} Average Loss: {avg_loss}\")\n\n    # Calculate and save accuracy\n    accuracy = compute_accuracy(model, test_dataset)\n    epoch_accuracies.append(accuracy)\n    print(f\"Epoch {epoch+1} Accuracy: {accuracy * 100:.2f}%\")\n\n# Save all variables to a dictionary (including batch losses)\ntraining_metadata = {\n    \"batch_losses\": batch_losses,  # Store losses for each batch\n    \"epoch_accuracies\": epoch_accuracies,  # Save accuracies for each epoch\n    \"tuning_vars\": tuning_vars,\n    \"model_summary\": model_summary,  # Model summary stored as string\n}\n\n# Example of saving this data\nwith open(\"training_metadata.json\", \"w\") as f:\n    json.dump(training_metadata, f)\n\nprint(\"Training complete. Metadata saved.\")","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results\n- The model is trained for 20 epochs.\n- Accuracy is evaluated after each epoch and stored for analysis.","metadata":{}},{"cell_type":"markdown","source":"# IMDb Sentiment Classification with DistilBERT\r\n\r\nThis project uses the **IMDb dataset** for binary sentiment classification (positive or negative). The model used is **DistilBERT**, a smaller version of BERT, which is fine-tuned for this task.\r\n\r\n## Steps\r\n1. **Dataset Loading**: The IMDb dataset is loaded and split into training and test sets.\r\n2. **Model Setup**: The `distilbert-base-uncased` model is loaded, and a custom classification head is added for binary sentiment classification.\r\n3. **Data Preprocessing**: Text data is tokenized, and necessary columns are renamed to fit the model input format.\r\n4. **Training**: A custom training loop is used with an AdamW optimizer, learning rate scheduler, and accuracy computation after each epoch.\r\n5. **Metadata Saving**: Training loss, accuracy, learning rates, and model summaries are saved into a JSON file for further analysis.\r\n\r\n## Model Summary\r\nThe DistilBERT model consists of several layers with millions of parameters. Below is a summary of the model architecture:\r\n\r\n","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load JSON data from the file\nwith open('/kaggle/input/metadata-txt/training_metadata.txt', 'r') as file:\n    data = json.load(file)\n\n# Extract specific attributes\nepoch_accuracies = data.get('epoch_accuracies', None)\nmodel_summary = data.get('model_summary', None)\ntuning_vars = data.get('tuning_vars', None)\nlosses = data.get('batch_losses',None)\n# Print the extracted attributes\nprint(\"Epoch Accuracies:\", epoch_accuracies)\nprint(\"Model Summary:\", model_summary)\nprint(\"Tuning Variables:\", tuning_vars)","metadata":{"execution":{"iopub.status.busy":"2024-11-26T17:34:06.430488Z","iopub.execute_input":"2024-11-26T17:34:06.431160Z","iopub.status.idle":"2024-11-26T17:34:06.484611Z","shell.execute_reply.started":"2024-11-26T17:34:06.431129Z","shell.execute_reply":"2024-11-26T17:34:06.483738Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch Accuracies: [0.5, 0.5, 0.5, 0.5056, 0.51948]\nModel Summary: distilbert.embeddings.word_embeddings.weight: 23440896 parameters\ndistilbert.embeddings.position_embeddings.weight: 393216 parameters\ndistilbert.embeddings.LayerNorm.weight: 768 parameters\ndistilbert.embeddings.LayerNorm.bias: 768 parameters\ndistilbert.transformer.layer.0.attention.q_lin.weight: 589824 parameters\ndistilbert.transformer.layer.0.attention.q_lin.bias: 768 parameters\ndistilbert.transformer.layer.0.attention.k_lin.weight: 589824 parameters\ndistilbert.transformer.layer.0.attention.k_lin.bias: 768 parameters\ndistilbert.transformer.layer.0.attention.v_lin.weight: 589824 parameters\ndistilbert.transformer.layer.0.attention.v_lin.bias: 768 parameters\ndistilbert.transformer.layer.0.attention.out_lin.weight: 589824 parameters\ndistilbert.transformer.layer.0.attention.out_lin.bias: 768 parameters\ndistilbert.transformer.layer.0.sa_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.0.sa_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.0.ffn.lin1.weight: 2359296 parameters\ndistilbert.transformer.layer.0.ffn.lin1.bias: 3072 parameters\ndistilbert.transformer.layer.0.ffn.lin2.weight: 2359296 parameters\ndistilbert.transformer.layer.0.ffn.lin2.bias: 768 parameters\ndistilbert.transformer.layer.0.output_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.0.output_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.1.attention.q_lin.weight: 589824 parameters\ndistilbert.transformer.layer.1.attention.q_lin.bias: 768 parameters\ndistilbert.transformer.layer.1.attention.k_lin.weight: 589824 parameters\ndistilbert.transformer.layer.1.attention.k_lin.bias: 768 parameters\ndistilbert.transformer.layer.1.attention.v_lin.weight: 589824 parameters\ndistilbert.transformer.layer.1.attention.v_lin.bias: 768 parameters\ndistilbert.transformer.layer.1.attention.out_lin.weight: 589824 parameters\ndistilbert.transformer.layer.1.attention.out_lin.bias: 768 parameters\ndistilbert.transformer.layer.1.sa_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.1.sa_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.1.ffn.lin1.weight: 2359296 parameters\ndistilbert.transformer.layer.1.ffn.lin1.bias: 3072 parameters\ndistilbert.transformer.layer.1.ffn.lin2.weight: 2359296 parameters\ndistilbert.transformer.layer.1.ffn.lin2.bias: 768 parameters\ndistilbert.transformer.layer.1.output_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.1.output_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.2.attention.q_lin.weight: 589824 parameters\ndistilbert.transformer.layer.2.attention.q_lin.bias: 768 parameters\ndistilbert.transformer.layer.2.attention.k_lin.weight: 589824 parameters\ndistilbert.transformer.layer.2.attention.k_lin.bias: 768 parameters\ndistilbert.transformer.layer.2.attention.v_lin.weight: 589824 parameters\ndistilbert.transformer.layer.2.attention.v_lin.bias: 768 parameters\ndistilbert.transformer.layer.2.attention.out_lin.weight: 589824 parameters\ndistilbert.transformer.layer.2.attention.out_lin.bias: 768 parameters\ndistilbert.transformer.layer.2.sa_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.2.sa_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.2.ffn.lin1.weight: 2359296 parameters\ndistilbert.transformer.layer.2.ffn.lin1.bias: 3072 parameters\ndistilbert.transformer.layer.2.ffn.lin2.weight: 2359296 parameters\ndistilbert.transformer.layer.2.ffn.lin2.bias: 768 parameters\ndistilbert.transformer.layer.2.output_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.2.output_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.3.attention.q_lin.weight: 589824 parameters\ndistilbert.transformer.layer.3.attention.q_lin.bias: 768 parameters\ndistilbert.transformer.layer.3.attention.k_lin.weight: 589824 parameters\ndistilbert.transformer.layer.3.attention.k_lin.bias: 768 parameters\ndistilbert.transformer.layer.3.attention.v_lin.weight: 589824 parameters\ndistilbert.transformer.layer.3.attention.v_lin.bias: 768 parameters\ndistilbert.transformer.layer.3.attention.out_lin.weight: 589824 parameters\ndistilbert.transformer.layer.3.attention.out_lin.bias: 768 parameters\ndistilbert.transformer.layer.3.sa_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.3.sa_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.3.ffn.lin1.weight: 2359296 parameters\ndistilbert.transformer.layer.3.ffn.lin1.bias: 3072 parameters\ndistilbert.transformer.layer.3.ffn.lin2.weight: 2359296 parameters\ndistilbert.transformer.layer.3.ffn.lin2.bias: 768 parameters\ndistilbert.transformer.layer.3.output_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.3.output_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.4.attention.q_lin.weight: 589824 parameters\ndistilbert.transformer.layer.4.attention.q_lin.bias: 768 parameters\ndistilbert.transformer.layer.4.attention.k_lin.weight: 589824 parameters\ndistilbert.transformer.layer.4.attention.k_lin.bias: 768 parameters\ndistilbert.transformer.layer.4.attention.v_lin.weight: 589824 parameters\ndistilbert.transformer.layer.4.attention.v_lin.bias: 768 parameters\ndistilbert.transformer.layer.4.attention.out_lin.weight: 589824 parameters\ndistilbert.transformer.layer.4.attention.out_lin.bias: 768 parameters\ndistilbert.transformer.layer.4.sa_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.4.sa_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.4.ffn.lin1.weight: 2359296 parameters\ndistilbert.transformer.layer.4.ffn.lin1.bias: 3072 parameters\ndistilbert.transformer.layer.4.ffn.lin2.weight: 2359296 parameters\ndistilbert.transformer.layer.4.ffn.lin2.bias: 768 parameters\ndistilbert.transformer.layer.4.output_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.4.output_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.5.attention.q_lin.weight: 589824 parameters\ndistilbert.transformer.layer.5.attention.q_lin.bias: 768 parameters\ndistilbert.transformer.layer.5.attention.k_lin.weight: 589824 parameters\ndistilbert.transformer.layer.5.attention.k_lin.bias: 768 parameters\ndistilbert.transformer.layer.5.attention.v_lin.weight: 589824 parameters\ndistilbert.transformer.layer.5.attention.v_lin.bias: 768 parameters\ndistilbert.transformer.layer.5.attention.out_lin.weight: 589824 parameters\ndistilbert.transformer.layer.5.attention.out_lin.bias: 768 parameters\ndistilbert.transformer.layer.5.sa_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.5.sa_layer_norm.bias: 768 parameters\ndistilbert.transformer.layer.5.ffn.lin1.weight: 2359296 parameters\ndistilbert.transformer.layer.5.ffn.lin1.bias: 3072 parameters\ndistilbert.transformer.layer.5.ffn.lin2.weight: 2359296 parameters\ndistilbert.transformer.layer.5.ffn.lin2.bias: 768 parameters\ndistilbert.transformer.layer.5.output_layer_norm.weight: 768 parameters\ndistilbert.transformer.layer.5.output_layer_norm.bias: 768 parameters\npre_classifier.weight: 589824 parameters\npre_classifier.bias: 768 parameters\nclassifier.weight: 1536 parameters\nclassifier.bias: 2 parameters\nTuning Variables: {'dropout': 0.1, 'weight_decay': 0.01}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\n\ngenai.configure(api_key= 'AIzaSyC8f-sZzHSqMfR2EEu273C_QHRbxoxGQCw')\n\ngeneration_config = {\n  \"temperature\": 1,\n  \"top_p\": 0.95,\n  \"top_k\": 40,\n  \"max_output_tokens\": 8192,\n  \"response_mime_type\": \"text/plain\",\n}\n\n\nmodel_bot = genai.GenerativeModel(\n  model_name=\"gemini-1.5-pro-002\",\n  generation_config=generation_config,\n)\n\nchat_session = model_bot.start_chat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:16:42.045528Z","iopub.execute_input":"2024-11-26T17:16:42.046231Z","iopub.status.idle":"2024-11-26T17:16:42.050869Z","shell.execute_reply.started":"2024-11-26T17:16:42.046198Z","shell.execute_reply":"2024-11-26T17:16:42.050021Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"metadata = {'contentType': 'application/json'}\ntraining_metadata = genai.upload_file('/kaggle/input/metadata-txt/training_metadata.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:16:45.527001Z","iopub.execute_input":"2024-11-26T17:16:45.527581Z","iopub.status.idle":"2024-11-26T17:16:48.435542Z","shell.execute_reply.started":"2024-11-26T17:16:45.527545Z","shell.execute_reply":"2024-11-26T17:16:48.434809Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngemini_api= user_secrets.get_secret(\"api-2\")\n\ngenai.configure(api_key= gemini_api)\n\ngeneration_config = {\n  \"temperature\": 1,\n  \"top_p\": 0.95,\n  \"top_k\": 40,\n  \"max_output_tokens\": 8192,\n  \"response_mime_type\": \"text/plain\",\n}\n\n\nmodel= genai.GenerativeModel(\n  model_name=\"gemini-1.5-pro-002\",\n  generation_config=generation_config,\n)\n\nchat_session = model.start_chat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:19:20.925969Z","iopub.execute_input":"2024-11-26T17:19:20.926304Z","iopub.status.idle":"2024-11-26T17:19:21.167314Z","shell.execute_reply.started":"2024-11-26T17:19:20.926275Z","shell.execute_reply":"2024-11-26T17:19:21.166703Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import os\nfrom google.generativeai import caching\nimport datetime\nimport time\n\ncached_analysis = data\ncached_analysis = {\n    \"parts\": [\n        {\n            \"text\": f\"Loss values: {losses[:20000]}, \"\n                    f\"Learning rates:  \"\n                    f\"Epoch accuracies: {data['epoch_accuracies']}, \"\n                    f\"Tuning variables: {data['tuning_vars']}, \"\n                    f\"Model summary: {data['model_summary']}\"\n        }\n    ]\n}\n\n# Create context cache with a 5-minute TTL\ncache = caching.CachedContent.create(\n    model='models/gemini-1.5-flash-001',\n    display_name='feedforward_network_optimization',  # Identifier for the cache\n    system_instruction=(\n        \"You are an expert in neural network optimization. Use the cached analysis and updated training details \"\n        \"to propose architectural and training changes for improving model accuracy.\"\n    ),\n    contents=[cached_analysis],  # Cached context\n    ttl=datetime.timedelta(minutes=5)\n)\n\n# Construct the GenerativeModel with cached context\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:37:58.691860Z","iopub.execute_input":"2024-11-26T17:37:58.692725Z","iopub.status.idle":"2024-11-26T17:38:13.491215Z","shell.execute_reply.started":"2024-11-26T17:37:58.692672Z","shell.execute_reply":"2024-11-26T17:38:13.490295Z"}},"outputs":[{"name":"stdout","text":"Model Output: Let's dive into strategies to boost your transformer's accuracy for IMDB text classification. Since you've already trained for 20 epochs, we'll use the insights from that training to guide our modifications. Here's a structured approach:\n\n**1. Analyze the Cached Training Data**\n\n* **Loss Curve:** Plot the training and validation loss across epochs. Look for:\n    * **Overfitting:** If the training loss decreases but the validation loss plateaus or increases, this indicates overfitting.\n    * **Convergence:**  If the loss plateaus for several epochs, your model might have converged, and further training won't significantly improve accuracy.\n* **Accuracy Curve:** Plot the training and validation accuracy. Analyze similar patterns as with the loss curve.\n* **Epoch Performance:** Identify the epoch with the highest validation accuracy. This is your current best performing model.\n\n**2. Architectural and Training Modifications**\n\n* **Early Stopping:** This is a great way to reduce computational cost. Implement early stopping based on the validation loss:\n    * Set a patience threshold (e.g., 3 epochs).\n    * Monitor validation loss. If it doesn't improve for 'patience' epochs, stop training.\n* **Dropout:** Dropout is a powerful regularization technique that combats overfitting. Consider increasing the dropout rate slightly (e.g., from 0.1 to 0.2), but be careful not to make it too high. \n* **Weight Decay:** Weight decay (L2 regularization) penalizes large weights, which can prevent overfitting. Experiment with a slightly lower value (e.g., 0.005) to see if it helps.\n* **Layer Normalization:** Ensure you have layer normalization within the transformer layers. It helps with the stability of the model and its ability to learn.\n\n**3. Optimization Strategies**\n\n* **Learning Rate Scheduler:**  Using a learning rate scheduler can significantly help with training stability and accuracy. \n    * **ReduceLROnPlateau:** This scheduler automatically reduces the learning rate when the validation loss plateaus. \n    * **CosineAnnealingLR:** This scheduler gradually reduces the learning rate to a minimum value and then cycles through that range.\n* **Optimizer:**  Experiment with different optimizers:\n    * **AdamW:** A good default choice, often better than Adam.\n    * **SGD (Stochastic Gradient Descent):**  With momentum and a good learning rate scheduler, SGD can sometimes achieve excellent results.\n\n**4. Fine-Tuning**\n\n* **Fine-Tuning Pretrained DistilBERT:** You're using DistilBERT. Fine-tuning the pre-trained model for your specific task is crucial.\n    * **Frozen Layers:** Consider freezing some of the DistilBERT layers (e.g., the embedding layers) during the initial epochs and gradually unfreezing more layers as training progresses. This helps prevent the model from forgetting its pre-trained knowledge.\n* **Learning Rate:** Experiment with different learning rates for fine-tuning. Start with a lower rate (e.g., 2e-5).\n\n**5. Data Augmentation**\n\n* **IMDB Dataset:**  Data augmentation techniques can increase the diversity of your training data.\n    * **Synonym Replacement:** Replace words with their synonyms (e.g., \"good\" -> \"excellent,\" \"bad\" -> \"terrible\").\n    * **Back Translation:** Translate the text to another language (e.g., Spanish), then back to English.\n    * **Random Deletion:** Randomly delete words from the text.\n\n**Code Example (PyTorch):**\n\n```python\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Load IMDB dataset and tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n# ... (load IMDB dataset, split into train/val/test)\n\n# Define the model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\n# Define the optimizer and learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n\n# Define the training loop\ndef train(epoch):\n    model.train()\n    # ... (iterate through training data, calculate loss, update weights)\n\n# Define the validation loop\ndef evaluate(epoch):\n    model.eval()\n    # ... (iterate through validation data, calculate accuracy, update scheduler)\n\n# Training epochs\nfor epoch in range(20):\n    train(epoch)\n    evaluate(epoch)\n    if early_stopping.early_stop:\n        break\n\n# Save the best performing model\ntorch.save(model.state_dict(), 'best_model.pt')\n\n# Evaluate the model on the test set\n# ... (load test set, evaluate accuracy)\n```\n\n**Important Notes:**\n\n* **Hyperparameter Tuning:**  Experiment with the suggested hyperparameters, and use grid search or random search to find the optimal combination for your data.\n* **Experimentation:**  Don't be afraid to experiment with different architectural and training choices. The best approach might not be the most straightforward one.\n* **Data Quality:** Ensure your data is clean, accurate, and diverse. This is the foundation of a good model.\n\nLet me know if you have a specific architecture or training settings in mind. I can help you analyze the training details and suggest more targeted modifications. \n\nUsage Metadata: prompt_token_count: 364852\ncandidates_token_count: 1217\ntotal_token_count: 366069\ncached_content_token_count: 364726\n\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"### Next Steps:\n- Apply changes and test for improved accuracy.\n- Aim for 90% or higher accuracy.","metadata":{}},{"cell_type":"code","source":"message = (\n f\"\"\"\n### Role Specification:  \nYou are an expert machine learning engineer specializing in deep learning and optimization for NLP tasks. Your goal is to analyze, improve, and suggest enhancements to a transformer-based text classification model trained on the IMDb dataset in PyTorch. The aim is to increase its accuracy to at least 90% while optimizing computation time and cost.\n\n### Objective:  \n- **Task 1:** Analyze the current model setup (training metadata, architecture summary, tuning variables, and accuracy over epochs).  \n- **Task 2:** Recommend necessary changes to the model architecture or training procedure to achieve higher accuracy.  \n- **Task 3:** Provide code modifications and parameter adjustments for:  \n  1. Adding or modifying layers.  \n  2. Optimization techniques (e.g., advanced optimizers, learning rate schedules).  \n  3. Fine-tuning methods to improve results.  \n  4. Implementing early stopping to reduce computation time and cost.  \n\n### Input Information:  \n- **Dataset:** IMDb Text Classification  \ntraining details are attached as file\n\n---\n\n### Expected Output:  \nProvide a structured response in the following format:\n\n1. **Analysis of Current Model:**  \n   - Summarize observations based on the provided training metadata and model summary.  \n   - Highlight potential bottlenecks or limitations affecting accuracy.\n\n2. **Recommended Changes:**  \n   - **Architecture Enhancements:** Propose layer modifications, additions, or alternative architectures.  \n   - **Hyperparameter Tuning:** Suggest adjustments to learning rate, batch size, optimizers, or other parameters.  \n   - **Regularization and Fine-tuning:** Recommend regularization techniques or better pre-trained weights for transfer learning.  \n\n3. **Code Modifications:**  \n   - Include modified code snippets for PyTorch reflecting suggested changes.\n\n4. **Optimization Techniques:**  \n   - Outline steps for implementing learning rate schedules, gradient clipping, or other computational optimizations.  \n   - Provide early stopping logic to balance performance and training time.\n\n5. **Expected Outcomes:**  \n   - Explain how each suggestion will likely improve accuracy or reduce computation costs.\n\n6. **Future Recommendations:**  \n   - Long-term strategies for continuous model improvement. \"\"\"\n                 )\n\nresponse = model.generate_content([message],safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:HarmBlockThreshold.BLOCK_NONE\n        })  # Send the first message\n\n\n# Cache metadata for further use\nprint(\"Usage Metadata:\", response.usage_metadata)\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-26T17:39:46.488301Z","iopub.execute_input":"2024-11-26T17:39:46.488683Z","iopub.status.idle":"2024-11-26T17:39:59.131648Z","shell.execute_reply.started":"2024-11-26T17:39:46.488651Z","shell.execute_reply":"2024-11-26T17:39:59.130748Z"},"trusted":true},"outputs":[{"name":"stdout","text":"## Analysis of Current Model\n\n**Training Metadata:**\n\n- **Dataset:** IMDb Text Classification\n- **Epochs:** 100\n- **Batch Size:** 32\n- **Learning Rate:** 2e-5\n- **Optimizer:** AdamW\n- **Weight Decay:** 0.01\n- **Dropout:** 0.1\n- **Accuracy:** The model reaches an accuracy of 85.72% at the end of training.\n- **Loss:** The loss function is a Binary Cross Entropy.\n- **Training Time:** (Not specified)\n\n**Model Summary:**\n\n- **Architecture:** DistilBERT\n- **Number of Parameters:** 30,623,360 (approx.)\n- **Layers:** 6 Transformer layers followed by a pre-classifier and a classifier.\n\n**Observations:**\n\n- **Loss Curve:**  The loss curve provided shows that the model is learning and improving significantly in the initial epochs. However, it starts plateauing around epoch 20, suggesting potential overfitting or reaching the model's capacity.\n- **Accuracy:** While an accuracy of 85.72% is decent, it falls short of the target 90%. This could be due to various factors, including the model architecture, hyperparameter tuning, and overfitting.\n- **Bottlenecks:** The model's relatively simple architecture with only 6 transformer layers might not be sufficient to capture complex relationships in the IMDb dataset. The learning rate plateau also indicates that the current optimizer and learning rate schedule might not be optimally tuned.\n\n**Limitations:**\n\n- **Overfitting:** The model might be overfitting to the training data, leading to poor generalization.\n- **Limited Model Capacity:** The DistilBERT architecture with 6 layers could be insufficient for the dataset's complexity.\n\n## Recommended Changes\n\n**Architecture Enhancements:**\n\n1. **Increase Transformer Layers:** Consider increasing the number of transformer layers in the DistilBERT model to 8 or 10. This will allow the model to learn more complex patterns in the data.\n2. **Add Attention Layers:** Explore adding additional attention layers between transformer blocks or after the pre-classifier. This can improve focus on relevant information within the input sequence.\n3. **Experiment with Larger Transformer Models:** Consider using larger pre-trained transformer models like BERT or RoBERTa. These models have more parameters and potentially better capture semantic relationships.\n\n**Hyperparameter Tuning:**\n\n1. **Reduce Learning Rate:** Experiment with smaller learning rates (e.g., 1e-5 or 5e-6) to fine-tune the model in later epochs and prevent overfitting.\n2. **Adjust Batch Size:** Increase the batch size (e.g., 64) to potentially speed up training while also stabilizing the training process.\n3. **Experiment with Different Optimizers:** Try optimizers like AdamW with a different learning rate schedule like cosine annealing or ReduceLROnPlateau.\n\n**Regularization and Fine-tuning:**\n\n1. **Dropout Regularization:** Increase the dropout rate to 0.2 or 0.3 to further prevent overfitting.\n2. **L1/L2 Regularization:** Apply L1/L2 regularization on the model's weights during training to encourage sparsity and prevent overfitting.\n3. **Fine-tune Pre-trained Weights:** Explore fine-tuning the pre-trained weights of the transformer model with a more suitable learning rate (e.g., 1e-6) during the final epochs to adapt the model to the specific dataset.\n\n## Code Modifications\n\n```python\n# Increase transformer layers in DistilBERT model\nfrom transformers import DistilBertModel\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", num_hidden_layers=10)\n\n# Add attention layer between transformer blocks\nfrom transformers import Attention\nattention_layer = Attention(768, 768)\n\n# Experiment with larger transformer models\nfrom transformers import BertModel\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\n# Reduce learning rate and adjust batch size\noptimizer = AdamW(model.parameters(), lr=1e-5)\nbatch_size = 64\n\n# Apply dropout regularization\nmodel.config.dropout = 0.3\n\n# Apply L1/L2 regularization\nfrom torch import nn\nmodel = nn.Sequential(\n    model,\n    nn.Linear(768, 256),\n    nn.Dropout(0.3),\n    nn.Linear(256, 2),\n)\n\n# Fine-tune pre-trained weights\nfor param in model.distilbert.parameters():\n    param.requires_grad = False\nfor param in model.pre_classifier.parameters():\n    param.requires_grad = True\nfor param in model.classifier.parameters():\n    param.requires_grad = True\noptimizer = AdamW(model.parameters(), lr=1e-6)\n```\n\n## Optimization Techniques\n\n**Learning Rate Schedules:**\n\n- **Cosine Annealing:** This schedule gradually decreases the learning rate over time, allowing for more precise fine-tuning and preventing abrupt changes that can cause oscillations in the loss function.\n- **ReduceLROnPlateau:** This schedule dynamically adjusts the learning rate when the loss plateaus, enabling better performance without the need for manual intervention.\n\n```python\n# Cosine annealing learning rate schedule\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nscheduler = CosineAnnealingLR(optimizer, T_max=50)\n\n# ReduceLROnPlateau learning rate schedule\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n```\n\n**Gradient Clipping:**\n\n- This technique helps to prevent exploding gradients, which can lead to instability during training. It clips the gradients to a specific threshold value.\n\n```python\n# Gradient clipping\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n```\n\n**Early Stopping:**\n\n- This technique monitors the validation loss and stops training when the loss fails to improve after a certain number of epochs. This reduces computation time and cost by preventing unnecessary training.\n\n```python\n# Early stopping logic\nbest_val_loss = float('inf')\npatience = 10\nfor epoch in range(epochs):\n    # Train the model\n    ...\n    # Evaluate on validation set\n    ...\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience = 10\n    else:\n        patience -= 1\n        if patience == 0:\n            print(\"Early stopping at epoch {}\".format(epoch))\n            break\n```\n\n## Expected Outcomes\n\n- **Higher Accuracy:** The proposed changes are expected to significantly improve the model's accuracy. Increasing the number of transformer layers, adding attention layers, and using larger pre-trained transformer models will enable the model to learn more complex relationships in the data.\n- **Reduced Computation Time:** Increasing the batch size and using effective learning rate schedules like cosine annealing will reduce the number of training iterations and speed up the process.\n- **Reduced Computation Costs:** Early stopping will prevent unnecessary training and save on computational resources, leading to reduced costs.\n\n## Future Recommendations\n\n- **Data Augmentation:** Implement data augmentation techniques to increase the size and diversity of the training data. This can include techniques like back-translation, synonym replacement, and paraphrasing.\n- **Ensemble Learning:** Train multiple models with different architectures and hyperparameter settings and combine their predictions using ensemble methods. This can improve the model's robustness and accuracy.\n- **Hyperparameter Search:** Implement a more systematic hyperparameter search approach using techniques like Grid Search, Random Search, or Bayesian Optimization to identify optimal settings for the model.\n- **Continuous Monitoring:** Monitor the model's performance on a regular basis and re-train it when necessary using the latest training data or techniques.\n\nBy implementing these recommendations, it is possible to achieve the target accuracy of 90% while optimizing the model's training time and cost. Continuous model improvement through regular monitoring, data augmentation, and hyperparameter tuning will ensure optimal performance over time.\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"print(\"Usage Metadata:\", response.usage_metadata)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:30.810811Z","iopub.execute_input":"2024-11-26T17:42:30.811450Z","iopub.status.idle":"2024-11-26T17:42:30.816039Z","shell.execute_reply.started":"2024-11-26T17:42:30.811414Z","shell.execute_reply":"2024-11-26T17:42:30.815178Z"}},"outputs":[{"name":"stdout","text":"Usage Metadata: prompt_token_count: 365195\ncandidates_token_count: 1779\ntotal_token_count: 366974\ncached_content_token_count: 364726\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Implementing the changes suggested above","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import DistilBertModel, DistilBertTokenizer\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport numpy as np\nfrom datasets import load_dataset  # Hugging Face Datasets library\n\n# 1. Data Preparation (using Hugging Face Datasets for easier handling)\ndataset = load_dataset(\"imdb\")\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\ntrain_dataset, test_dataset = tokenized_datasets[\"train\"], tokenized_datasets[\"test\"]\n\n# Convert to PyTorch Datasets and DataLoaders\ntrain_dataset = TensorDataset(torch.tensor(train_dataset['input_ids']), torch.tensor(train_dataset['attention_mask']), torch.tensor(train_dataset['label']))\ntest_dataset = TensorDataset(torch.tensor(test_dataset['input_ids']), torch.tensor(test_dataset['attention_mask']), torch.tensor(test_dataset['label']))\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# 2. Model Definition (with improved classifier)\nclass SentimentClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.drop = nn.Dropout(p=0.3)  # Increased dropout for regularization\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        ).last_hidden_state[:, 0]  # CLS token for pooled output\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# 3. Training Loop (with early stopping and improved optimization)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = SentimentClassifier(n_classes=2).to(device)\nepochs = 5\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\ntotal_steps = len(train_loader) * epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)\n\nbest_accuracy = 0\npatience = 3  # Early stopping patience\nepochs_no_improve = 0\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for input_ids, attention_mask, labels in train_loader:\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        loss = loss_fn(outputs, labels)\n\n        total_loss += loss.item()\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    avg_train_loss = total_loss / len(train_loader)\n\n    # Validation\n    model.eval()\n    correct_predictions = 0\n    with torch.no_grad():\n        for input_ids, attention_mask, labels in test_loader:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n\n            _, preds = torch.max(outputs, dim=1)\n            correct_predictions += torch.sum(preds == labels)\n\n    accuracy = correct_predictions.double() / len(test_dataset)\n    print(f'Epoch: {epoch+1},  Train Loss: {avg_train_loss:.4f},  Test Accuracy: {accuracy:.4f}')\n\n    # Early Stopping\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        epochs_no_improve = 0\n        # Save the best model\n        torch.save(model.state_dict(), 'best_model.bin')\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(\"Early stopping triggered!\")\n            break\n","metadata":{"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fdd91e44aac48c2a055311f071ca3db"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1,  Train Loss: 0.3590,  Test Accuracy: 0.8726\nEpoch: 2,  Train Loss: 0.1982,  Test Accuracy: 0.8792\nEpoch: 3,  Train Loss: 0.0855,  Test Accuracy: 0.8734\nEpoch: 4,  Train Loss: 0.0357,  Test Accuracy: 0.8760\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"**It gave an accuracy of 87% after the changes made**","metadata":{}},{"cell_type":"markdown","source":"## Metadata Issue:\nWhile attempting to upload the model's metadata, it failed to read the file. As a result, we could only rely on the model summary and trained accuracies to retrieve the information. This limitation becomes a significant issue, particularly for handling the metadata of a relatively small model. When scaling to even larger models, this challenge will likely intensify, making it crucial to find a solution for efficiently managing and reading model metadata in such cases.","metadata":{}},{"cell_type":"markdown","source":" # Environmental Analysis Using Google Gemini 1.5 Pro\n## Overview\n","metadata":{}},{"cell_type":"markdown","source":"This section demonstrates the use of Google's Gemini 1.5 Pro model to assess the environmental impact of industries in the Amaravati region.The analysis was focused on evaluating the city's air quality, soil conditions, and weather patterns, followed by recommendations for improving the environment. The task also involved recommending suitable crops for local farmers. \n\nIt focuses on:\n\n>Evaluating environmental conditions.\n\n>Proposing mitigation strategies for industries.\n\n>Implementing context caching to efficiently handle long-context scenarios and avoid redundant computations.\n","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngemini_api= user_secrets.get_secret(\"api-5\")\ngenai.configure(api_key=gemini_api)\n\ngeneration_config = {\n  \"temperature\": 1,\n  \"top_p\": 0.95,\n  \"top_k\": 40,\n  \"max_output_tokens\": 8192,\n  \"response_mime_type\": \"text/plain\",\n}\n\n\nmodel_bot = genai.GenerativeModel(\n  model_name=\"gemini-1.5-pro-002\",\n  generation_config=generation_config,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:18:29.422720Z","iopub.execute_input":"2024-11-25T17:18:29.423042Z","iopub.status.idle":"2024-11-25T17:18:29.641961Z","shell.execute_reply.started":"2024-11-25T17:18:29.423016Z","shell.execute_reply":"2024-11-25T17:18:29.640970Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"chat_session = model_bot.start_chat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:18:33.802795Z","iopub.execute_input":"2024-11-25T17:18:33.803148Z","iopub.status.idle":"2024-11-25T17:18:33.807438Z","shell.execute_reply.started":"2024-11-25T17:18:33.803110Z","shell.execute_reply":"2024-11-25T17:18:33.806665Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"# Context Caching Mechanism","metadata":{}},{"cell_type":"markdown","source":"To optimize the notebook's performance by reusing previously processed or uploaded data.","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nfrom kaggle_secrets import UserSecretsClient\nimport pickle  # For saving cache to a file\n\n# Set up API key\nuser_secrets = UserSecretsClient()\ngemini_api = user_secrets.get_secret(\"api-5\")\ngenai.configure(api_key=gemini_api)\n\n# Configuration\ngeneration_config = {\n    \"temperature\": 1,\n    \"top_p\": 0.95,\n    \"top_k\": 40,\n    \"max_output_tokens\": 8192,\n    \"response_mime_type\": \"text/plain\",\n}\n\nmodel_bot = genai.GenerativeModel(\n    model_name=\"gemini-1.5-pro-002\",\n    generation_config=generation_config,\n)\nchat_session = model_bot.start_chat()\n\n# Caching Setup\nCACHE_FILE = \"/kaggle/working/context_cache.pkl\"\ncontext_cache = {}\n\n# Load cache if exists\nif os.path.exists(CACHE_FILE):\n    with open(CACHE_FILE, \"rb\") as cache_file:\n        context_cache = pickle.load(cache_file)\n\ndef upload_file_with_cache(file_path, mime_type):\n    \"\"\"Upload a file to Gemini with caching.\"\"\"\n    if file_path in context_cache:\n        print(f\"Using cached upload for: {file_path}\")\n        return context_cache[file_path]\n    else:\n        print(f\"Uploading: {file_path}\")\n        file_upload = genai.upload_file(path=file_path, mime_type=mime_type)\n        context_cache[file_path] = file_upload\n        # Save cache\n        with open(CACHE_FILE, \"wb\") as cache_file:\n            pickle.dump(context_cache, cache_file)\n        return file_upload\n\n# Upload files with caching\nair_quality = upload_file_with_cache('/kaggle/input/air-quality/AQI_daily_city_level_vijayawada_2023_vijayawada_2023 (1).csv', 'text/csv')\nweather = upload_file_with_cache('/kaggle/input/amaravathi-weather/Amaravathi_weather.csv', 'text/csv')\nsoil = upload_file_with_cache('/kaggle/input/water-and-soil-examination-report/2018DecAnExtensiveExaminationofWaterQualityandSoil.pdf', 'application/pdf')\n\n# Context and prompt\nfactory_details = \"\"\"You are an environmental analyst... (as in original code)\"\"\"\ncached_aqi_analysis = context_cache.get(\"aqi_analysis\", \"\")\n\nmessage = f\"\"\"\nYou are an environmentalist tasked with examining the environmental status of a city named Amaravati...\n\n**Available Data:**\n1. **Air Quality Data (AQI Levels):**\n   - Annual average AQI:\n     - 2010: 82 (Moderate)\n     - 2015: 97 (Moderate to Poor)\n     - 2020: 110 (Poor)\n     - 2023: 125 (Poor to Very Poor)\n\n2. **Weather Data:** (Details as in original code)\n\n3. **Soil Data:** (Details as in original code)\n\n4. **Previous AQI Analysis:**\n{cached_aqi_analysis}\n\"\"\"\n\n# Analyze data and save context\nif not cached_aqi_analysis:\n    print(\"Performing AQI analysis...\")\n    aqi_analysis = chat_session.send_message(message)\n    context_cache[\"aqi_analysis\"] = aqi_analysis\n    # Save cache\n    with open(CACHE_FILE, \"wb\") as cache_file:\n        pickle.dump(context_cache, cache_file)\nelse:\n    print(\"Using cached AQI analysis.\")\ncached_aqi_analysis = context_cache.get(\"aqi_analysis\",\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:21:54.902919Z","iopub.execute_input":"2024-11-25T17:21:54.903631Z","iopub.status.idle":"2024-11-25T17:21:55.122024Z","shell.execute_reply.started":"2024-11-25T17:21:54.903595Z","shell.execute_reply":"2024-11-25T17:21:55.121064Z"}},"outputs":[{"name":"stdout","text":"Using cached upload for: /kaggle/input/air-quality/AQI_daily_city_level_vijayawada_2023_vijayawada_2023 (1).csv\nUsing cached upload for: /kaggle/input/amaravathi-weather/Amaravathi_weather.csv\nUsing cached upload for: /kaggle/input/water-and-soil-examination-report/2018DecAnExtensiveExaminationofWaterQualityandSoil.pdf\nUsing cached AQI analysis.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"factory_details =f\"\"\"You are an environmental analyst. The following industries operate in the Amaravati region and their descriptions are provided:\n\n1. Amaravathi Textiles – Known for producing high-quality cotton yarn and textiles, this company operates a large spinning facility in Prakasam District.\n2. Arjas Steel – A prominent manufacturer of special steel, catering primarily to the automotive and broader manufacturing sectors.\n3. Champion Filters Manufacturing Company – Specializes in industrial filtration products, including automatic vertical and basket strainers.\n4. Dora Plastics – Focuses on PPE kits, hypodermic syringes, and needles, particularly for healthcare applications.\n5. Gypelite India – A leader in plasterboard and gypsum products, offering materials used in construction and wall finishing.\n6. Konda Industries – Provides electrical wiring solutions, such as house wiring, hook-up wires, and multicore cables.\n7. Kusalava International – Known for its manufacturing of cylinder liners, pistons, and related auto components.\n8. Mangal Industries – Produces various auto components, storage solutions, and battery components.\n9. Visakha Dairy – While based outside Amaravati, it serves the state with fortified dairy products.\n\n**Task 1: Analyze Environmental Impact**\n- Assess the potential environmental impact of each industry, focusing on:\n  - Energy consumption.\n  - Emissions (air, water, or soil pollution).\n  - Resource utilization (e.g., raw materials, water usage).\n\n**Task 2: Propose Mitigation Strategies**\n- Suggest tailored strategies for each company to reduce its environmental footprint. Ensure these strategies are practical and industry-specific.\n\n**Task 3: Context Integration**\n- If additional insights from previous analyses are available (e.g., local environmental conditions, previous pollution reports), incorporate them into this analysis.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:23:37.417818Z","iopub.execute_input":"2024-11-25T17:23:37.418156Z","iopub.status.idle":"2024-11-25T17:23:37.423450Z","shell.execute_reply.started":"2024-11-25T17:23:37.418126Z","shell.execute_reply":"2024-11-25T17:23:37.422510Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"## Preparing the Input Message\nThe next step was to prepare the input message for the AI model. This message contained the context and task information. The primary task was to evaluate the environmental conditions of Amaravati, taking into account various data inputs such as air quality, weather, and soil reports.\n\nHere is the structure of the input message:\n","metadata":{}},{"cell_type":"code","source":"cached_aqi_analysis=\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:23:48.668389Z","iopub.execute_input":"2024-11-25T17:23:48.669250Z","iopub.status.idle":"2024-11-25T17:23:48.672965Z","shell.execute_reply.started":"2024-11-25T17:23:48.669196Z","shell.execute_reply":"2024-11-25T17:23:48.671851Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"**Prompt Design**\nWe use detailed prompts to guide the model in analyzing the industries' environmental impact and proposing mitigation strategies:","metadata":{}},{"cell_type":"code","source":"message = (\n    \"You are an environmentalist tasked with examining the environmental status of a city named Amaravati, \"\n    \"situated in Andhra Pradesh, India, on the banks of the Krishna River.\\n\\n\"\n\n    \"**Available Data:**\\n\"\n    \"1. **Air Quality Data (AQI Levels):**\\n\"\n    \"   - Annual average AQI:\\n\"\n    \"     - 2010: 82 (Moderate)\\n\"\n    \"     - 2015: 97 (Moderate to Poor)\\n\"\n    \"     - 2020: 110 (Poor)\\n\"\n    \"     - 2023: 125 (Poor to Very Poor)\\n\\n\"\n\n    \"2. **Weather Data:**\\n\"\n    \"   - Temperature trends:\\n\"\n    \"     - Average summer temperature: 38°C (with peaks up to 45°C).\\n\"\n    \"     - Average winter temperature: 20°C.\\n\"\n    \"   - Rainfall:\\n\"\n    \"     - Annual average rainfall: 1100 mm, with high variability in monsoon months.\\n\"\n    \"     - Reports of heavy rainfall causing urban flooding in 2018 and 2022.\\n\\n\"\n\n    \"3. **Soil Data:**\\n\"\n    \"   - Soil types: Loamy soil in agricultural areas, clay-heavy soil in urban and semi-urban regions.\\n\"\n    \"   - Key findings from soil examination:\\n\"\n    \"     - Low nitrogen levels and reduced organic matter in agricultural zones.\\n\"\n    \"     - High levels of construction debris in urban soil samples.\\n\"\n    \"     - Salinity issues in areas near the Krishna River.\\n\\n\"\n\n    \"**Region Characteristics:**\\n\"\n    \"- The city has numerous construction sites and factories that contribute to environmental challenges.\\n\"\n    \"- The factories operating in the region are:\\n\"\n    f\"{factory_details}\\n\\n\"\n\n    \"**Tasks:**\\n\"\n    \"1. **Environmental Analysis**:\\n\"\n    \"   - Assess the air quality, weather patterns, and soil condition of the city.\\n\"\n    \"   - Identify the major environmental problems in the region caused by construction activities and factory operations.\\n\\n\"\n\n    \"2. **Mitigation Strategies**:\\n\"\n    \"   - Recommend actionable steps to reduce environmental problems specific to this region, such as:\\n\"\n    \"     - Reducing pollution caused by factories and construction sites.\\n\"\n    \"     - Improving soil quality and sustainable land use.\\n\"\n    \"     - Managing weather-related risks, such as heatwaves or heavy rainfall.\\n\\n\"\n\n    \"3. **Agricultural Recommendations**:\\n\"\n    \"   - Based on the soil examination report, provide a detailed schedule and a list of crops suitable for cultivation by farmers in the region.\\n\"\n    \"   - Include strategies to improve agricultural productivity while maintaining environmental sustainability.\\n\\n\"\n\n    \"**Output Format:**\\n\"\n    \"{\\n\"\n    '  \"Environmental Analysis\": {\\n'\n    '    \"Air Quality\": \"...\",\\n'\n    '    \"Weather Patterns\": \"...\",\\n'\n    '    \"Soil Condition\": \"...\"\\n'\n    \"  },\\n\"\n    '  \"Mitigation Strategies\": [\\n'\n    '    \"Step 1: ...\",\\n'\n    '    \"Step 2: ...\"\\n'\n    \"  ],\\n\"\n    '  \"Agricultural Recommendations\": {\\n'\n    '    \"Recommended Crops\": [\\n'\n    '      {\"Crop\": \"Crop Name\", \"Reason\": \"Suitability based on soil and weather\"}\\n'\n    \"    ],\\n\"\n    '    \"Cultivation Schedule\": [\\n'\n    '      {\"Month\": \"Month Name\", \"Crop\": \"Crop Name\", \"Activities\": \"Details of farming activities\"}\\n'\n    \"    ]\\n\"\n    \"  }\\n\"\n    \"}\\n\"\n)\n\nresponse = chat_session.send_message(\n    [message],\n    safety_settings={\n        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE\n    }\n)\n\nprint(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:29:25.644682Z","iopub.execute_input":"2024-11-25T17:29:25.645021Z","iopub.status.idle":"2024-11-25T17:32:30.706197Z","shell.execute_reply.started":"2024-11-25T17:29:25.644992Z","shell.execute_reply":"2024-11-25T17:32:30.705162Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n  \"Environmental Analysis\": {\n    \"Air Quality\": \"Amaravati's air quality is deteriorating, trending from 'Moderate' in 2010 to 'Poor to Very Poor' in 2023.  The increasing AQI suggests rising particulate matter, likely from industrial emissions, construction dust, and vehicular traffic.\",\n    \"Weather Patterns\": \"The region experiences extreme temperatures, with hot summers (peaking at 45°C) and mild winters.  Heavy, variable monsoon rainfall contributes to urban flooding, highlighting inadequate drainage infrastructure.\",\n    \"Soil Condition\": \"Agricultural soil suffers from low nitrogen and organic matter, impacting fertility. Construction debris contaminates urban soil, while salinity affects areas near the Krishna River. These issues hinder agricultural productivity and pose risks to the river ecosystem.\"\n  },\n  \"Mitigation Strategies\": [\n    \"Step 1: **Implement stricter emission standards for factories.** Enforce continuous monitoring and reporting of emissions. Promote the adoption of cleaner technologies (e.g., scrubbers, electrostatic precipitators) and renewable energy sources.\",\n    \"Step 2: **Control construction dust.** Mandate dust suppression measures like water spraying, covering exposed soil, and using barriers around construction sites. Implement green building practices.\",\n    \"Step 3: **Improve urban drainage.** Invest in upgrading drainage systems to manage heavy rainfall and prevent flooding. Implement permeable pavements and rainwater harvesting systems.\",\n    \"Step 4: **Promote sustainable waste management.**  Implement waste segregation, recycling programs, and composting initiatives to reduce landfill burden and soil contamination from construction debris.\",\n    \"Step 5: **Remediate contaminated soil.**  In agricultural areas, promote organic farming practices, including composting, crop rotation, and green manure to improve soil health. Phytoremediation can be explored for areas with salinity or heavy metal contamination.\",\n    \"Step 6: **Develop a comprehensive urban greening plan.** Increase tree cover to mitigate heat island effect, improve air quality, and enhance biodiversity.\"\n  ],\n  \"Agricultural Recommendations\": {\n    \"Recommended Crops\": [\n      {\"Crop\": \"Millets (Sorghum, Pearl Millet)\", \"Reason\": \"Drought-tolerant and suitable for low-nitrogen soils.\"},\n      {\"Crop\": \"Pulses (Lentils, Chickpeas)\", \"Reason\": \"Nitrogen-fixing, improving soil fertility, and relatively drought-resistant.\"},\n      {\"Crop\": \"Oilseeds (Sesame, Sunflower)\", \"Reason\": \"Tolerant to varying rainfall and can be grown in rotation with other crops.\"},\n      {\"Crop\": \"Vegetables (Okra, Brinjal)\", \"Reason\": \"Can be grown with appropriate water management strategies during non-peak monsoon seasons.\"}\n    ],\n    \"Cultivation Schedule\": [\n      {\"Month\": \"June-September (Monsoon)\", \"Crop\": \"Millets/Pulses\", \"Activities\": \"Sowing, weeding, and initial irrigation as needed.\"},\n      {\"Month\": \"October-December (Post-Monsoon)\", \"Crop\": \"Oilseeds/Vegetables\", \"Activities\": \"Sowing, irrigation, and pest management.\"},\n      {\"Month\": \"January-May (Winter/Pre-Monsoon)\", \"Crop\": \"Short-duration vegetables/Fallow period with green manure\", \"Activities\": \"Sowing, irrigation, harvesting, and soil preparation with green manure for the next season.\"}\n    ]\n  },\n  \"Industrial Impact Analysis and Mitigation\": {\n    \"Amaravathi Textiles\": {\"Impact\": \"Water pollution (dyeing), energy consumption.\", \"Mitigation\": \"Water recycling and treatment plants, energy-efficient machinery, explore natural dyes.\"},\n    \"Arjas Steel\": {\"Impact\": \"Air pollution (particulate matter, SOx), high energy consumption.\", \"Mitigation\": \"Air pollution control devices, energy efficiency measures, explore sustainable steelmaking processes.\"},\n    \"Champion Filters\": {\"Impact\": \"Waste generation (filter materials).\", \"Mitigation\": \"Waste minimization strategies, explore recyclable filter materials.\"},\n    \"Dora Plastics\": {\"Impact\": \"Plastic waste.\", \"Mitigation\": \"Explore biodegradable plastics, promote responsible disposal and recycling of medical waste.\"},\n    \"Gypelite India\": {\"Impact\": \"Dust generation, energy consumption.\", \"Mitigation\": \"Dust suppression measures, energy-efficient processes, use of recycled gypsum.\"},\n    \"Konda Industries\": {\"Impact\": \"Heavy metal waste (from wire production).\", \"Mitigation\": \"Proper waste management and treatment of heavy metal waste.\"},\n    \"Kusalava International & Mangal Industries\": {\"Impact\": \"Metal waste, energy consumption.\", \"Mitigation\": \"Waste minimization, recycling of metal scraps, energy-efficient manufacturing processes.\"},\n    \"Visakha Dairy\": {\"Impact\": \"Wastewater (from dairy processing).\", \"Mitigation\": \"Wastewater treatment plant, anaerobic digestion for biogas generation.\"}\n  }\n}\n\n```\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"response = chat_session.send_message(\"what is the data u needed to further analysis and plans\",safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:HarmBlockThreshold.BLOCK_NONE\n        })  \nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:32:43.662326Z","iopub.execute_input":"2024-11-25T17:32:43.662664Z","iopub.status.idle":"2024-11-25T17:32:56.095599Z","shell.execute_reply.started":"2024-11-25T17:32:43.662636Z","shell.execute_reply":"2024-11-25T17:32:56.094611Z"}},"outputs":[{"name":"stdout","text":"To perform a more comprehensive analysis and develop more robust plans for Amaravati, the following additional data would be beneficial:\n\n**Environmental Data:**\n\n* **More granular air quality data:**  Real-time monitoring of specific pollutants (PM2.5, PM10, SO2, NOx, Ozone) at multiple locations across the city. This helps pinpoint pollution hotspots and sources.\n* **Water quality data:**  Regular testing of the Krishna River and other water bodies for pollutants (heavy metals, pesticides, industrial effluents, fecal coliforms) to assess the impact of industrial and agricultural activities.\n* **Groundwater data:**  Monitoring of groundwater levels and quality to assess the impact of over-extraction and potential contamination from industrial or agricultural runoff.\n* **Biodiversity data:**  Surveys of flora and fauna to understand the region's biodiversity and identify potential threats from pollution, habitat loss, and climate change.\n* **Waste generation and management data:**  Detailed information on the types and quantities of waste generated by different sectors (residential, commercial, industrial) and the current waste management practices.\n* **Traffic data:**  Traffic volume and congestion data to assess the contribution of vehicular emissions to air pollution.\n\n**Socioeconomic Data:**\n\n* **Demographic data:**  Population growth projections to estimate future demands on resources and infrastructure.\n* **Economic activity data:**  Information on the major industries, their economic contribution, and employment patterns.\n* **Public health data:**  Data on respiratory illnesses and other health issues potentially linked to environmental pollution.\n* **Community perception data:**  Surveys and consultations with local communities to understand their concerns and priorities regarding environmental issues.\n\n**Infrastructure Data:**\n\n* **Existing infrastructure:**  Detailed maps of water supply and sanitation systems, transportation networks, and waste management facilities.\n* **Planned infrastructure:**  Information on upcoming development projects and their potential environmental impacts.\n\n**Policy and Regulatory Data:**\n\n* **Existing environmental regulations:**  Information on local, state, and national environmental laws and standards.\n* **Enforcement mechanisms:**  Information on the capacity and effectiveness of environmental regulatory agencies.\n\n**Other Data:**\n\n* **Land use maps:**  Detailed land use maps showing agricultural areas, urban areas, industrial zones, and natural habitats.\n* **Climate change projections:**  Data on projected temperature changes, rainfall patterns, and extreme weather events to assess future climate change impacts.\n* **Best practices:**  Research on successful environmental mitigation strategies implemented in other cities with similar challenges.\n\n\nBy gathering this additional data, we can:\n\n* Develop more targeted and effective mitigation strategies.\n* Prioritize actions based on the most pressing environmental issues.\n* Monitor the effectiveness of interventions over time.\n* Develop a more comprehensive and sustainable development plan for Amaravati.\n\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"# Web scraping of air quality data of amaravati","metadata":{}},{"cell_type":"markdown","source":"We leverage Google Gemini 1.5 Pro to perform web scraping to gather real-time Air Quality Index (AQI) and Weather Data for Amaravati, India. This enables us to:\n\n>Analyze the current environmental conditions\n\n>Identify trends in AQI and weather patterns\n\n>Provide actionable insights and recommendations for improving air quality and mitigating weather-related challenges\n","metadata":{}},{"cell_type":"code","source":"air =\"\"\"You are an environmental data analyst tasked with interpreting real-time and historical air quality and weather data for Amaravati, India. Below is the detailed information extracted from the website:\n\n**Location: Secretariat, Amaravati**\n\n1. **Air Quality Index (AQI)**:\n   - Current AQI: 65 (Moderate)\n   - Components:\n     - PM2.5: 65 (Min: 5, Max: 161)\n     - PM10: 45 (Min: 14, Max: 97)\n     - O3: 23 (Min: 7, Max: 31)\n     - NO2: 4 (Min: 3, Max: 44)\n     - SO2: 6 (Min: 5, Max: 69)\n     - CO: 7 (Min: 0, Max: 15)\n\n2. **Weather Data**:\n   - Temperature: Current: 27°C (Min: 27°C, Max: 28°C)\n   - Pressure: Current: 754 hPa (Min: 754 hPa, Max: 759 hPa)\n   - Humidity: Current: 59% (Min: 42%, Max: 86%)\n   - Wind Speed: Current: 4 km/h (Min: 0 km/h, Max: 4 km/h)\n\n**Tasks**:\n1. **Analyze Current Environmental Status**:\n   - Provide insights into the air quality and weather conditions based on the provided data.\n   - Identify the key pollutants contributing to AQI levels and their likely sources in the region (e.g., PM2.5 from construction or industrial activity).\n\n2. **Historical Trends Analysis**:\n   - Use the min and max values of AQI components and weather metrics to infer trends or anomalies in environmental conditions over time.\n\n3. **Recommendations**:\n   - Propose actionable steps to reduce air pollution, considering the identified key pollutants.\n   - Suggest methods for mitigating weather-related issues (e.g., high humidity or low wind speed).\n\n4. **Forecasting and Strategies**:\n   - Predict potential environmental risks based on the data (e.g., worsening air quality due to stagnant winds or temperature inversion).\n   - Recommend long-term strategies to improve air quality and resilience to weather variability.\n\n**Output Format**:\n{\n  \"Environmental Analysis\": {\n    \"Current Status\": {\n      \"Air Quality\": \"Detailed analysis of current AQI and pollutants.\",\n      \"Weather Conditions\": \"Insights on temperature, pressure, humidity, and wind speed.\"\n    },\n    \"Trends Analysis\": \"Inferences from historical min/max data.\"\n  },\n  \"Recommendations\": {\n    \"Air Quality Improvements\": [\n      \"Actionable step 1\",\n      \"Actionable step 2\"\n    ],\n    \"Weather Mitigation\": [\n      \"Actionable step 1\",\n      \"Actionable step 2\"\n    ]\n  },\n  \"Forecast and Strategies\": [\n    \"Potential risk 1 and its mitigation\",\n    \"Potential risk 2 and its mitigation\"\n  ]\n}\n|\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:33:13.837420Z","iopub.execute_input":"2024-11-25T17:33:13.838141Z","iopub.status.idle":"2024-11-25T17:33:13.843697Z","shell.execute_reply.started":"2024-11-25T17:33:13.838107Z","shell.execute_reply":"2024-11-25T17:33:13.842496Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"In this step, we pass the web-scraped air quality and weather data for Amaravati to Google Gemini 1.5 Pro. The model is tasked with extracting the information and structuring it into a well-defined JSON schema for better readability and further processing.","metadata":{}},{"cell_type":"code","source":"# Ensure 'air' contains the web-scraped data\nair = \"\"\"\nLocation: Secretariat, Amaravati\nAQI Components:\n- PM2.5: 65 (Min: 5, Max: 161)\n- PM10: 45 (Min: 14, Max: 97)\n- O3: 23 (Min: 7, Max: 31)\n- NO2: 4 (Min: 3, Max: 44)\n- SO2: 6 (Min: 5, Max: 69)\n- CO: 7 (Min: 0, Max: 15)\nWeather:\n- Temperature: 27°C (Min: 27°C, Max: 28°C)\n- Pressure: 754 hPa (Min: 754 hPa, Max: 759 hPa)\n- Humidity: 59% (Min: 42%, Max: 86%)\n- Wind: 4 km/h (Min: 0 km/h, Max: 4 km/h)\n\"\"\"\n\n# Send the request to the chat session\nresponse = chat_session.send_message(\n    f\"Here is the air quality data for Amaravati:\\n{air}\\n\\n\"\n    \"Extract the data and structure it into a JSON format with the following schema:\\n\"\n    \"{\"\n    \"  'Location': 'string',\"\n    \"  'AQI_Components': {\"\n    \"    'PM2.5': {'current': int, 'min': int, 'max': int},\"\n    \"    'PM10': {'current': int, 'min': int, 'max': int},\"\n    \"    'O3': {'current': int, 'min': int, 'max': int},\"\n    \"    'NO2': {'current': int, 'min': int, 'max': int},\"\n    \"    'SO2': {'current': int, 'min': int, 'max': int},\"\n    \"    'CO': {'current': int, 'min': int, 'max': int}\"\n    \"  },\"\n    \"  'Weather': {\"\n    \"    'Temperature': {'current': int, 'min': int, 'max': int},\"\n    \"    'Pressure': {'current': int, 'min': int, 'max': int},\"\n    \"    'Humidity': {'current': int, 'min': int, 'max': int},\"\n    \"    'Wind': {'current': int, 'min': int, 'max': int}\"\n    \"  }\"\n    \"}\\n\\n\"\n    \"Ensure the JSON is well-structured and matches the schema.\",\n    safety_settings={\n        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n    }\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:33:25.843054Z","iopub.execute_input":"2024-11-25T17:33:25.843417Z","iopub.status.idle":"2024-11-25T17:33:34.722159Z","shell.execute_reply.started":"2024-11-25T17:33:25.843386Z","shell.execute_reply":"2024-11-25T17:33:34.721426Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"response.text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:33:45.797538Z","iopub.execute_input":"2024-11-25T17:33:45.798258Z","iopub.status.idle":"2024-11-25T17:33:45.804085Z","shell.execute_reply.started":"2024-11-25T17:33:45.798199Z","shell.execute_reply":"2024-11-25T17:33:45.803215Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"'```json\\n{\\n  \"Location\": \"Secretariat, Amaravati\",\\n  \"AQI_Components\": {\\n    \"PM2.5\": {\\n      \"current\": 65,\\n      \"min\": 5,\\n      \"max\": 161\\n    },\\n    \"PM10\": {\\n      \"current\": 45,\\n      \"min\": 14,\\n      \"max\": 97\\n    },\\n    \"O3\": {\\n      \"current\": 23,\\n      \"min\": 7,\\n      \"max\": 31\\n    },\\n    \"NO2\": {\\n      \"current\": 4,\\n      \"min\": 3,\\n      \"max\": 44\\n    },\\n    \"SO2\": {\\n      \"current\": 6,\\n      \"min\": 5,\\n      \"max\": 69\\n    },\\n    \"CO\": {\\n      \"current\": 7,\\n      \"min\": 0,\\n      \"max\": 15\\n    }\\n  },\\n  \"Weather\": {\\n    \"Temperature\": {\\n      \"current\": 27,\\n      \"min\": 27,\\n      \"max\": 28\\n    },\\n    \"Pressure\": {\\n      \"current\": 754,\\n      \"min\": 754,\\n      \"max\": 759\\n    },\\n    \"Humidity\": {\\n      \"current\": 59,\\n      \"min\": 42,\\n      \"max\": 86\\n    },\\n    \"Wind\": {\\n      \"current\": 4,\\n      \"min\": 0,\\n      \"max\": 4\\n    }\\n  }\\n}\\n```\\n'"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"air_out ={\"location\": \"Secretariat, Amaravati, India\",\"date\": \"Wednesday, Nov 13th 2024\",  \"time\": \"15:00\",\"overall_aqi\": 65,\"overall_aqi_category\": \"Moderate\",\"temperature\": 27, \"pollutants\": {\"PM2.5\": {\"aqi\": 65, \"min_past_2_days\": 5, \"max_past_2_days\": 161 }, \"PM10\": { \"aqi\": 45,\"min_past_2_days\": 14,\"max_past_2_days\": 97},\"O3\": {    \"aqi\": 23,\"min_past_2_days\": 7,\"max_past_2_days\": 31},\"NO2\": {\"aqi\": 4,\"min_past_2_days\": 3,\"max_past_2_days\": 44},\"SO2\": {\"aqi\": 6, \"min_past_2_days\": 5,\"max_past_2_days\": 69 },\"CO\": {\"aqi\": 7, \"min_past_2_days\": 0, \"max_past_2_days\": 15   } },\"weather\":{\"pressure\":754,\"min_pressure_past_2_days\":754,\"max_pressure_past_2_days\":759,\"humidity\":59,\"min_humidity_past_2_days\":42,\"max_humidity_past_2_days\":86,\"wind\":4,\"min_wind_past_2_days\":0,\"max_wind_past_2_days\":4  }}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:34:00.477784Z","iopub.execute_input":"2024-11-25T17:34:00.478456Z","iopub.status.idle":"2024-11-25T17:34:00.484262Z","shell.execute_reply.started":"2024-11-25T17:34:00.478423Z","shell.execute_reply":"2024-11-25T17:34:00.483077Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"**Caching Intermediate Results**\nTo optimize computation and reuse results, a context cache is implemented to store intermediate outputs for:\n\n>*Air Quality Analysis*: Insights into AQI, pollutants, and mitigation strategies\n>\n>*Soil and Water Analysis*: Assessment of soil health, water resources, and sustainable farming practices\n>\n>*Weather Trends Analysis*: Examination of weather patterns and their impact on agriculture and infrastructure\n","metadata":{}},{"cell_type":"markdown","source":"Using the cached results, a comprehensive report is created with actionable recommendations for pollution reduction, sustainable agriculture, and urban development.","metadata":{}},{"cell_type":"code","source":"# Step 1: Define cache dictionary to store intermediate results\ncontext_cache = {}\n\n# Step 2: Process Air Quality Data and Cache Results\nif \"air_quality_analysis\" not in context_cache:\n    air_quality_prompt = (\n        f\"Here is the air quality data of Amaravati:\\n{air_out}\\n\\n\"\n        \"Analyze the data and provide insights into the air quality index (AQI), \"\n        \"major pollutants, and their possible sources. Provide recommendations \"\n        \"to improve air quality.\"\n    )\n    air_quality_response = chat_session.send_message(air_quality_prompt)\n    context_cache[\"air_quality_analysis\"] = air_quality_response.text  # Corrected attribute access\n\n# Step 3: Process Soil Data and Cache Results\nif \"soil_analysis\" not in context_cache:\n    soil_prompt = (\n        \"You are provided with the soil and water data for Amaravati. Analyze the soil quality, \"\n        \"availability of water resources, and socio-economic factors affecting farming. \"\n        \"Suggest strategies to improve soil health and water management for sustainable agriculture.\"\n    )\n    soil_response = chat_session.send_message(soil_prompt)\n    context_cache[\"soil_analysis\"] = soil_response.text  # Corrected attribute access\n\n# Step 4: Process Weather Data and Cache Results\nif \"weather_trends\" not in context_cache:\n    weather_prompt = (\n        \"Based on Amaravati's daily weather reports from 2010 onwards, analyze weather patterns and trends, \"\n        \"including temperature, precipitation, and wind data. Provide insights into how these trends \"\n        \"impact agriculture and urban infrastructure.\"\n    )\n    weather_response = chat_session.send_message(weather_prompt)\n    context_cache[\"weather_trends\"] = weather_response.text  # Corrected attribute access\n\n# Step 5: Generate Final Report Using Cached Data\nfinal_report_prompt = (\n    \"You are an environmentalist tasked with creating a comprehensive environmental report for Amaravati. \"\n    \"Use the following analyses:\\n\\n\"\n    f\"1. Air Quality Analysis:\\n{context_cache['air_quality_analysis']}\\n\\n\"\n    f\"2. Soil and Water Analysis:\\n{context_cache['soil_analysis']}\\n\\n\"\n    f\"3. Weather Trends Analysis:\\n{context_cache['weather_trends']}\\n\\n\"\n    \"Combine these insights to provide detailed recommendations for reducing pollution, improving agriculture, \"\n    \"and achieving sustainable urban development in Amaravati.\"\n)\nfinal_report_response = chat_session.send_message(final_report_prompt)\n\n# Step 6: Output Final Report\nprint(\"Final Environmental Report for Amaravati:\")\nprint(final_report_response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:37:09.840832Z","iopub.execute_input":"2024-11-25T17:37:09.841691Z","iopub.status.idle":"2024-11-25T17:38:13.889926Z","shell.execute_reply.started":"2024-11-25T17:37:09.841657Z","shell.execute_reply":"2024-11-25T17:38:13.888928Z"}},"outputs":[{"name":"stdout","text":"Final Environmental Report for Amaravati:\n## Amaravati Comprehensive Environmental Report\n\nThis report assesses the current environmental status of Amaravati, focusing on air quality, soil and water resources, and weather patterns.  Based on the analyses provided, we offer recommendations for sustainable development.\n\n**I. Air Quality:**\n\n**Current Status:**  Amaravati's air quality is currently classified as \"Moderate\" with an AQI of 65, primarily driven by PM2.5 pollution. While not at alarming levels, this poses risks, especially to sensitive groups. Fluctuations in PM2.5 levels suggest variable pollution sources or meteorological influences.\n\n**Major Concerns:**  PM2.5 and PM10 exceed ideal levels, potentially stemming from vehicular and industrial emissions, construction activities, and agricultural practices like residue burning.\n\n**Recommendations:**\n\n* **Implement a comprehensive air quality management plan:** This plan should include stricter emission standards for vehicles and industries, promoting cleaner technologies, and incentivizing the adoption of electric vehicles.\n* **Control construction dust:** Enforce dust suppression measures at construction sites, including covering exposed soil, using water sprays, and erecting barriers.\n* **Improve traffic management:** Invest in public transportation, promote cycling and walking, and optimize traffic flow to reduce congestion and vehicular emissions.\n* **Expand green cover:** Increase green spaces and tree cover in the city to act as natural air purifiers.\n* **Enhance air quality monitoring and data transparency:** Expand the air quality monitoring network and make real-time data publicly available to raise awareness and inform decision-making.\n\n\n**II. Soil and Water Resources:**\n\n**(Data pending)**  A detailed analysis of soil and water resources is crucial for sustainable agriculture. The following data points are required for a complete assessment:  [List of data points from Soil and Water Analysis section, as previously provided].  Once this data is available, a comprehensive analysis will be conducted and tailored recommendations provided.  However, some general recommendations can be made:\n\n* **Promote sustainable agricultural practices:** Encourage organic farming, crop rotation, and reduced reliance on chemical fertilizers and pesticides.\n* **Improve water use efficiency:** Implement water-efficient irrigation techniques like drip irrigation and rainwater harvesting.\n* **Protect and restore water bodies:** Prevent pollution of the Krishna River and other water bodies by enforcing regulations on industrial and agricultural discharge.\n\n**III. Weather Patterns and Trends:**\n\n**(Data pending)**  A thorough analysis of long-term weather data is crucial for understanding climate change impacts and adapting agricultural practices and urban infrastructure. We require daily weather reports from 2010 onwards to perform a comprehensive analysis.  General considerations include:\n\n* **Assess climate change vulnerability:** Analyze historical weather data to identify trends in temperature, precipitation, and extreme weather events. This information can help assess the city's vulnerability to climate change impacts.\n* **Develop climate adaptation strategies:** Based on the climate vulnerability assessment, develop adaptation strategies for agriculture and urban infrastructure. These might include drought-resistant crops, improved drainage systems, and heat-resistant building materials.\n\n**IV. Integrated Recommendations for Sustainable Development:**\n\n* **Develop a comprehensive sustainability plan:** This plan should integrate air quality management, water resource management, climate change adaptation, and sustainable agricultural practices.\n* **Promote green building practices:** Encourage the construction of energy-efficient and environmentally friendly buildings.\n* **Invest in renewable energy:** Transition to renewable energy sources to reduce reliance on fossil fuels and lower greenhouse gas emissions.\n* **Strengthen community engagement:**  Involve local communities in the planning and implementation of sustainable development initiatives.\n\n\n**Next Steps:**\n\n1. **Data collection:** Gather the necessary soil, water, and weather data to complete the analysis.\n2. **Detailed analysis:** Conduct a thorough analysis of the collected data to identify specific environmental challenges and opportunities.\n3. **Stakeholder engagement:** Consult with relevant stakeholders, including government agencies, local communities, and businesses, to develop and implement a comprehensive sustainability plan.\n4. **Monitoring and evaluation:** Regularly monitor and evaluate the effectiveness of implemented strategies and adapt them as needed.\n\n\n\n\nThis report provides a framework for addressing Amaravati's environmental challenges. By collecting the necessary data and implementing the recommendations outlined above, Amaravati can move towards a more sustainable and resilient future.\n\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"The final environmental report synthesizes air quality, soil, water, and weather analyses to propose targeted solutions for Amaravati's sustainable growth.However it could be more detailed.","metadata":{}},{"cell_type":"markdown","source":"## Experimentation with Gemini as an Environmentalist: Focus on Delhi","metadata":{}},{"cell_type":"markdown","source":"In this subsection, we explore the capabilities of Gemini 1.5 Pro as an environmental analyst by analyzing Delhi's air quality and weather conditions. Leveraging real-time web-scraped data, Gemini provides valuable insights into air pollution levels, weather trends, and actionable recommendations for improving environmental conditions.","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\nimport pickle  # For saving cache to a file\nfrom kaggle_secrets import UserSecretsClient\n\n# Set up API key\nuser_secrets = UserSecretsClient()\ngemini_api = user_secrets.get_secret(\"api-5\")\ngenai.configure(api_key=gemini_api)\n\n# Configuration for Gemini API\ngeneration_config = {\n    \"temperature\": 1,\n    \"top_p\": 0.95,\n    \"top_k\": 40,\n    \"max_output_tokens\": 8192,\n    \"response_mime_type\": \"text/plain\",\n}\n\n# Create model instance and start chat session\nmodel_bot = genai.GenerativeModel(\n    model_name=\"gemini-1.5-pro-002\",\n    generation_config=generation_config,\n)\nchat_session = model_bot.start_chat()\n\n# Caching Setup\nCACHE_FILE = \"/kaggle/working/context_cache.pkl\"\ncontext_cache = {}\n\n# Load cache if it exists\nif os.path.exists(CACHE_FILE):\n    with open(CACHE_FILE, \"rb\") as cache_file:\n        context_cache = pickle.load(cache_file)\n\ndef upload_file_with_cache(file_path, mime_type):\n    \"\"\"Upload a file to Gemini with caching.\"\"\"\n    if file_path in context_cache:\n        print(f\"Using cached upload for: {file_path}\")\n        return context_cache[file_path]\n    else:\n        print(f\"Uploading: {file_path}\")\n        file_upload = genai.upload_file(path=file_path, mime_type=mime_type)\n        context_cache[file_path] = file_upload\n        # Save cache\n        with open(CACHE_FILE, \"wb\") as cache_file:\n            pickle.dump(context_cache, cache_file)\n        return file_upload\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:14:24.122848Z","iopub.execute_input":"2024-11-25T18:14:24.123526Z","iopub.status.idle":"2024-11-25T18:14:24.325396Z","shell.execute_reply.started":"2024-11-25T18:14:24.123491Z","shell.execute_reply":"2024-11-25T18:14:24.324631Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"air_quality = upload_file_with_cache('/kaggle/input/air-quality/AQI_daily_city_level_vijayawada_2023_vijayawada_2023 (1).csv', 'text/csv')\nweather = upload_file_with_cache('/kaggle/input/amaravathi-weather/Amaravathi_weather.csv', 'text/csv')\nsoil = upload_file_with_cache('/kaggle/input/water-and-soil-examination-report/2018DecAnExtensiveExaminationofWaterQualityandSoil.pdf', 'application/pdf')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:15:01.846657Z","iopub.execute_input":"2024-11-25T18:15:01.847012Z","iopub.status.idle":"2024-11-25T18:15:01.852116Z","shell.execute_reply.started":"2024-11-25T18:15:01.846980Z","shell.execute_reply":"2024-11-25T18:15:01.851100Z"}},"outputs":[{"name":"stdout","text":"Using cached upload for: /kaggle/input/air-quality/AQI_daily_city_level_vijayawada_2023_vijayawada_2023 (1).csv\nUsing cached upload for: /kaggle/input/amaravathi-weather/Amaravathi_weather.csv\nUsing cached upload for: /kaggle/input/water-and-soil-examination-report/2018DecAnExtensiveExaminationofWaterQualityandSoil.pdf\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"#Context and Prompt for AQI, Weather, and Soil Data\nfactory_details = \"\"\"You are an environmental analyst tasked with analyzing the environmental status of a city named Amaravati. \nYou have the following resources available for analysis:\n1. Air Quality Index (AQI) data from 2010-2023.\n2. Weather data for Amaravathi, including temperature, precipitation, and wind speed.\n3. Soil and water quality examination report.\nUse these resources to provide a detailed analysis and recommendations for sustainable urban development and agriculture.\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:16:07.831103Z","iopub.execute_input":"2024-11-25T18:16:07.831493Z","iopub.status.idle":"2024-11-25T18:16:07.835674Z","shell.execute_reply.started":"2024-11-25T18:16:07.831460Z","shell.execute_reply":"2024-11-25T18:16:07.834708Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"web =\"\"\"\"|     |     |     |     |\n| --- | --- | --- | --- |\n| [**New Delhi US Embassy** AQI](https://aqicn.org/city/india/new-delhi/us-embassy/ \"New Delhi US Embassy (नई दिल्ली अमेरिकी दूतावास)\"): New Delhi US Embassy Real-time Air Quality Index (AQI). | ![](https://aqicn.org/images/icons/p/mapb2.png) | ![](<Base64-Image-Removed>) | [![](<Base64-Image-Removed>)](https://aqicn.org/city/india/new-delhi/us-embassy/m/ \"view in full screen\") |\n\n|     |     |\n| --- | --- |\n| 422 | Hazardous<br>Updated on Saturday 12:00<br>temperature: **27** °C |\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n| current | past 2 days | min | max |\n| PM2.5 AQI | 422 | ![New Delhi US Embassy, India PM25 (fine particulate matter)  measured by U.S. Embassy and Consulates  Air Quality Monitor in India. Values are converted to the US EPA AQI standard.](<Base64-Image-Removed>) | 167 | 547 |\n| Weather Information |\n| Temp. | 27 | ![New Delhi US Embassy, India  t (temp.)  measured by World Meteorological Organization - surface synoptic observations (WMO-SYNOP).](<Base64-Image-Removed>) | 17 | 30 |\n| Pressure | 1015 | ![New Delhi US Embassy, India  p (pressure:)  measured by World Meteorological Organization - surface synoptic observations (WMO-SYNOP).](<Base64-Image-Removed>) | 1013 | 1018 |\n| Humidity | 45 | ![New Delhi US Embassy, India  h (humidity)  measured by World Meteorological Organization - surface synoptic observations (WMO-SYNOP).](<Base64-Image-Removed>) | 28 | 93 |\n| Wind | 2 | ![New Delhi US Embassy, India  w (wind)  measured by World Meteorological Organization - surface synoptic observations (WMO-SYNOP).](<Base64-Image-Removed>) | 1 | 5 |\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:38:26.175856Z","iopub.execute_input":"2024-11-25T17:38:26.176448Z","iopub.status.idle":"2024-11-25T17:38:26.181214Z","shell.execute_reply.started":"2024-11-25T17:38:26.176414Z","shell.execute_reply":"2024-11-25T17:38:26.180202Z"}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"Source\n\nData Reference: New Delhi US Embassy Real-Time AQI\n\nThrough this analysis, Gemini highlights the severe air pollution levels in New Delhi, offering potential avenues for mitigation and environmental improvements.","metadata":{}},{"cell_type":"markdown","source":"After extracting air quality and weather data for New Delhi (US Embassy), the next step was to structure this data into a JSON format for further analysis. The following prompt was used to achieve this:","metadata":{}},{"cell_type":"code","source":"cached_aqi_analysis = context_cache.get(\"aqi_analysis\", \"\")\n\nmessage = f\"\"\"\nYou are an environmentalist tasked with examining the environmental status of a city named Amaravati.\n\n**Available Data:**\n1. **Air Quality Data (AQI Levels):**\n   - Annual average AQI:\n     - 2010: 82 (Moderate)\n     - 2015: 97 (Moderate to Poor)\n     - 2020: 110 (Poor)\n     - 2023: 125 (Poor to Very Poor)\n\n2. **Weather Data:**\n   (Details from Amaravathi_weather.csv)\n\n3. **Soil Data:**\n   (Details from the soil and water examination report)\n\n4. **Previous AQI Analysis:**\n{cached_aqi_analysis}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:16:59.531814Z","iopub.execute_input":"2024-11-25T18:16:59.532430Z","iopub.status.idle":"2024-11-25T18:16:59.537870Z","shell.execute_reply.started":"2024-11-25T18:16:59.532395Z","shell.execute_reply":"2024-11-25T18:16:59.536927Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"message = (\n   f\"\"\"I have provided the web-scraped air quality data for New Delhi in the following format:\n\n{web}\n\nPlease extract the information and structure it in a JSON format. Ensure the JSON includes the following details:\n\n1. **General Information**:\n   - City name: \"New Delhi\"\n   - Source: \"US Embassy\"\n   - AQI Level and Description: (e.g., 422, Hazardous)\n   - Last Updated: (e.g., Saturday 12:00)\n\n2. **Pollutant Details**:\n   - PM2.5: Include current, past 2 days, minimum, and maximum values.\n   - Temperature, Pressure, Humidity, and Wind: Include current, minimum, and maximum values.\n\nOutput a JSON object with clear key-value pairs representing this data.\nn\"\"\"\n\n\n)\n\nresponse = chat_session.send_message([message],safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:HarmBlockThreshold.BLOCK_NONE\n        })  # Send the first message\n\n\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T17:38:34.620446Z","iopub.execute_input":"2024-11-25T17:38:34.621239Z","iopub.status.idle":"2024-11-25T17:38:41.718378Z","shell.execute_reply.started":"2024-11-25T17:38:34.621184Z","shell.execute_reply":"2024-11-25T17:38:41.717561Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n  \"General Information\": {\n    \"City name\": \"New Delhi\",\n    \"Source\": \"US Embassy\",\n    \"AQI Level\": 422,\n    \"AQI Description\": \"Hazardous\",\n    \"Last Updated\": \"Saturday 12:00\"\n  },\n  \"Pollutant Details\": {\n    \"PM2.5\": {\n      \"Current\": 422,\n      \"Past 2 Days\": {\n        \"Min\": 167,\n        \"Max\": 547\n      }\n    },\n    \"Temperature\": {\n      \"Current\": 27,\n      \"Min\": 17,\n      \"Max\": 30\n    },\n    \"Pressure\": {\n      \"Current\": 1015,\n      \"Min\": 1013,\n      \"Max\": 1018\n    },\n    \"Humidity\": {\n      \"Current\": 45,\n      \"Min\": 28,\n      \"Max\": 93\n    },\n    \"Wind\": {\n      \"Current\": 2,\n      \"Min\": 1,\n      \"Max\": 5\n    }\n  }\n}\n```\n\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"The structured data was stored in the following JSON format:","metadata":{}},{"cell_type":"code","source":"json = \"\"\"{\n  \"location\": \"New Delhi US Embassy\",\n  \"overall_aqi\": 422,\n  \"overall_quality\": \"Hazardous\",\n  \"last_updated\": \"Saturday 12:00\",\n  \"temperature\": 27,\n  \"temperature_unit\": \"°C\",\n  \"pollutants\": {\n    \"PM2.5\": {\n      \"current_aqi\": 422,\n      \"min\": 167,\n      \"max\": 547\n    }\n  },\n  \"weather\": {\n    \"Temperature\": {\n      \"current\": 27,\n      \"min\": 17,\n      \"max\": 30,\n      \"unit\": \"°C\"\n    },\n    \"Pressure\": {\n      \"current\": 1015,\n      \"min\": 1013,\n      \"max\": 1018,\n      \"unit\": \"hPa\" \n    },\n    \"Humidity\": {\n      \"current\": 45,\n      \"min\": 28,\n      \"max\": 93,\n      \"unit\": \"%\"\n    },\n    \"Wind\": {\n      \"current\": 2,\n      \"min\": 1,\n      \"max\": 5,\n      \"unit\": \"m/s\"\n    }\n  }\n}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:17:21.357841Z","iopub.execute_input":"2024-11-25T18:17:21.358531Z","iopub.status.idle":"2024-11-25T18:17:21.362733Z","shell.execute_reply.started":"2024-11-25T18:17:21.358498Z","shell.execute_reply":"2024-11-25T18:17:21.361798Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"This structured format enables efficient storage, querying, and analysis of air quality and weather metrics for actionable insights.","metadata":{}},{"cell_type":"code","source":"if not cached_aqi_analysis:\n    print(\"Performing AQI analysis...\")\n    aqi_analysis = chat_session.send_message(message, files=[air_quality, weather, soil])\n    context_cache[\"aqi_analysis\"] = aqi_analysis\n    # Save cache\n    with open(CACHE_FILE, \"wb\") as cache_file:\n        pickle.dump(context_cache, cache_file)\nelse:\n    print(\"Using cached AQI analysis.\")\n\ncached_aqi_analysis = context_cache.get(\"aqi_analysis\", \"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:17:50.957648Z","iopub.execute_input":"2024-11-25T18:17:50.957996Z","iopub.status.idle":"2024-11-25T18:17:50.963818Z","shell.execute_reply.started":"2024-11-25T18:17:50.957968Z","shell.execute_reply":"2024-11-25T18:17:50.962934Z"}},"outputs":[{"name":"stdout","text":"Using cached AQI analysis.\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"# Step 4: Generate Comprehensive Environmental Report for Amaravati\nfinal_report_prompt = f\"\"\"\nYou are tasked with creating a comprehensive environmental report for Amaravati using the following insights:\n\n**1. AQI Analysis:**\n{cached_aqi_analysis}\n\n**2. Weather Trends Analysis:**\nUse the weather data and trends to discuss the impact on urban infrastructure and agriculture.\n\n**3. Soil and Water Analysis:**\nDiscuss the impact of soil and water quality on farming, and suggest sustainable practices to improve agricultural productivity.\n\nProvide recommendations for reducing pollution, enhancing agricultural output, and ensuring sustainable urban development.\n\"\"\"\n\n# Now we upload the files before sending the message\nair_quality_upload = upload_file_with_cache('/kaggle/input/air-quality/AQI_daily_city_level_vijayawada_2023_vijayawada_2023 (1).csv', 'text/csv')\nweather_upload = upload_file_with_cache('/kaggle/input/amaravathi-weather/Amaravathi_weather.csv', 'text/csv')\nsoil_upload = upload_file_with_cache('/kaggle/input/water-and-soil-examination-report/2018DecAnExtensiveExaminationofWaterQualityandSoil.pdf', 'application/pdf')\n\n# Send the final message without the 'context' argument, since it is handled implicitly\nfinal_report_response = chat_session.send_message(final_report_prompt)\n\n# Step 5: Output the final report\nprint(\"Final Environmental Report for Amaravati:\")\nprint(final_report_response.text)\n\n# Save updated cache\nwith open(CACHE_FILE, \"wb\") as cache_file:\n    pickle.dump(context_cache, cache_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:24:09.161087Z","iopub.execute_input":"2024-11-25T18:24:09.161921Z","iopub.status.idle":"2024-11-25T18:24:29.146681Z","shell.execute_reply.started":"2024-11-25T18:24:09.161885Z","shell.execute_reply":"2024-11-25T18:24:29.145782Z"}},"outputs":[{"name":"stdout","text":"Using cached upload for: /kaggle/input/air-quality/AQI_daily_city_level_vijayawada_2023_vijayawada_2023 (1).csv\nUsing cached upload for: /kaggle/input/amaravathi-weather/Amaravathi_weather.csv\nUsing cached upload for: /kaggle/input/water-and-soil-examination-report/2018DecAnExtensiveExaminationofWaterQualityandSoil.pdf\nFinal Environmental Report for Amaravati:\n## Environmental Report: Amaravati\n\nThis report assesses the environmental status of Amaravati, focusing on air quality, weather trends, soil and water resources, and their interconnected impact on urban development and agriculture.\n\n**1. Air Quality Analysis:**\n\nThe provided AQI data reveals a worrying trend of deteriorating air quality in Amaravati over the past 13 years, with the annual average AQI rising from 82 (Moderate) in 2010 to 125 (Unhealthy) in 2023. This signifies a substantial increase in harmful pollutants, posing significant risks to public health and the environment. Potential contributing factors include increased vehicular traffic, industrial emissions, construction activities, seasonal factors, and agricultural practices like crop residue burning.\n\n**Recommendations:**\n\n* **Comprehensive air quality monitoring and source apportionment:** Identify specific pollutants and their sources for targeted interventions.\n* **Stricter emission standards and enforcement:**  Implement and enforce regulations for industries, vehicles, and construction activities.\n* **Promote sustainable transportation:** Encourage public transport, cycling, and electric vehicle adoption.\n* **Develop green infrastructure:** Increase green spaces to absorb pollutants and improve air quality.\n* **Public awareness campaigns:** Educate citizens about air pollution risks and promote individual actions to reduce emissions.\n* **Inter-agency collaboration and regional cooperation:** Coordinate efforts between relevant agencies and neighboring regions to manage air pollution effectively.\n\n\n\n**2. Weather Trends Analysis:**\n\n**(Hypothetical example, as no weather data was provided):**\n\nAssuming a trend of increasing temperatures and erratic rainfall patterns, the impact on Amaravati could be substantial:\n\n* **Urban Infrastructure:**  Increased heat stress could strain power grids, increase water demand, and damage infrastructure like roads and buildings.  Erratic rainfall may lead to flooding and water scarcity issues.\n* **Agriculture:**  Higher temperatures and unpredictable rainfall can negatively impact crop yields, increase water stress for irrigation, and necessitate changes in cropping patterns and water management practices.\n\n**Recommendations:**\n\n* **Climate-resilient infrastructure:** Design and build infrastructure to withstand extreme weather events and temperature fluctuations.\n* **Water conservation and management:** Implement efficient irrigation techniques, rainwater harvesting, and water storage solutions.\n* **Drought-resistant crops:** Promote the cultivation of drought-resistant crop varieties to ensure food security.\n* **Weather forecasting and early warning systems:** Develop accurate weather forecasting and early warning systems to prepare for extreme events and minimize their impact.\n\n\n\n**3. Soil and Water Analysis:**\n\n**(Hypothetical example, as no soil and water data was provided):**\n\nAssuming concerns about soil degradation and water contamination, the following impacts on agriculture are likely:\n\n* **Reduced crop yields:**  Poor soil health and contaminated water can significantly reduce crop productivity and quality.\n* **Increased input costs:** Farmers may need to use more fertilizers and pesticides to compensate for degraded soil, increasing production costs.\n* **Health risks:** Contaminated water used for irrigation can pose risks to human health through the consumption of contaminated produce.\n\n**Recommendations:**\n\n* **Soil testing and nutrient management:** Conduct regular soil testing to determine nutrient deficiencies and implement targeted fertilization strategies.\n* **Promote organic farming practices:** Encourage the use of organic fertilizers, compost, and crop rotation to improve soil health and reduce chemical inputs.\n* **Integrated pest management:** Implement integrated pest management strategies to minimize pesticide use and protect beneficial insects.\n* **Water quality monitoring and treatment:** Regularly monitor water quality and implement appropriate treatment methods to ensure safe irrigation water.\n* **Watershed management:** Implement measures to protect and restore watersheds, ensuring a sustainable supply of clean water for agriculture.\n\n\n**4. Sustainable Urban Development:**\n\nIntegrating environmental considerations into urban planning is crucial for Amaravati's sustainable development. This includes promoting green building practices, incorporating green spaces and urban forests, managing waste effectively, and promoting sustainable transportation options.\n\n**Conclusion:**\n\nAddressing the environmental challenges facing Amaravati requires a holistic and integrated approach. By implementing the recommended measures, the city can mitigate environmental risks, enhance agricultural productivity, and ensure a sustainable and healthy future for its citizens.  Acquiring and analyzing detailed data on weather, soil, and water resources is crucial for developing more specific and effective strategies for Amaravati's environmental management. \n\n","output_type":"stream"}],"execution_count":71},{"cell_type":"markdown","source":"This generates a comprehensive environmental report for Delhi using multiple data sources, providing an analysis of air quality, weather, and socio-economic factors. However, the generated report could be more detailed by incorporating additional factors such as pollution sources, longer-term environmental trends, and more specific recommendations for each identified issue. The integration of real-time data from various sectors enables a more dynamic and actionable report, paving the way for better environmental management and sustainable city development.","metadata":{}},{"cell_type":"markdown","source":"#   Gemini For Music Notes or Tune Generation\n","metadata":{}},{"cell_type":"markdown","source":"This analysis takes a deep dive into the Carnatic music structure to craft a melody and arrangement for a Telugu song centered around the themes of love, longing, and devotion. The song's emotional context is translated into musical elements such as ragas, swara notation, ornamentation, and tempo changes. The goal is to infuse the song with emotional depth while adhering to the classical principles of Lalitha Sangeetham, blending tradition with expression","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngemini_api= user_secrets.get_secret(\"api-3\")\ngenai.configure(api_key=gemini_api)\n\ngeneration_config = {\n  \"temperature\": 1,\n  \"top_p\": 0.95,\n  \"top_k\": 40,\n  \"max_output_tokens\": 8192,\n  \"response_mime_type\": \"text/plain\",\n}\n\n\nmodel_bot = genai.GenerativeModel(\n  model_name=\"gemini-1.5-pro-002\",\n  generation_config=generation_config,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\n\n# ... (API key configuration)\n\nmodel = genai.GenerativeModel()\n\nlyrics = \"\"\"Kaalla patteelu ne pattkuntoone undipovalane manasu korukunee....\n\nRojoo mothamu ne\nnee pakkane  undipovalane\nvayasu vedukone....\n\nPremo..? emoo..?  prasnalaki\nnuvenemo... samadhaanamey...\n\nMayoo.. ? Maikamoo..?\nrenditiki ento.. emo..\nSambandhamey...\n\nKadhile kaalamtho pane..\n leni ee premaloo... Prapancham anthaa nve Ney.. naa Prapancham anthaa nveney...\n( Kaalla pattelu ne.. (2).)\n\nRendu kallathone.. nee roopanne..chusthoo.. undipovalane .. manasu korukunee..\n\nAntha andhamu ne.. aaksam thone.. thookameyalane\nSogasu vedukone...\n\nUndho..? ledhoo..? bramalaki\nnuvenemo ... Nijaroopamey..\n\nMarupamoo..? Muripemo...?\nNeno..? Nuvoo?\nIddarumoo epduavuthamo.. Yekamey..\n\"\"\"\n\n\nchat_session = model.start_chat()  # No initial messages needed. History is built as you send messages\n\n\nmessage = (  \"Imagine yourself as a  telugu Music composer\"\n    \"I'm going to give you the lyrics for an Telugu song I wrote. \"\n    \"\"\"Create a Carnatic music composition based on the  lyrics. The composition should express the emotion of [specified emotion, e.g., joy, sorrow, romance, devotion, etc.] and align with the genre of [specified genre, e.g., sad, happy, romantic, devotional]. The emotional tone of the lyrics should be the guiding factor in determining the raga, tala, and overall mood of the composition.\n    \n    1. **Emotion and Genre Match**:\n       - Based on the **emotion** and **genre** provided, choose a **raga** that best captures the mood:\n         - **Sadness** (Sad genre): Use **Yaman** or **Bhairavi** — these ragas evoke melancholy, introspection, and deep emotion.\n         - **Happiness** (Happy genre): Use **Hamsadhwani** or **Sindhu Bhairavi** — these ragas create an uplifting, joyous atmosphere.\n         - **Romantic** (Romantic genre): Use **Shankarabharanam** or **Desh** — these ragas evoke warmth, love, and affection.\n         - **Devotion** (Devotional genre): Use **Raghupriya** or **Vasantha** — these ragas create a meditative, reverent mood suitable for spiritual or devotional themes.\n    \n    2. **Composition Structure**:\n       - The composition should follow the traditional Carnatic format:\n         - **Alapana**: Start with an **Alapana** (improvised exposition of the raga) that introduces the melodic phrases of the raga in a free-flowing manner, setting the emotional tone. The Alapana should gradually build in intensity but stay true to the raga's emotional essence.\n         - **Tanam**: After the Alapana, include a **Tanam** (rhythmic improvisation) in the same raga. This section is where the raga's rhythmic elements are explored in a more structured, yet still improvised, manner. The Tanam should complement the emotional atmosphere created in the Alapana.\n         - **Kriti**: The main song, or **Kriti**, should be based on the provided lyrics and should showcase the raga's characteristic mood. The melody should be crafted with the swaras of the raga to enhance the emotional tone and flow of the lyrics.\n    \n    3. **Tempo and Tala**:\n       - **Tempo**: The tempo of the composition should be set to [specified tempo, e.g., slow, medium, fast]. The tempo will determine the speed of the **Alapana**, **Tanam**, and **Kriti**. For example:\n         - **Slow tempo**: Suitable for sad or devotional compositions (e.g., **Adi Tala** or **Trishtai Tala**).\n         - **Medium tempo**: Appropriate for romantic or reflective pieces (e.g., **Adi Tala**).\n         - **Fast tempo**: Best for energetic or joyful compositions (e.g., **Rupaka Tala**).\n       - **Tala (Rhythmic Cycle)**: Choose a **Tala** based on the raga and tempo:\n         - **Adi Tala** (8 beats) for a balanced rhythm, commonly used for medium-tempo compositions.\n         - **Rupaka Tala** (6 beats) for faster compositions that require a more energetic feel.\n         - **Trishtai Tala** (9 beats) for more intricate, slower compositions, often used for devotional or reflective pieces.\n    \n    4. **Swaras and Melodic Phrasing**:\n       - The **swaras** (Sa, Ri, Ga, Ma, Pa, Da, Ni) of the chosen raga should be incorporated into the melodic phrases, with special attention paid to the emotional expression of the lyrics. For example:\n         - In a **sad** composition (e.g., **Yaman**), the phrases should explore lower swaras, using longer, slower note durations to convey deep emotion.\n         - In a **happy** composition (e.g., **Hamsadhwani**), the swaras should be bright, with more rapid note changes and lighter melodic lines to convey joy and liveliness.\n    \n    5. **Emotional Flow & Dynamic Shifts**:\n       - The composition should begin slowly, with a reflective **Alapana** that introduces the raga’s emotional core. The intensity should gradually build with the **Tanam**, leading to a more energetic **Kriti**. \n       - The **Kriti** should highlight the emotional core of the lyrics, with the **swaras** in the raga providing a dynamic flow from the intro to the climax of the composition, before tapering off towards a peaceful or meditative conclusion in the **Outro**.\n    \n    6. **Final Composition Length and Structure**:\n       - The composition should be approximately [x minutes] in length, ensuring that the emotional progression of the piece (from intro to climax to outro) remains balanced and engaging. The **Alapana** should be around [x minutes], followed by a [x-minute] **Tanam**, and concluding with a [x-minute] **Kriti**.\n       - Ensure that the lyrics are seamlessly incorporated into the **Kriti**, with the melody and rhythm complementing the words, so that the emotional tone of the lyrics is amplified by the raga and swaras.\"\"\"\n    \n    \n    \"carnatic music for the song with pitch notations like `s3 g1`,style and ornament for the music to bby analysing the context of the music , and  for sure with mixing more ragalu and give instructions where I want to modulate and changes and give me timings and speeds  .\"\n    \"for example the output should be in this way- \"\n           \"\"\" * Melody: `s G m P d N S`\n* Harmony: `I - vi - IV - V - I`\n* Orchestration: Strings, flute, light percussion\n\n**Chorus**:\n\n* Melody: `S R g M P d n S`\n* Harmony: `I - IV - V - I`\n* Orchestration: Strings, flute, percussion\"\"\"\n)\n\nresponse = chat_session.send_message(message,safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:HarmBlockThreshold.BLOCK_NONE\n        })  # Send the first message\n\n\nresponse = chat_session.send_message(lyrics,safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:HarmBlockThreshold.BLOCK_NONE\n        }) # Send the lyrics as the next message\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T13:39:56.151932Z","iopub.execute_input":"2024-11-26T13:39:56.152311Z","iopub.status.idle":"2024-11-26T13:40:15.404607Z","shell.execute_reply.started":"2024-11-26T13:39:56.152279Z","shell.execute_reply":"2024-11-26T13:40:15.403576Z"}},"outputs":[{"name":"stdout","text":"**Verse 1:**\n\nKaalla patteelu ne pattkuntoone undipovalane manasu korukunee....\nRojoo mothamu ne\nnee pakkane  undipovalane\nvayasu vedukone....\n\n**Melody:**\n\n`S R2 G3 M1 P D2 N2 S`\n\n**Ornaments:**\n\n* Grace notes on 'R2' and 'D2'\n* Gamaka on 'N2'\n\n**Verse 2:**\n\nPremo..? emoo..?  prasnalaki\nnuvenemo... samadhaanamey...\n\n**Melody:**\n\n`M1 G3 R2 G3 M1 P D2 N2 S`\n\n**Ornaments:**\n\n* Swara combination 'GM'\n* Gamaka on 'N2'\n\n**Chorus:**\n\nKadhile kaalamtho pane..\nleni ee premaloo... Prapancham anthaa nve Ney.. naa Prapancham anthaa nveney...\n\n**Melody:**\n\n`P D2 N2 S R2 G3 M1 P D2 N2 S`\n\n**Ornaments:**\n\n* Grace notes on 'R2' and 'D2'\n* Gamaka on 'N2'\n\n**Verse 3:**\n\nRendu kallathone.. nee roopanne..chusthoo.. undipovalane .. manasu korukunee..\n\n**Melody:**\n\n`S R2 G3 M1 P D2 N2 S`\n\n**Ornaments:**\n\n* Grace notes on 'R2' and 'D2'\n* Gamaka on 'N2'\n\n**Verse 4:**\n\nAntha andhamu ne.. aaksam thone.. thookameyalane\nSogasu vedukone...\n\n**Melody:**\n\n`M1 G3 R2 G3 M1 P D2 N2 S`\n\n**Ornaments:**\n\n* Swara combination 'GM'\n* Gamaka on 'N2'\n\n**Chorus:**\n\nKadhile kaalamtho pane..\nleni ee premaloo... Prapancham anthaa nve Ney.. naa Prapancham anthaa nveney...\n\n**Melody:**\n\n`P D2 N2 S R2 G3 M1 P D2 N2 S`\n\n**Ornaments:**\n\n* Grace notes on 'R2' and 'D2'\n* Gamaka on 'N2'\n\n**Outro:**\n\nUndho..? ledhoo..? bramalaki\nnuvenemo ... Nijaroopamey..\n\nMarupamoo..? Muripemo...?\nNeno..? Nuvoo?\nIddarumoo epduavuthamo.. Yekamey..\n\n**Melody:**\n\n`S R2 G3 M1 P D2 N2 S`\n\n**Ornaments:**\n\n* Grace notes on 'R2' and 'D2'\n* Gamaka on 'N2'\n\n**Modulations and Changes:**\n\n* The composition briefly modulates to Hamir Kalyani (a related raga) during the second verse, using the swaras 'Ri2' and 'Ga2' to create a heightened sense of emotion.\n* Returns to Yaman seamlessly at the beginning of the chorus.\n* The tempo slightly accelerates during the outro, conveying a sense of urgency and closure.\n\n**Emotional Expression:**\n\nThe composition captures the longing, confusion, and intensity of the emotions expressed in the lyrics through the use of the Yaman raga, with its poignant and introspective melodies. The slow tempo and extended notes convey a sense of contemplation and introspection, allowing the emotions to linger and resonate with the listener.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"**This is the output we got:**\n\n**Ragam**: Hamsadhwani\r\n\r\n**Tala**: Adi Tala\r\n\r\n**Tempo**: Medium (100 BPM)\r\n\r\n**Verse 1**:\r\n\r\n* Melody: `s G m P d N S`\r\n* Harmony: `I - vi - IV - V - I`\r\n* Orchestration: Strings, flute, light percussion\r\n\r\n**Chorus**:\r\n\r\n* Melody: `S R g M P d n S`\r\n* Harmony: `I - IV - V - I`\r\n* Orchestration: Strings, flute, percussion\r\n\r\n**Verse 2**:\r\n\r\n* Melody: `s G m P d N S`\r\n* Harmony: `I - vi - IV - V - I`\r\n* Orchestration: Strings, flute, light percussion\r\n\r\n**Bridge**:\r\n\r\n* Melody: `S r G M P n S`\r\n* Harmony: `I - ii - IV - V - I`\r\n* Orchestration: Strings, flute, percussion, crescendo\r\n\r\n**Chorus**:\r\n\r\n* Melody: `S R g M P d n S`\r\n* Harmony: `I - IV - V - I`\r\n* Orchestration: Strings, flute, percussion\r\n\r\n**Verse 3**:\r\n\r\n* Melody: `s G m P d N S`\r\n* Harmony: `I - vi - IV - V - I`\r\n* Orchestration: Strings, flute, light percussion\r\n\r\n**Chorus**:\r\n\r\n* Melody: `S R g M P d n S`\r\n* Harmony: `I - IV - V - I`\r\n* Orchestration: Strings, flute, percussion\r\n\r\n**Outro**:\r\n\r\n* Melody: `s G m P d N S`\r\n* Harmony: `I - vi - IV - V - I`\r\n* Orchestration: Strings, flute, light percussion, decrescendo\r\n\r\n**Singer's Instructions**:\r\n\r\n* Sing with a warm, expressive tone.\r\n* Use vibrato and legato techniques to convey emotion.\r\n* Emphasize the gamakas on longer notes.\r\n\r\n**Orchestration Suggestions**:\r\n\r\n* Strings: Play the melody and provide soft accompaniment.\r\n* Flute: Add melodic embellishments and countermelodies.\r\n* Percussion: Use light tabla or mridangam beats to provide a subtle rhythmic foundation.","metadata":{}},{"cell_type":"markdown","source":"Insights from the Analysis:\nRaga Selection:\n\nHamsadhwani: A serene raga for peaceful introspection.\nYaman: Adds emotional depth and longing, perfect for expressing yearning.\nShankarabharanam: Uplifts the intensity of devotion and emotional intensity.\nKalyani: Used for modulation, representing a peak of emotion and emotional release.\nSwara Notation: Mapping Carnatic swaras to Western piano keys provides a universal framework for melody creation. Notes like S (C), M1 (F), and P (G) create the foundation for the melodic line.\n\nOrnamentation:\n\nMeend and Gamaka create smooth transitions between notes, bringing the emotional intensity to life, especially during key phrases in the lyrics like \"Kaalla patteelu\" and \"Prapancham anthaa nve Ney\".\nRhythm and Tempo:\n\nThe song follows a slow to moderate tempo progression, beginning with a calm pace (around 60 BPM) and gradually increasing as emotional intensity builds.\nAdhi Talam (8-beat cycle) provides a stable rhythm, with tempo modulations accentuating the emotional shifts in the lyrics.\nModulation and Emotional Flow:\n\nThe song flows from peaceful introspection to intense longing, and then to emotional peaks, reflecting the lyrical themes of love and desire.\nStrategic modulations between ragas like Hamsadhwani, Yaman, and Kalyani mirror the emotional journey of the lyrics.\nThis music analysis serves as a bridge between Carnatic tradition and modern Telugu song composition, ensuring that the song not only adheres to classical musical principles but also resonates with contemporary listeners. By thoughtfully integrating ragas, swaras, ornamentations, and rhythm, the song will express a deep emotional connection that mirrors the themes of love, longing, and devotion in the lyrics.","metadata":{}},{"cell_type":"markdown","source":"# Telugu Song Composition: \"Vennelave Vennelave\" ","metadata":{}},{"cell_type":"markdown","source":"**Song Overview**\nThis composition, based on the heartfelt lyrics of \"Vennelave Vennelave\", incorporates Carnatic music elements to enhance the emotional depth and expressiveness of the song. The primary themes of love, longing, and yearning are reflected through the use of ragas, swara notations, tempo modulations, and ornamentations. The musical style is influenced by Lalitha Sangeetham, with intricate ornamentations to create a tender and intimate atmosphere","metadata":{}},{"cell_type":"code","source":"lyrics = \"\"\"Vennelave Vennelave\n\nMinne Daati Vastaava\n\nVirahana Jodi Neevay Hey\n\nVennelave Vennelave\n\nMinne Daati Vastava\n\nVirahana Jodi Neevay hey\n\nInstrumentals\n\nVennelavey Vennelavey\n\nMinney Dati Vastaava\n\nVirahana Jodi Neeve\nNeeku Booloku La Kannu Sokemunde\n\nPoddu Telareloga Pampista\n\nIdi Sarasaala Toli Paruvaala\n\nJata Sayentra Saiyanna Mandaram\n\nIdi Sarasaala Toli Paruvala\n\nJata Sayentra Saiyanna Mandaram\n\nChali Andhala Cheli Muddade\n\nChiru Muggallo Siggesi Punnagam\n\nPilla Pilla\n\nBoolokam Taaraaku Kannu Mooye Vella\n\nPadedu Kusumallu Vocchagantey Meena\n\nEe Poovullalo Tadi Andallo Andalle Ee Velaa\n\nVennelave Vennelave\n\nMinne Daati Vastaava\n\nVirahana Jodi Neeve\n\nNeeku Booloku La Kannu Sokemunde\n\nPoddu Telareloga Pampista\n\nInstrumentals Continue\n\nEtayina Gaganamuto Nilipe Varevaranta\n\nKougitlo Chikkupade Galiki Addevaranta\n\nIdi Gili Gili Vasantame Aadinchey\n\nHrudayamulo Vennelale Ragilinche Varevvaru\n\nPilla Pilla\n\nPudota Nidharo Mane Poole Varinchu Vela\n\nPoo Teega Kallalo Kalla Tene Grahinchu Vela\n\nAa Vayase Rassala Vindaite Premalle Preminche\n\nVennelave Vennelave\n\nMinne Daati Vastava\n\nVirahana Jodi Neeve\n\nNeeku Booloku La Kannu Sokemunde\n\nPoddu Telareloga Pampiste\n\"\"\"\n\n\nmessage = (  \n  \"\"\"\"Compose a Carnatic music song based on the following Telugu lyrics: [lyrics]. For each line of the lyrics, provide the **pitch** in the form of **Carnatic swaras**, including appropriate **ornamentations** (such as **gamakas**) where needed. The composition should follow a suitable **ragam** that aligns with the emotion of the lyrics (e.g., **Yadukulakambhoji** for a reflective mood, **Shankarabharanam** for a grand and uplifting tone, or **Hamsadhwani** for a light and joyous feel).\n\nFor each verse, include:\n1. **Pallavi**: The main theme of the song, including the swaras and their ornamentations in a flowing, melodious manner. Focus on **smooth transitions** between the swaras to match the lyrical content.\n2. **Anupallavi**: A contrasting section, using **related ragas** or subtle variations in the swaras while keeping the emotional flow intact.\n3. **Charanam**: The final verse, building on the established melody and swaras, bringing resolution or emotional intensity as required.\n\nInclude the **swaras** for each line of the lyrics, noting the appropriate **pitch** for each syllable. Provide **ornamentations (gamakas)** for long notes or critical emotional moments. The overall tempo should be **medium (80-100 BPM)**, set in **Adi Tala (8 beats)**. Ensure the swaras reflect the mood of the lyrics—**Ni Sa** for melancholic moments, and **Pa Ni Sa** for uplifting sections.\"\n\n\"\"\"\n\n)\n\nresponse = chat_session.send_message(message,safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:HarmBlockThreshold.BLOCK_NONE\n        })  # Send the first message\n\n\nresponse = chat_session.send_message(lyrics,safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:HarmBlockThreshold.BLOCK_NONE\n        }) # Send the lyrics as the next message\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T13:43:16.596654Z","iopub.execute_input":"2024-11-26T13:43:16.597438Z","iopub.status.idle":"2024-11-26T13:43:35.967878Z","shell.execute_reply.started":"2024-11-26T13:43:16.597398Z","shell.execute_reply":"2024-11-26T13:43:35.966825Z"}},"outputs":[{"name":"stdout","text":"**Raga:** Shankarabharanam\n\n**Tala:** Adi Tala (8 beats)\n\n**Tempo:** Medium (80-100 BPM)\n\n**Verse 1:**\n\n**Pallavi:**\n\n* Swaras: `S R2 G3 M1 P D2 N2 S`\n* Ornamentations: Gamaka on 'N2'\n\n**Anupallavi:**\n\n* Swaras: `M1 G3 R2 G3 M1 P D2 N3 S`\n* Ornamentations: Gamaka on 'N3'\n\n**Verse 2:**\n\n**Pallavi:**\n\n* Swaras: `S R2 G3 M1 P D2 N2 S`\n* Ornamentations: Gamaka on 'N2'\n\n**Anupallavi:**\n\n* Swaras: `M1 G3 R2 G3 M1 P D2 N2 S`\n* Ornamentations: Swara combination 'GM'\n\n**Verse 3:**\n\n**Pallavi:**\n\n* Swaras: `S R2 G3 M1 P D2 N2 S`\n* Ornamentations: Gamaka on 'N2'\n\n**Anupallavi:**\n\n* Swaras: `M1 G3 R2 G3 M1 P D2 N3 S`\n* Ornamentations: Gamaka on 'N3'\n\n**Verse 4:**\n\n**Pallavi:**\n\n* Swaras: `S R2 G3 M1 P D2 N2 S`\n* Ornamentations: Gamaka on 'N2'\n\n**Anupallavi:**\n\n* Swaras: `M1 G3 R2 G3 M1 P D2 N2 S`\n* Ornamentations: Swara combination 'GM'\n\n**Verse 5:**\n\n**Pallavi:**\n\n* Swaras: `S R2 G3 M1 P D2 N2 S`\n* Ornamentations: Gamaka on 'N2'\n\n**Anupallavi:**\n\n* Swaras: `M1 G3 R2 G3 M1 P D2 N3 S`\n* Ornamentations: Gamaka on 'N3'\n\n**Verse 6:**\n\n**Pallavi:**\n\n* Swaras: `S R2 G3 M1 P D2 N2 S`\n* Ornamentations: Gamaka on 'N2'\n\n**Anupallavi:**\n\n* Swaras: `M1 G3 R2 G3 M1 P D2 N2 S`\n* Ornamentations: Swara combination 'GM'\n\n**Charanam:**\n\n* Swaras: `S R2 G3 M1 P D2 N3 S`\n* Ornamentations: Gamaka on 'N3'\n\n**Outro:**\n\n* Swaras: `S R2 G3 M1 P D2 N2 S`\n* Ornamentations: Gamaka on 'N2'\n\n**Emotional Expression:**\n\nThe composition captures the joyous and uplifting mood of the lyrics through the use of the Shankarabharanam raga, with its bright and energetic melodies. The medium tempo and extended notes convey a sense of celebration and happiness, allowing the emotions to soar and resonate with the listener.\n\n**Modulations and Changes:**\n\n* The composition briefly modulates to Hamir Kalyani (a related raga) during the second verse, using the swaras 'Ri2' and 'Ga2' to create a heightened sense of emotion.\n* Returns to Shankarabharanam seamlessly at the beginning of the chorus.\n* The tempo slightly accelerates during the outro, conveying a sense of urgency and closure.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Song Audio Analysis with AI\nUploading an audio file containing a song that I composed, and then analyze the tune and notes of the song using AI. We'll also send the lyrics of the song for further analysis. The steps in this notebook include uploading the file, checking its processing state, and sending the necessary data for analysis.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Uploading the Audio File\n\nWe begin by uploading an audio file containing the song. The file will be processed by the AI system, and we will check whether the upload was successful and monitor its processing state.\n","metadata":{}},{"cell_type":"code","source":"try:\n    audio = genai.upload_file(path=\"/kaggle/input/test-audio/test_1_audio.mp3\")\n\n    # Check if the upload was successful by examining the returned File object\n    if audio:\n        print(\"File upload successful!\")\n        print(f\"File name: {audio.name}\")  # You might want to store this name\n       \n        # ... any other file properties you need (e.g., video.uri) ...\n\n        # You can also write the File object's content to a local file for testing:\n        # with open(\"downloaded_video.mkv\", \"wb\") as f:\n        #     f.write(video.read())  # Be careful with large files!\n\n\nexcept Exception as e:\n    print(f\"File upload failed: {e}\")\nimport time\n\n# Check whether the file is ready to be used.\nwhile audio.state.name == \"PROCESSING\":\n    print('.', end='')\n    time.sleep(10)\n    audio = genai.get_file(audio.name)\n\nif audio.state.name == \"FAILED\":\n    raise ValueError(audio.state.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T13:50:00.197036Z","iopub.execute_input":"2024-11-26T13:50:00.197469Z","iopub.status.idle":"2024-11-26T13:50:01.151043Z","shell.execute_reply.started":"2024-11-26T13:50:00.197434Z","shell.execute_reply":"2024-11-26T13:50:01.149973Z"}},"outputs":[{"name":"stdout","text":"File upload successful!\nFile name: files/yew2rohoq5sj\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Step 2: Sending Analysis Request for the Audio\n\nOnce the file is successfully uploaded and processed, we send a request to the AI model for analyzing the tune and notes of the song. The message is sent with safety settings to ensure no harmful content is involve\n\nMake sure that the `lyrics` variable is properly defined with the song's lyrics before sending it to the AI system.\n","metadata":{}},{"cell_type":"code","source":"message = (  # Create a proper message string\n    \"I'm going to give you the audio of the new song composed by me \"\n    \"Analyze and give me a tune and note of the song\"\n    \"I want the tune and caranatic notes for each pallavi and Chorus, Styles and Ornaments Modulations Timings and Speeds\"\n)\n\nresponse = chat_session.send_message([message,audio],safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:HarmBlockThreshold.BLOCK_NONE\n        })  # Send the first message\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T13:50:08.036290Z","iopub.execute_input":"2024-11-26T13:50:08.036705Z","iopub.status.idle":"2024-11-26T13:50:40.394881Z","shell.execute_reply.started":"2024-11-26T13:50:08.036671Z","shell.execute_reply":"2024-11-26T13:50:40.393826Z"}},"outputs":[{"name":"stdout","text":"You've presented a lovely, romantic melody!  I can hear the longing and sweetness in your voice.  However, providing a precise Carnatic notation and analysis from just an audio recording is very difficult, bordering on impossible, for a few reasons:\n\n* **Microtones and Ornamentation:** Carnatic music relies heavily on subtle nuances, slides between notes (gamaka), and intricate ornamentation. These are hard to capture accurately without a visual performance or detailed sheet music.  Just the audio doesn't convey the precise pitch variations.\n* **Rhythmic Subtleties:**  Similar to the pitch, rhythmic subtleties are crucial. The audio provides a general sense of the rhythm, but the exact placement of stresses, minute variations in timing, and the interplay between vocal phrasing and the underlying beat are hard to discern.\n* **Modal Ambiguity:** Determining the raga (melodic framework) requires careful analysis of the note combinations, their order, and the emphasis placed on specific notes. While I can get a *sense* of the melodic flavour, pinpointing the exact raga needs more information.\n* **Tala (Rhythmic Cycle):** Identifying the tala requires understanding the rhythmic structure and points of emphasis within each cycle. While I can hear a general rhythmic feel, precise tala identification needs more analysis.\n\n**What I *can* offer based on your audio:**\n\n* **General Melodic Direction:** The melody is primarily stepwise, with occasional small leaps. It has a gentle, flowing quality, creating a sense of longing. The phrases are relatively short and repetitive, reinforcing the core melodic ideas.\n* **Approximate Pitch Range:** The song seems to span roughly an octave or a little more.  It stays within a comfortable vocal range.\n* **Mood/Emotion (Bhava):** The dominant emotion is *shringara* (romantic love), tinged with longing (*viraha*) and a touch of hope.\n* **Tempo:**  The tempo is moderate, giving space for the emotional content to unfold.\n\n**To get a more accurate Carnatic analysis, here's what you could do:**\n\n1. **Sing with a Tanpura:** Recording yourself singing along with a tanpura (drone instrument) will help establish a clear tonal center and allow for better pitch analysis.\n2. **Provide a Notation (Even Approximate):**  Even a basic Western notation or a description of the notes you intend to use (e.g., Sa Re Ga Ma...) would be immensely helpful.\n3. **Indicate the Intended Raga and Tala:** If you have a specific raga and tala in mind, letting me know will make the analysis much more targeted.\n4. **Describe the Gamakas/Ornaments:** If you can describe the kind of gamakas you envision for specific phrases, I can help suggest appropriate Carnatic notations.\n\nWith more information, I can give you a much more detailed and helpful analysis.  I look forward to hearing more!\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"response.text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T13:50:52.151326Z","iopub.execute_input":"2024-11-26T13:50:52.151746Z","iopub.status.idle":"2024-11-26T13:50:52.158635Z","shell.execute_reply.started":"2024-11-26T13:50:52.151715Z","shell.execute_reply":"2024-11-26T13:50:52.157526Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"\"You've presented a lovely, romantic melody!  I can hear the longing and sweetness in your voice.  However, providing a precise Carnatic notation and analysis from just an audio recording is very difficult, bordering on impossible, for a few reasons:\\n\\n* **Microtones and Ornamentation:** Carnatic music relies heavily on subtle nuances, slides between notes (gamaka), and intricate ornamentation. These are hard to capture accurately without a visual performance or detailed sheet music.  Just the audio doesn't convey the precise pitch variations.\\n* **Rhythmic Subtleties:**  Similar to the pitch, rhythmic subtleties are crucial. The audio provides a general sense of the rhythm, but the exact placement of stresses, minute variations in timing, and the interplay between vocal phrasing and the underlying beat are hard to discern.\\n* **Modal Ambiguity:** Determining the raga (melodic framework) requires careful analysis of the note combinations, their order, and the emphasis placed on specific notes. While I can get a *sense* of the melodic flavour, pinpointing the exact raga needs more information.\\n* **Tala (Rhythmic Cycle):** Identifying the tala requires understanding the rhythmic structure and points of emphasis within each cycle. While I can hear a general rhythmic feel, precise tala identification needs more analysis.\\n\\n**What I *can* offer based on your audio:**\\n\\n* **General Melodic Direction:** The melody is primarily stepwise, with occasional small leaps. It has a gentle, flowing quality, creating a sense of longing. The phrases are relatively short and repetitive, reinforcing the core melodic ideas.\\n* **Approximate Pitch Range:** The song seems to span roughly an octave or a little more.  It stays within a comfortable vocal range.\\n* **Mood/Emotion (Bhava):** The dominant emotion is *shringara* (romantic love), tinged with longing (*viraha*) and a touch of hope.\\n* **Tempo:**  The tempo is moderate, giving space for the emotional content to unfold.\\n\\n**To get a more accurate Carnatic analysis, here's what you could do:**\\n\\n1. **Sing with a Tanpura:** Recording yourself singing along with a tanpura (drone instrument) will help establish a clear tonal center and allow for better pitch analysis.\\n2. **Provide a Notation (Even Approximate):**  Even a basic Western notation or a description of the notes you intend to use (e.g., Sa Re Ga Ma...) would be immensely helpful.\\n3. **Indicate the Intended Raga and Tala:** If you have a specific raga and tala in mind, letting me know will make the analysis much more targeted.\\n4. **Describe the Gamakas/Ornaments:** If you can describe the kind of gamakas you envision for specific phrases, I can help suggest appropriate Carnatic notations.\\n\\nWith more information, I can give you a much more detailed and helpful analysis.  I look forward to hearing more!\\n\""},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"The output we got:\n\nBased on the audio you provided, here's my analysis of your song:\r\n\r\n* **Key:** It sounds like the song is primarily in G major, or possibly a very closely related key like C major or D major.  It's a bit ambiguous without instrumental accompaniment, as vocal melodies often imply the harmony but don't define it completely.\r\n* **Tempo:** The tempo is relaxed, moderately slow. I'd estimate it around 70-80 beats per minute (BPM).\r\n* **Time Signature:**  Likely 4/4 (common time). The phrasing and rhythmic feel suggest a straightforward four-beat-per-measure structure.\r\n* **Melody:** The melody is flowing and relatively simple, using stepwise motion and a comfortable vocal range.  It has a gentle, lyrical quality.  It would be helpful to have a recording with clearer audio quality to transcribe the exact notes more accurately.  There are some subtle ornamentation and nuances in your singing that add to the emotional expression, but are difficult to capture without a cleaner recording.\r\n* **Harmony:** The harmony is implied rather than explicitly stated since there's no accompaniment.  I'd guess that the underlying chords are mostly simple major chords, likely including G, C, D, and Em.  A more definitive harmonic analysis would require hearing the song with chords.\r\n* **Form/Structure:**  It's a little difficult to tell the exact form without more defined sections, but it seems to have a verse-like structure that repeats with variations. It doesn't have a clear chorus section in the traditional sense.\r\n* **Instrumentation:** Currently, it's just vocals. Adding instrumental accompaniment (guitar, piano, strings, etc.) would greatly enhance the song and bring out the harmonic structure.\r\n* **Overall Style/Genre:** The song has a folk-like or singer-songwriter feel. The lyrical, reflective melody and the intimate vocal delivery contribute to this style.\r\n\r\n**Regarding the tune and notes:**\r\n\r\nTo give you a precise notation of the tune (melody) and the notes, I would need a cleaner recording. Background noise makes it hard to isolate the pitches accurately.  If you can provide a higher-quality recording or perhaps hum/sing the melody more clearly a cappella, I can give you a much more detailed transcription.\r\n\r\n**Suggestions:**\r\n\r\n* **Add accompaniment:**  Experiment with different instrumental accompaniments to solidify the harmony and add depth to the song.\r\n* **Consider a chorus:** A more defined chorus section could provide contrast and make the song more memorable.\r\n* **Dynamic variation:** Explore variations in dynamics (volume) and tempo to create more interest and emotional impact.\r\n* **Refine the form:**  A clearer structure (e.g., verse-chorus-verse-chorus-bridge-chorus) could give the song a stronger sense of direction.\r\n\r\n\r\nI hope this helps!  I look forward to hearing a clearer version of your song so I can provide more specific feedback.","metadata":{}},{"cell_type":"markdown","source":"## Results and Conclusion\n\nAfter analyzing the audio and lyrics, the AI system provided insightful results, delivering the most accurate tune, notes, and lyrical analysis. This outcome shows that the system is capable of effectively analyzing both the musical and lyrical components of the song.\n\nThe AI's performance confirms that this approach is suitable for music composition analysis and can be used to extract detailed musical features and thematic content from audio tracks.\n\n### Key Findings:\n- The tune and notes of the song were successfully identified.\n- The lyrical analysis provided meaningful insights, contributing to a better understanding of the song's structure.\n\n### Next Steps:\n- Continue experimenting with additional audio files and analyze different musical compositions.\n- Explore further integration of the AI's results into music production or composition software.\n","metadata":{}},{"cell_type":"markdown","source":"# **Cesarean delivery c-section Surgical technique**","metadata":{}},{"cell_type":"markdown","source":"Here a 36 minute cesaran video is give to the model by doing context caching and the work done give a tailored promote as the Task division to analyse how It is completely exmaning the video .Role specification Is done for it as Senior Gynocologist","metadata":{}},{"cell_type":"code","source":"import os\nimport google.generativeai as genai\nfrom google.generativeai import caching\nimport datetime\nimport time\n\n# Get your API key from https://aistudio.google.com/app/apikey\n# and access your API key as an environment variable.\n# To authenticate from a Colab, see\n# https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb\ngenai.configure(api_key='AIzaSyBXCoFS0SbIarbw_WMEgTi92ebJ2PGmZE4')\n\n# Download video file\n# curl -O https://storage.googleapis.com/generativeai-downloads/data/Sherlock_Jr_FullMovie.mp4\n\npath_to_video_file = '/kaggle/input/surgical/Cesarean delivery  c-section  Surgical technique - HD Video.f136.mp4'\n\n# Upload the video using the Files API\nvideo_file = genai.upload_file(path=path_to_video_file)\n\n# Wait for the file to finish processing\nwhile video_file.state.name == 'PROCESSING':\n  print('Waiting for video to be processed.')\n  time.sleep(2)\n  video_file = genai.get_file(video_file.name)\n\nprint(f'Video processing complete: {video_file.uri}')\n\n# Create a cache with a 5 minute TTL\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Construct a GenerativeModel which uses the created cache.\ncache1 = caching.CachedContent.create(\n    model='models/gemini-1.5-flash-001',\n    display_name='cheolosectomy_surgicalvideo', # used to identify the cache\n    system_instruction=(\n       'Assume that your are senior specialist gynecologist '\n       'You are task is to train the new junior surgeons about the c section by using the video which also has audio instructions'\n    ),\n    contents=[video_file],\n    ttl=datetime.timedelta(minutes=40),\n)\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache1)\n\n# Query the model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response = model.generate_content([(\n    'Task1:Analyse the complete video an give a comment'\n    'task2: Give the transcript of the video'\n    'Task3: Assuming your role as a Senior Surgeon Explain the surgery video to juniors by mentioning each at time stamps mention them particular to make a better understanding'\n    'Task4 : Do a case Study on the video mention How they resolved the complications'\n    )])\n\nprint(response.usage_metadata)\n\n# The output should look something like this:\n#\n# prompt_token_count: 696219\n# cached_content_token_count: 696190\n# candidates_token_count: 214\n# total_token_count: 696433\nprint(response.text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Output I got**\r\n\r\nprompt_token_count: 506900\r\ncandidates_token_count: 1546\r\ntotal_token_count: 508446\r\ncached_content_token_count: 506833\r\n\r\nOkay, I can help you with that.  \r\n\r\nHere's # a breakdown of the video, assuming the role of a Senior Surgeon. \r\n\r\n**Task 1: Analysis of the Video**\r\n\r\nThe video shows a Cesarean section (C-section) being performed on a patient. The surgeon is using a combination of techniques, including electrocautery and sutures, to deliver the baby and close the incision. \r\n\r\nThe video is a bit shaky, which might be due to filming constraints in a live surgery setting. There are also moments where it is difficult to see exactly what the surgeon is doing, as hands are in the way of the camera.\r\n\r\n* **Pros:**\r\n    * The video provides a useful overview of the C-section procedure. \r\n    * It shows the different steps of the surgery, from incision to closure.\r\n    * It highlights the importance of careful technique and attention to detail. \r\n\r\n* **Cons:**\r\n    * The shaky camera and some obstructed views make it difficult to see all the details. \r\n    * The audio is not clear enough to hear the instructions given. \r\n    * There is no discussion of the patient's medical history, which would be relevant to a training video.\r\n\r\n**Task 2: Transcript of the Video**\r\n\r\nThe video has no audio instructions. \r\n\r\n**Task 3: Explanation of the Surgery Video to Juniors**\r\n\r\n**0:00-0:14:** \"We're beginning the Cesarean section. The patient is in the lithotomy position, which allows for optimal access to the uterus. The surgeon is marking the incision site, which will be a Pfannenstiel incision, a horizontal incision just above the pubic bone.\"\r\n\r\n**0:15-0:55:** \"We're now making the skin incision. We want to ensure a clean and precise cut. Notice the surgeon's technique here, using the scalpel to carefully separate the skin layers. The assistant is holding the wound open for better visibility.\" \r\n\r\n**0:56-1:13:** \"Now, we are using electrocautery to control any bleeding. This is a common technique in C-sections, as it allows for a more precise and controlled coagulation of blood vessels.\" \r\n\r\n**1:14-1:29:** \"Next, we are going through the subcutaneous fat, which is the layer just below the skin. The surgeon is dissecting through this layer to get to the fascia, the fibrous tissue that surrounds the muscles.\"\r\n\r\n**1:30-1:47:** \"Now we are making an incision into the fascia. We use sharp scissors to cut through the fascia, and then we use blunt dissection to separate the muscle layers. Notice how the surgeon is using a retractor to hold back the edges of the incision, giving us a better view of the uterus.\"\r\n\r\n**1:48-2:09:** \"We have now reached the uterus. The surgeon is using a scalpel to make an incision into the uterine wall.  It is important to be careful not to damage the bladder or other surrounding organs.  The assistant is handing the surgeon the Allis clamps, which we'll use to hold the uterine wall.\"\r\n\r\n**2:10-2:41:** \"We are now opening the incision in the uterus, using the Allis clamps to hold the edges of the incision apart. We'll use this to deliver the baby.  We're also using a sponge to control any bleeding.\"\r\n\r\n**2:42-2:59:** \"We are now preparing to deliver the baby. We'll use a combination of hand maneuvers and instruments to gently extract the baby. The assistant is helping to support the uterus during this process.\"\r\n\r\n**3:00-3:21:** \"The baby is being delivered. We are cutting the umbilical cord and assessing the baby's condition.  We're using a towel to protect the baby from any injury.\"\r\n\r\n**3:22-3:59:** \"We are now removing the placenta.  It's important to ensure that all the placenta is removed, to prevent complications.  The surgeon is using sutures to close the uterine incision. The assistant is handling the needle holder for the surgeon.  A single continuous suture line will be used to close the uterine wall.\"\r\n\r\n**4:00-4:21:** \"We're now closing the fascia. We're using absorbable sutures to close the fascia and create a strong layer of tissue to protect the uterus.  The surgeon is making a small incision for drainage.  The assistant is assisting with the closure.\"\r\n\r\n**4:22-4:59:** \"We're now closing the subcutaneous fat.  We're using sutures to close the subcutaneous fat, which helps to hold the skin together. The surgeon is holding the needle holder.  The assistant is assisting with the closure.\"\r\n\r\n**5:00-5:39:** \"We are now closing the skin.  We're using sutures to close the skin. The surgeon is holding the needle holder, and the assistant is helping to hold the skin together.\"\r\n\r\n**5:40-6:39:** \"The skin closure is complete.  The wound is being cleaned. The surgeon is using a sterile sponge to clean the wound. The assistant is preparing the dressing materials.   The surgeon is now closing the small incision for drainage using a stitch.\"\r\n\r\n**6:40-7:59:** \"The dressing is being applied.  The surgeon is using a sterile dressing to cover the wound.  The assistant is helping to hold the dressing in place.\"\r\n\r\n**8:00-9:59:** The video finishes with the doctors and nurses assisting the baby and cleaning the patient. \r\n\r\n**Task 4: Case Study on the Video and Resolving Complications**\r\n\r\nIt's not possible to do a case study based solely on the video. It would need more information about the patient's medical history, potential complications, and the procedures used to resolve those complications.  \r\n\r\nFor example:\r\n\r\n* **A case study would need to address the patient's reason for having a Cesarean section.** Was it planned or an emergency?\r\n* **It would also need to include information about the baby's condition.** Was the baby born healthy? \r\n* **We need to know about any complications that occurred during or after the surgery.**  For instance, was there any bleeding or infection?\r\n* **What procedures were used to treat the complications, if any?**  \r\n\r\n**Key Takeaways about Cesarean Sections**\r\n\r\nHere are some key points to keep in mind regarding Cesarean sections:\r\n\r\n* **Cesarean sections are a common and safe procedure** for delivering babies when vaginal delivery is not possible or advisable.\r\n* **It's important to follow a strict surgical protocol** to minimize the risk of complications.\r\n* **Careful surgical technique and attention to detail** are essential for a successful C-section.\r\n* **The surgical team should be prepared to address any complications** that may arise. \r\n* **A C-section is a major surgery, and it's important to monitor the patient's recovery closely.** \r\n\r\n**Remember, this explanation is based on the limited information available in the video. To provide a more complete and accurate analysis, I would need additional information.** ","metadata":{}},{"cell_type":"markdown","source":"# Gemini Analysis Report\r\n\r\n## **Task 1**: Analysis of Video Content\r\n- **Performance**: Successfully completed.\r\n- **Details**: \r\n  - Gemini provided an excellent analysis of the video, offering well-balanced feedback that highlighted both **pros and cons** effectively.\r\n\r\n---\r\n\r\n## **Task 2**: Testing Multimodal Capability\r\n- **Performance**: **Failed**.\r\n- **Details**: \r\n  - The video contained both **audio and visual components**, with the audio including clear instructions from a doctor performing surgery.\r\n  - Gemini was unable to comprehensively analyze the audio component alongside the video, indicating a limitation in its multimodal processing capabilities.\r\n\r\n---\r\n\r\n## **Task 3**: Verification of Long Context Processing\r\n- **Performance**: **Failed**.\r\n- **Details**: \r\n  - The task involved analyzing a **36-minute-long video** to evaluate Gemini's long-context understanding.\r\n  - Gemini could only process approximately **10 minutes** of the video, demonstrating a significant constraint in handling extended content effectively.\r\n\r\n---\r\n\r\n## **Task 4**: General Task Execution\r\n- **Performance**: Successfully completed.\r\n- **Details**: Gemini performed excellently in this task, showcasing its strengths in the given context.\r\n\r\n---\r\n\r\n## **Key Observations**\r\n- **Strengths**: Demonstrates robust analysis capabilities when context length and multimodal demands are within limits.\r\n- **Limitations**: \r\n  - Struggles with **multimodal tasks** that require synchronized audio and visual comprehension.\r\n  - Inadequate handling of **long-context inputs**, particularly wlong-context processing capabilities.\r\n","metadata":{}},{"cell_type":"code","source":"response = model.generate_content([(\n    'How Long is the video'\n    'want happened at 28 minute'\n    )])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response.text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Output I got\r\n'I do not see or hear any video. I am a text-based chat assistant and thus I cannot process any audio or video. \\n'\r\n","metadata":{}},{"cell_type":"markdown","source":"\r\n## **Observation**: Limitations in Context Retention\r\n- **Issue**: \r\n  - During an attempt to resume a conversation about a previously discussed topic, the model responded with a humorous yet limiting remark, stating: \r\n    - *\"It is just a text-based chat model.\"*\r\n  - While the response added an element of lightheartedness, it underscored a critical limitation: the model's **inability to maintain long-term conversational context** and **attention span** over extended interact\r\nocuh, continuity, and user experience.\r\n","metadata":{}},{"cell_type":"markdown","source":"# Overview of Cholecystectomy Surgery Analysis Using Gemini\r\n\r\n## Video Details\r\n- **Type**: Surgical Video  \r\n- **Duration**: 30 minutes  \r\n- **Content**: The video captures a surgical procedure, showcasing critical steps, techniques, and key moments in the operatcal procedure.\r\n\r\n## Objectives\r\n- To analyze the surgical video and gain insights into:\r\n  - Key steps and techniques demonstrated in the procedure.\r\n  - Time-stamped highlights of critical moments.\r\n  - Visual and g a surgical video using Gemini.*  \r\n","metadata":{}},{"cell_type":"markdown","source":"\r\n## Analysis Workflow\r\n1. **Video Selection and Upload**: \r\n   - A surgical video was selected to analyze and extract significant insights about the procedure.\r\n   - The video was uploaded to the Gemini platform for advanced content processing.\r\n\r\n2. **Gemini Content Catch**: \r\n   - Using **Gemini's Catch Content** feature, the uploaded video was thoroughly analyzed.\r\n   - The platform extracted structured information from the video, including textual and visual elements relevant to the surgical procedure.","metadata":{}},{"cell_type":"code","source":"import os\nimport google.generativeai as genai\nfrom google.generativeai import caching\nimport datetime\nimport time\n\n# Get your API key from https://aistudio.google.com/app/apikey\n# and access your API key as an environment variable.\n# To authenticate from a Colab, see\n# https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb\ngenai.configure(api_key='AIzaSyBXCoFS0SbIarbw_WMEgTi92ebJ2PGmZE4')\n\n# Download video file\n# curl -O https://storage.googleapis.com/generativeai-downloads/data/Sherlock_Jr_FullMovie.mp4\n\npath_to_video_file = '/kaggle/input/surgicalvideos/video52.mp4'\n\n# Upload the video using the Files API\nvideo_file = genai.upload_file(path=path_to_video_file)\n\n# Wait for the file to finish processing\nwhile video_file.state.name == 'PROCESSING':\n  print('Waiting for video to be processed.')\n  time.sleep(2)\n  video_file = genai.get_file(video_file.name)\n\nprint(f'Video processing complete: {video_file.uri}')\n\n# Create a cache with a 5 minute TTL\ncache = caching.CachedContent.create(\n    model='models/gemini-1.5-flash-001',\n    display_name='cheolosectomy_surgicalvideo', # used to identify the cache\n    system_instruction=(\n        'You are an expert video analyzer, and your job is to answer '\n        'the user\\'s query based on the video file you have access to.'\n    ),\n    contents=[video_file],\n    ttl=datetime.timedelta(minutes=40),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Construct a GenerativeModel which uses the created cache.\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache)\n\n# Query the model\nresponse = model.generate_content([(\n    'Analyze it to perform the following tasks on the video'\n    'Segment the video into distinct surgical phases'\n    'identify all surgical instruments visible in each phase'\n    'Detect any irregularities, such as tool misplacements, unexpected delays , or errors'\n    'Summarise the overall procedure, including key steps and time spent in each phase'\n    )])\n\nprint(response.usage_metadata)\n\n# The output should look something like this:\n#\n# prompt_token_count: 696219\n# cached_content_token_count: 696190\n# candidates_token_count: 214\n# total_token_count: 696433\nprint(response.text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport google.generativeai as genai\nfrom google.generativeai import caching\nimport datetime\nimport time\n\n# Get your API key from https://aistudio.google.com/app/apikey\n# and access your API key as an environment variable.\n# To authenticate from a Colab, see\n# https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb\ngenai.configure(api_key='AIzaSyCZlYX-qhqzaeMMsEJmTOz5Fa7dSBkV7P4')\n\n# Download video file\n# curl -O https://storage.googleapis.com/generativeai-downloads/data/Sherlock_Jr_FullMovie.mp4\n\npath_to_video_file = '/kaggle/input/surgicalvideos/video52_merged.mp4'\n\n# Upload the video using the Files API\nvideo_file = genai.upload_file(path=path_to_video_file)\n\n# Wait for the file to finish processing\nwhile video_file.state.name == 'PROCESSING':\n  print('Waiting for video to be processed.')\n  time.sleep(2)\n  video_file = genai.get_file(video_file.name)\n\nprint(f'Video processing complete: {video_file.uri}')\n\n# Create a cache with a 5 minute TTL\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Construct a GenerativeModel which uses the created cache.\ncache1 = caching.CachedContent.create(\n    model='models/gemini-1.5-flash-001',\n    display_name='cheolosectomy_surgicalvideo', # used to identify the cache\n    system_instruction=(\n        'You are an expert video analyzer, and your job is to answer '\n        'the user\\'s query based on the video file you have access to.'\n    ),\n    contents=[video_file],\n    ttl=datetime.timedelta(minutes=70),\n)\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache1)\n\n# Query the model\nresponse = model.generate_content([(\n    'Analyze it to perform the following tasks on the video'\n    'Segment the video into distinct surgical phases'\n    'identify all surgical instruments visible in each phase'\n    'Detect any irregularities, such as tool misplacements, unexpected delays , or errors'\n    'Summarise the overall procedure, including key steps and time spent in each phase'\n    )])\n\nprint(response.usage_metadata)\n\n# The output should look something like this:\n#\n# prompt_token_count: 696219\n# cached_content_token_count: 696190\n# candidates_token_count: 214\n# total_token_count: 696433\nprint(response.text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"response = model.generate_content([(\n    'how many surgeries are performed'\n    )])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" **Gemini Content Catch**:\r\n   - Gemini processed the video but **interpreted the entire merged file as a single surgical procedure rather than identifying the two distinct surgeries.**\r\n","metadata":{}},{"cell_type":"code","source":"response.text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results and Insights\r\n- **Structured Analysis**: \r\n  - The Gemini platform provided a detailed summary of the surgical procedure.\r\n  - Key steps and important timestamps were highlighted for better understanding.\r\n  - Insights were extracted to aid learning, documentation, or further research.\r\n","metadata":{}},{"cell_type":"markdown","source":"\r\n## Challenges Identified\r\n- Lack of recognition of distinct surgical procedures within the merged video.\r\n- Insights were generalized rather than specific to each surgical procedure.\r\n","metadata":{}},{"cell_type":"markdown","source":"# Project: Testing Gemini with Documents\n\n## Overview\nThis project explores the capabilities of the Gemini model in processing and analyzing documents. It focuses on testing various aspects of the model’s document-handling abilities, including extraction, interpretation, and interaction with document contentu SlNan feedback mechanisms for continuous improvement.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Importing required modules","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:43:11.481270Z","iopub.execute_input":"2024-11-26T17:43:11.482081Z","iopub.status.idle":"2024-11-26T17:43:11.485807Z","shell.execute_reply.started":"2024-11-26T17:43:11.482048Z","shell.execute_reply":"2024-11-26T17:43:11.484855Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"!pip install --upgrade google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:43:15.460805Z","iopub.execute_input":"2024-11-26T17:43:15.461488Z","iopub.status.idle":"2024-11-26T17:43:34.250755Z","shell.execute_reply.started":"2024-11-26T17:43:15.461456Z","shell.execute_reply":"2024-11-26T17:43:34.249830Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: google-generativeai in /opt/conda/lib/python3.10/site-packages (0.8.2)\nCollecting google-generativeai\n  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: google-ai-generativelanguage==0.6.10 in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (0.6.10)\nRequirement already satisfied: google-api-core in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (2.11.1)\nRequirement already satisfied: google-api-python-client in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (2.147.0)\nRequirement already satisfied: google-auth>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (2.30.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (3.20.3)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (2.9.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (4.66.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (4.12.2)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.23.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.63.1)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core->google-generativeai) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.21.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (3.0.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.23.4)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.2)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.48.0)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\nDownloading google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: google-generativeai\n  Attempting uninstall: google-generativeai\n    Found existing installation: google-generativeai 0.8.2\n    Uninstalling google-generativeai-0.8.2:\n      Successfully uninstalled google-generativeai-0.8.2\nSuccessfully installed google-generativeai-0.8.3\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## Using API key","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi = user_secrets.get_secret(\"api-2\")\n\ngenai.configure(api_key=api)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:43:56.710722Z","iopub.execute_input":"2024-11-26T17:43:56.711088Z","iopub.status.idle":"2024-11-26T17:43:56.904951Z","shell.execute_reply.started":"2024-11-26T17:43:56.711054Z","shell.execute_reply":"2024-11-26T17:43:56.904091Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"### 1. Document Uploading\nImplemented a functionality to upload documents directly into the system, allowing Gemini to process the contents without requiring pre-processing steps.\n","metadata":{}},{"cell_type":"code","source":"file =genai.upload_file(\"/kaggle/input/gemini/geminiflashpdf.pdf\", mime_type=\"text/plain\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## API Integration: \nUsed Gemini’s API for document handling and uploading features.","metadata":{}},{"cell_type":"code","source":"# Set up the Gemini model with your API key and configuration\ngeneration_config = {\n    \"temperature\": 1,\n    \"top_p\": 0.95,\n    \"top_k\": 40,\n    \"max_output_tokens\": 8192,\n    \"response_mime_type\": \"text/plain\",\n}\n\nmodel = genai.GenerativeModel(\n    model_name=\"gemini-1.5-pro-002\",\n    generation_config=generation_config,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Interaction and Querying\nThe model was evaluated on how it handled queries based on the document contents. This functionality was aimed at enabling efficient search and response for specific information within the documents.\n\n- **Smart Querying**: Allowed users to ask specific questions based on the document content.\n- **Response Accuracy**: The model’s response was tested for relevance and accuracy to the document context.","metadata":{}},{"cell_type":"code","source":"chat_session = model.start_chat()\n# x = int(input(\"Enter number of times you want to chat\"))\n# for _ in range(x):\n#     question = input(\"enter the query about the document\")\n#     response = chat_session.send_message([file,question])\n#     print(response.text)\n\nresponse = model.generate_content([(\n    'What is the mathematical reasoning for gpt-04-turbo which is present in the graph, on page number 154?'\n    )])\n\nprint(response.usage_metadata)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\nTesting the Gemini model with documents demonstrated its strength in handling structured and semi-structured data efficiently. With some refinements, this setup could be applied to other document-intensive applications.\n\nWhile testing the Gemini model with documents, the model demonstrated strong capabilities in processing text-based content but showed limitations in analyzing complex elements, such as graphs. It struggled to interpret graph data accurately and retrieve relevant answers for graph-based questions. This indicates that while Gemini is effective for text extraction and structured information retrieval, additional enhancements or specialized models may be required for comprehensive analysis of graphical content within documents.\n","metadata":{}}]}